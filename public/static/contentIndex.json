{"Server-and-Software/Basic-Linux-Commands":{"slug":"Server-and-Software/Basic-Linux-Commands","filePath":"content/Server and Software/Basic Linux Commands.md","title":"Basic Linux Commands","links":["tmux"],"tags":["Command"],"content":"1 Basic commands\nShells such as Bash, Zsh, and Tcsh provide powerful command-line environments for Unix-like systems, each with its own syntax and features. While they all allow users to execute multiple commands sequentially or in parallel, their scripting syntax and built-in functionalities differ. For example, Bash offers rich scripting capabilities and is widely adopted for its compatibility and flexibility, while Zsh includes advanced features like improved tab completion and a robust plugin system. Tcsh, known for its C-like syntax, is often preferred for interactive use due to its user-friendly command-line editing features. For example, AFNI uses the tsch shell.\nPrint Working Directory (pwd) will show you the current directory.\npwd\n\nList the files in the directory (ls), with some parameters, such as long format with human readable.\nls -lh  directory\n\nIf you want to list files Recursively, you can use ls -R.\nChange the Current Directory (cd)\ncd to/sub/folder\n\nCoPy files (cp)\ncp original duplicate\n\nrsync — a fast, versatile, remote (and local) file-copying tool. You need to have an SSH connection setup.\n#copy from remote files to local\nrsync -rv username@remote:/path/to/directory local/directory\n \n#copy from local to remote\nrsync -rv local/directory username@remote:/path/to/destination\nYou can remove (rm) files, rename (rm) files, or move (mv) files to elsewhere.\nFor directories, you can make a new directory (mkdir) or remove directory (rmdir).\nTo quickly show a text file in the console, you can cat the file.\ncat readme.txt\n\nIf you only want to show the head (head) or the tail (tail), you can do\n# show the first 5 rows \nhead -n 5 readme.txt\n\nSimilarly, you can selective show columns from a table text file using cut, but you need to specify the columns (fields, -f) and tell the delimiter (-d).\ncut -f 2-5,8 -d , exp1.csv\n\nThe above command will show the columns 2 to 5 and column 8 with the delimiter of , from the csv file exp1.csv.\nTo store a command’s output in a file &gt;, which serves as a pipe line.\nhead readme.txt &gt; head.csv\n\nIf you want to append the output to the existing file, you replace &gt; with &gt;&gt;.\nA general pipe symbol is |. For example, list the last 5 trials of the first block (48 trials) from data.csv file,\nhead -n 48 data.csv | tail -n 5\n\n2 Regular expression\ngrep takes a piece of text followed by one or more filenames and prints all of the lines in those files that contain specified text or patterns.\nFor example, grep memory readme.txt prints lines from readme.txt that contain ‘memory’.\ngrep common parameters:\n\n-c: a count of matching lines rather than the lines themselves\n-i: ignore case (e.g., treat “Memory” and ‘memory’ as the same)\n-l: print the names of files that contain matches, not the matches\n-n: print line numbers for matching lines\n-v: invert selection, list not matched lines.\n\nThe command wc (short for “word count”) prints the number of characters, words, and lines in a file. You can make it print only one of these using -c, -w, or -l respectively.\nFor example, let’s suppose we have Exp1.csv file with the column 8 name curDur. You want to show how many trials with curDur = 0.5:\ncut -f 8 -d , Exp1.csv | grep -v curDur | grep 0.5 | wc -l \n\nwildcards\nThe most common wildcard is *, which mean ‘match everything’. There are others:\n\n? matches a single character, so sub-0?.txt will match sub-01.txt or sub-02.txt, but not sub-11.txt.\n[...] matches any one of the characters inside the square brackets, so 201[78].txt matches 2017.txt or 2018.txt, but not 2016.txt.\n{...} matches any of the comma-separated patterns inside the curly brackets, so {*.txt, *.csv} matches any file whose name ends with .txt or .csv.\n\n\narithmatic expansion. $[] or $(()) will calculate the value inside. For example, echo $[3 + 2*4] will output 11.\n\nVariable replacements\nYou can construct a text string using variables. For example,\na=sub\necho ${a}-01.txt\n\nit will output sub-01.txt.  Note, if $a is separable from the rest, such as the above example, {} can be omitted. Otherwise, it may caused a confusion. For example,\necho $ab.txt  #error\necho ${a}b.txt #output: subb.txt\n\nsorting\nsort puts data in order. By default, it in ascending order, but the flags -r can reverse the order (i.e., descending), -n to sort numerically, -b to ignore leading blanks, and -f tells it to fold case (i.e., be case-insensitive).\nunique\nuniq is to remove duplicated lines that are adjacent. So to remove all duplicate lines, you need to sort the data first. With the flag -c, it also output the number of duplicates.\nFor example, count the number of trials for each sample durations (column 8 curDur) from Exp1.csv file:\ncut -d , -f 8 Exp1.csv | grep -v curDur | sort | uniq -c\n\n3 String manipulations\nManipulating file names is a common task using command lines. For example, we want to substitute a file name with a specified pattern. Stream editor sed command can perform many functions on file like searching, finding and replacing, insertion, or deletion.\nReplacing or substituting string\nsed &#039;s/unix/linux&#039; aText.txt\n\nIt will replace the word unix with linux in the file. Here, s specifies the substitution operation, and the ”/” delimiters are used.\nReplacing the nth occurrence of a pattern in a line by adding a number at the end of the expression. For example,\nsed &#039;s/unix/linux/2&#039; aText.txt\n\nReplacing all the occurrence of the pattern in a line with the flag /g (global replacement)\nsed &#039;s/unix/linux/g&#039; aText.txt\n\nReplacing from nth occurrence to all occurrence in a line. Use the combination of number and g flag together.\nsed &#039;s/unix/linux/3g&#039; aText.txt\n\nThe delimiter / could be |.\n\n\n                  \n                  Task \n                  \n                \n\nSuppose you want to replace all filenames with ‘_run’ to ‘_task-bisection_run’. Think about the pipe and commands.\n\n\n\n\n                  \n                  Solution\n                  \n                \n\nls -R | grep _run | sed &#039;s/_run/_task-bisection_run/&#039;\n\n\n\n4 tmux\ntmux is a command-line terminal multiplexer for Unix-like systems. You can control tmux using key combinations; you first type a prefix key combination (by default ctrl + b) followed by additional command keys.\n# list existing tmux sessions\n$ tmux ls\n# create a new session\n$ tmux new -s fmriAna\n\nTo make the connection and remember what you are doing; I recommend using tmux.\nBasic usage as follows:\n# list existing tmux sessions\ntmux ls\n \n# create a new session\ntmux new -s fmriAna\n \n# attach to an existing tmux session\ntmux a -t mysession\n \n# terminate an existing tmux session\ntmux kill-session -t mysession\n \n# creat a new window (ctrl + b, c)\n# navigate windows (ctrl + b, [0-9])\n# flip to next window (ctrl + b, n)\n# split horizontal (ctrl+b, &quot;)\n# split vertical (ctrl+b, %)"},"Server-and-Software/GNode-and-Datalad":{"slug":"Server-and-Software/GNode-and-Datalad","filePath":"content/Server and Software/GNode and Datalad.md","title":"1.2 GNode and Datalad","links":["Server-and-Software/Git-Annex"],"tags":["GNode","Datalad"],"content":"1. G-Node\nFocusing on the development and free distribution of tools for handling and analyzing neurophysiological data, G-Node aims to address these aspects as part of the International Neuroinformatics Coordinating Facility (INCF) and the German Bernstein Network for Computational Neuroscience (NNCN). G-Node also serves as an international forum for Computational Neuroscientists interested in sharing experimental data and tools for data analysis and modeling. G-Node is funded through the German Federal Ministry of Education and Research and hosted by Ludwig-Maximilians-Universität München.\nBenefits of G-Node over Github:\n\nHosting large binary files\nAssigning unique DOI. It is good to open data and get citations.\nUsing Git Annex as infrastructure helps manage versions.\n\ngin\nManagement of scientific data, including consistent organization, annotation, and storage of data, is a challenging task. Accessing and managing data from multiple workplaces while keeping it in sync, backed up, and easily accessible from within or outside the lab is even more demanding. The GIN (G-Node Infrastructure) service is a free and open data management system designed for comprehensive and reproducible management of neuroscientific data.\ngin client installation\nPlease see the official website for installations. For Mac OS users, the easiest way to install the client on macOS is via homebrew. G-Node homebrew formulae are maintained in the G-Node tap. Install the client, including any dependencies, with:\nbrew tap g-node/pkg\nbrew install g-node/pkg/gin-cli\n\nAlternatively, if you already have git and git-annex installed on your system, or you want to install them manually or via homebrew, the recommended and simplest way to install git-annex is via Homebrew using brew install git-annex. Alternatively, download git-annex from the git-annex website.\nOnce you’ve installed git-annex, simply download the gin client for macOS, extract the archive, and put the file named gin in a location that’s included in your $PATH.\nBasic usage of gin\n\nregister g-node.org website and sign into the GIN Server.\nCreate a new repository using the ”+” on the top right. Alterantively, you can create locally:\n\ngin create &lt;repository name&gt;\n\n\nCopy new files into the newly created directory via Drag &amp; Drop, Copy &amp; Paste etc.\nIn the GIN client (terminal) window, navigate into the newly created local workspace by typing cd &lt;repository name&gt;.\nUpload the new files using\n\n gin upload .\n\nNote the period at the end of the command. This command will commit your changes. In other words, it will detect the new files in the directory, add them to the repository, and start uploading to the GIN server. Every time you perform a gin upload . the changes are saved and uploaded and a checkpoint is made of your data.\nYou can instead upload individual files or directories by listing them on the command line. For example:\n    gin upload file1.data recordings/recording1.h5\n\nThis will upload changes made to two files: file1.data and recording1.h5, where the latter is in the recordings directory.\nNote that upload here doesn’t only mean sending new files and changes to the server. This command sends all changes made in the directory to the server, including deletions, renames, etc. Therefore, if you delete files from the directory on your computer and perform a gin upload, the deletion will also be sent and the file will be removed from the server as well. Such changes can be synchronized without uploading any new files by not specifying any files or directories.\n    gin upload\n\nFetch any repository updates from the server\nIf changes are made to your data elsewhere, for example on another computer (assuming they were uploaded to the server), or from another user that you share your data with, you can download these changes by typing the download command from within the repository.\ngin download\n\nThis command will only download changes made to the repository (file deletions, renames, etc.) but any new files are downloaded as placeholders. Placeholder files are empty files that represent files uploaded to the repository but do not hold any of the data. This is useful for downloading the contents of larger files on demand without downloading the entire repository.\nIf you would like to download all the data contained in a repository, you can do so using the --content flag.\ngin download --content\n\nThis will synchronize the local directory with all changes made on the server and download the content of all files.\nSelective download\nWhen new data has become available or existing files have been changed on the GIN server, a selected subset of the changes can be downloaded to the local workspace.\n\nDownload a summary of the changes on the GIN server using gin download. IMPORTANT: This does not download any data. New files and files changed on the GIN server are considered to be “unsynced”.\nUse gin ls to check the sync status of the files in the repository.\nUse gin get-content &lt;file name&gt; to download the data of a specific file.\n\nSelective upload\nWhen new data has been added to or existing files changed in the local workspace, a selected subset of the changes can be uploaded to the GIN server.\n\nUse gin ls to check the sync status of the files in the repository.\nUse gin upload &lt;file name&gt; to only upload the specified new file or changes to the specified existing file.\n\n2 DataLad\nA good start of Datalad Handbook.\nIt assists with the combination of all things necessary in the digital workflow of data and science.\nDataLad only cares (knows) about two things: Datasets and files. A dataset is a Git repository. A DataLad dataset can take care of managing and version controlling arbitrarily large data.\nDataLad can manage Gnode repository, see Gin and DataLad.\nBasic usage:\ndatalad save -m &#039;save something&#039;\ndatalad update \ndatalad push\n\nThis is similar to the follow traditional git commands\ngit add\ngit commit\ngit pull\ngit push\n"},"Server-and-Software/Git-Annex":{"slug":"Server-and-Software/Git-Annex","filePath":"content/Server and Software/Git Annex.md","title":"Git Annex","links":[],"tags":[],"content":"git annex tips\nTo ensure that your CSV files are managed by Git and not by git-annex, you can use the git annex unannex command to convert files that are currently managed by git-annex back to being managed by Git. After using unannex, you will commit the change to record the fact that these files are no longer managed by git-annex.\nHere’s how you can do it:\n\n\nNavigate to your repository’s directory in the terminal.\n\n\nUnannex the CSV files you want to be managed by Git. You can do this for individual files or for all CSV files in a directory. Here’s how you do it for all CSV files in the current directory:\ngit annex unannex *.csv\nIf your CSV files are spread out in different directories or you want to be more selective, you’ll need to adjust the command accordingly.\n\n\nCommit the change to record that these files are no longer managed by git-annex:\ngit commit -am &quot;Move CSV files from git-annex to Git management&quot;\n\n\nPush the changes to your remote repository:\ngit push\n\n"},"Server-and-Software/LRZ-Cloud-Computing":{"slug":"Server-and-Software/LRZ-Cloud-Computing","filePath":"content/Server and Software/LRZ Cloud Computing.md","title":"LRZ Cloud Computing","links":[],"tags":["Cloud","HPC"],"content":"The following information is useful for those who can access Linux cluster at LRZ.de\nAccess Cloud Computing\nMsense Lab also has two cloud computing systems, Lora and Emma. Emma is accessible from the external internet, while Lora is walled by the intranet.\n\n\n                  \n                  Tips\n                  \n                \n\nFor convenience, it’s better that you create aliases for those servers. For example, in your .zshrc or .bashrc, put similar lines:\nalias lora=&quot;10.195.2.##&quot; (please change the ip accordingly)\n\n\n1. Via SSH\n# access to Lora\nssh -Y your_account@lora\n\n\n2. Via VNC\n\nVia VNC\nFirst, you need to tunnel to the server and initiate the VNC server (this only needs to be done once). The following commands should be run on the server side.\n\n2.1 Setup VNC password\nvncpasswd\nYou’ll be prompted to enter and verify your password. This password is required when connecting with your VNC client.\n2.2 Start VNC server\nvncserver\nIt will show you “New ‘X’ desktop is lora:#“. Remember this, we will first kill it and edit the file -  xstartup and restart it again.\n2.3 Edit VNC xstartup file\nFirst, kill the VNC service\nvncserver -kill :# (replace the number)\nNow edit the xstartup\nnano ~/.vnc/xstartup\nEdit/Put the following text in the file:\n#!/bin/sh\nxrdb $HOME/.Xresources\n\n# Start Xfce\nstartxfce4 &amp;\n\n2.4 Start VNC server localhost\nvncserver -localhost -geometry 1024x768\n\nDepending on your preference, you can specify the dimension of the virtual desktop. If you want to change it, you have to kill the service first, and restart it again (see above instructions). And remember which port number it started. You can check it in the ~/.vnc/ folder, by finding the lora:#.pd file.\n2.5 Local connection\nNow you can exit the server, and start a local command window, type:\n# Access to Lora and replace the # with your VNC port number\nssh -L 590#:localhost:590# -C -N -l your_account 10.195.?.?\n\nReplace the correct server IP, depending on your server.\nNext, you can start your local VNC (under Mac) using “Finder ⇒ Go ⇒ Connect to Server”, type in:\nvnc://localhost:590#\n\nThe connection password is the password you set earlier on.\n3. Via Code Server\nThe code server is configured individually. If your account is set up, you can first establish the tunnel:\nssh -Y -N -L 808@:localhost:808@ your_account@lora_ip\nPlease replace the port number, account name, and IP above. And leave this command window open.\nNext, open your browser and type localhost:808@. A code server interface will appear. Code Server is convenient when you don’t have a local installation of Visual Studio Code. However, if you accidentally close the brower tab, you may lose some working that is running.\n4. Via Visual Studio Code\nYou need to install ‘remote-SSH’ and ‘Remote Explorer’ plugins. After you install these plugins, by click the left icon of ‘Remote Explorer’, add remote SSH, using the option 1 ‘Via SSH’ to connect to the server.\nWhen the VS Code connects to the server, it will show the connection in the lower-left corner. The first time it opens nothing. You need to select ‘Open Folder’ to open a desired destination.\n5. Mapping the remote server locally\nThis is desirable when checking remote MRI results like local files. It won’t do any remote computation. And it requires changing the security level in your local Mac computer.\nFirst, install MacFUSE and SSHFS\nbrew install macfuse\nbrew tap gromgit/fuse\nbrew install gromgit/fuse/sshfs\nSecond, create a local mount point\nmkdir ~/dss\nThird, mount the remote linux server folder:\nsshfs username@server:/path/remote ~/dss\nAccess Linux Cluster\nIt requires a Linux cluster LRZ account and 2FA. Details can be found in the LRZ website.\n# connect to linux cluster at LRZ\n \nssh -Y your_account@cool.hpc.lrz.de\n \n1. Resource\nOverview of cluster specifications and limits\ndoku.lrz.de/job-processing-on-the-linux-cluster-10745970.html\nStatus here:\ndoku.lrz.de/high-performance-computing-10613431.html\n2. Install singularity\nWhen logged in to the Linux cluster (on the login nodes), you can follow this recipe (this has been done):\nmodule rm intel-mpi intel-mkl intel  \nmodule load user_spack  \nspack install squashfs squashfuse  \nspack load squashfs squashfuse  \nspack install singularity ~suid  \nspack -c&quot;modules:tcl:blacklist_implicits:False&quot; module tcl refresh --upstream-modules squashfs singularity\n\nUsage later is then simple (new terminal/or in a Slurm Job):\n\nmodule use spack/modules/x86_avx2/linux-sles15-haswell   \nmodule load singularity squashfs  \n\n\n3. Slurm partition and job settings\nThe following information needs to be updated; please check lrz.\njob settings\n**cm2_std:** \n--clusters=cm2  \n--partition=cm2_std  \n--qos=cm2_std\n\n**cm2_inter**:\n--clusters=inter  \n--partition=cm2_inter\n\n**cm2_inter_large_mem**\n56 cpu, memory 120 G\n--clusters=inter  \n--partition=cm2_inter_large_mem  \n--mem=&lt;memory_per_node_MB&gt;M\n\nCommon Slurm commands for job management:\n56 cpu 56 G\nsqueue -M cm2_tiny -u $USER  \nscancel -M cm2_tiny &lt;JOB-ID&gt;  \nsacct -M cm2_tiny -X -u $USER --starttime=2021-01-01T00:00:01\n\nsqueue -M inter -u $USER  \nscancel -M inter &lt;JOB-ID&gt;  \nsacct -M inter -X -u $USER --starttime=2021-01-01T00:00:01\n\n4. Interactive\nsalloc -N 1 -M inter -p cm4_inter -t 8:00:00\n\n\nshow CPUs and Memory\nlscpu | grep &#039;^CPU(s):&#039;\nfree -h\n\nsubmit to slurm\nsbatch sing_cluster.sh\n"},"Server-and-Software/Neurocommand":{"slug":"Server-and-Software/Neurocommand","filePath":"content/Server and Software/Neurocommand.md","title":"Neurocommand","links":[],"tags":["Neurocommand"],"content":"Neurocommand is a command-line interface to Neurodesk, which is designed to run on virtual machine or HPC.\nTo get NeuroCommand to work, you need to tell the bash shell system where to find the singularity folder and modules. For those set up users, the following codes are already in your .bashrc.\nexport PATH=~/bin:/usr/local/go/bin:${PATH}\nsource /etc/profile.d/lmod.sh\n \nexport SINGULARITY_BINDPATH=&#039;/cvmfs,/mnt,/home,/dss&#039;\nexport SINGULARITYENV_TEMPLATEFLOW_HOME=/dss/.templateflow\nmodule use /cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/*\n1 Command line and bash file usage\nmodule load module_name\nmodule list\nFor example, if you want to load module, you can use\nmodule load fsl\nmodule list\nit shows\nCurrently Loaded Modules:\n  1) fsl/6.0.7.16\n\nmodule avail can show all modules available from the Neurocommand.\nScenario 1: Interactive Analysis\nIf you tunnel to the server with option -Y (i.e., ssh -Y account@lora), you should be able to send back fsl interface back to your local computer.\nmodule load fsl\nFeat &amp;\nIt will show the ‘Feat’ interface locally.\n\n\n                  \n                  Note\n                  \n                \n\nTunnelling via SSH for the interface may be slow, particularly when you want to open the design matrix with multiple inputs. A better way to do this is via VNC interface, remotely running Feat or related software.\n\n\nScenario 2: Batch analysis using bash files\nThe following is an example of using the bet function from the fsl package in a bash file.\n#!/bin/bash\nmodule load fsl\n# Set the main BIDS directory (modify if needed)\nBIDS_DIR=&quot;/dss/open/ds000102&quot;\n# Loop through subject folders\nfor subj in ${BIDS_DIR}/sub-*; do\n\tsubj_id=$(basename ${subj}) # Extract subject ID (e.g., sub-01)\n\t# Define input and output paths\n\tanat_dir=&quot;${subj}/anat&quot;\n\tinput_image=&quot;${anat_dir}/${subj_id}_T1w.nii.gz&quot;\n\toutput_image=&quot;${anat_dir}/${subj_id}_T1w_brain.nii.gz&quot;\n\t# Check if input file exists\n\tif [ -f &quot;${input_image}&quot; ]; then\n\t\techo &quot;Running BET on ${input_image}...&quot;\n\t\tbet &quot;${input_image}&quot; &quot;${output_image}&quot; -R -f 0.25 -g 0 # Adjust parameters as needed\n\telse\n\t\techo &quot;T1w image not found for ${subj_id}, skipping...&quot;\n\tfi\ndone\n \necho &quot;Brain extraction completed for all subjects.&quot;\n2 Use in Python\nUnfortunately, the Python environment cannot directly read the bash environment. So you need to set up the environment first in your Python. If you are using Jupyter Notebook, copy the following code to the beginning of your notebook:\nimport os\nos.environ[&quot;SINGULARITY_BINDPATH&quot;] = &quot;/cvmfs,/mnt,/home,/home/lu32pog/dss&quot;\nos.environ[&quot;LMOD_CMD&quot;] = &quot;/usr/share/lmod/lmod/libexec/lmod&quot;\nos.environ[&quot;SINGULARITYENV_TEMPLATEFLOW_HOME&quot;] = &quot;/home/lu32pog/dss/.templateflow&quot;\nos.environ[&quot;MODULEPATH&quot;] = &quot;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/*&quot;\nThen you can load modules via lmod\n# need to load base (Python) from miniconda (all packages installed)\nimport lmod\n#await lmod.purge(force=True)\nawait lmod.load(&#039;fsl&#039;)\nawait lmod.list()\n3. Use with Google Colab\nThe official guidance is here\nAt the beginning of your notebook, you need to add:\nimport os\nos.environ[&quot;LD_PRELOAD&quot;] = &quot;&quot;;\nos.environ[&quot;APPTAINER_BINDPATH&quot;] = &quot;/content&quot;\nos.environ[&quot;MPLCONFIGDIR&quot;] = &quot;/content/matplotlib-mpldir&quot;\nos.environ[&quot;LMOD_CMD&quot;] = &quot;/usr/share/lmod/lmod/libexec/lmod&quot;\n \n!curl -J -O raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n!chmod +x googlecolab_setup.sh\n!./googlecolab_setup.sh\n \nos.environ[&quot;MODULEPATH&quot;] = &#039;:&#039;.join(map(str, list(map(lambda x: os.path.join(os.path.abspath(&#039;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/&#039;), x),os.listdir(&#039;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/&#039;)))))\nThen, you can use it:\nimport lmod\nawait lmod.avail()\n \nawait lmod.load(&#039;fsl/6.0.4&#039;)\n!bet\n "},"Server-and-Software/Tips-and-Tricks":{"slug":"Server-and-Software/Tips-and-Tricks","filePath":"content/Server and Software/Tips and Tricks.md","title":"Tips and Tricks","links":[],"tags":[],"content":"Neurocommand in Linux Cluster\nInstall Container\n\nSearch and install Specific Containers\n\nbash containers.sh fmriprep\nThe above will give you options of which specific containers you want to install. For example:\n| fmriprep | 24.1.0 | 20241003 | functional imaging,workflows | Run:\n\n--------------------------------------------------------------------\n\n./local/fetch_containers.sh fmriprep 24.1.0 20241003\n\nInstall Singularity containers\nFind docker images, and then build it with singularity. For example, install FitLins:\nsingularity build /path/to/fitlins-&lt;VERSION&gt;.simg \\\n  docker://poldracklab/fitlins:&lt;VERSION&gt;"},"fMRI-Tutorial/1.-MRI-Basics":{"slug":"fMRI-Tutorial/1.-MRI-Basics","filePath":"content/fMRI Tutorial/1. MRI Basics.md","title":"1. MRI Basics","links":[],"tags":["mri","Physics"],"content":"Principles of Magnetic Resonance\nMagnetic Resonance Imaging (MRI) is based on the behavior of nuclear spins in a strong magnetic field. Atomic nuclei with an odd number of protons or neutrons (like the hydrogen proton in water) possess an intrinsic spin, which gives rise to a magnetic moment. In an external magnetic field B_0, these magnetic moments tend to align with the field, and any that are not perfectly aligned will precess around the field direction at a characteristic Larmor frequency. This is given by the Larmor equation:\n\\omega_0 = \\gamma B_0,\nwhere \\omega_0 is the angular precession frequency (in radians per second) and \\gamma is the gyromagnetic ratio (a constant for each nuclear species).\nThe gyromagnetic ratio determines how fast a given type of nucleus precesses in a magnetic field. For protons (^1H), \\gamma \\approx 42.58 MHz/Tesla, meaning at B_0=1.5 T the proton precesses at about 64 MHz and at 3 T about 128 MHz .\nDifferent nuclei have different \\gamma values. For examples::\n\n^{13}C   — ~10.7 MHz/T,\n^{19}F — 40.05 MHz/T,\n^{31}P — 17.24 MHz/T.\n\nAmong biologically relevant nuclei, the proton has one of the highest gyromagnetic ratios, which is why hydrogen (abundant in water and fat) is predominantly used in MRI.\nIn essence, a nucleus with spin behaves like a tiny top or bar magnet. When placed in B_0, it will precess analogously to a spinning top in a gravitational field. The precession frequency is proportional to the field strength. This forms the basis for nuclear magnetic resonance: if one applies radiofrequency (RF) energy at the Larmor frequency, one can resonantly excite the spins. In MRI, the main static field B_0 is very strong (typically 1.5 T, 3 T, or higher in research scanners), setting the stage for all protons to precess in unison at a well-defined frequency.\nSpin Excitation and Relaxation (T1 and T2)\nIn thermal equilibrium, slightly more spins align with B_0 (low energy state) than against it, yielding a net longitudinal magnetization M_z along the field. An RF pulse (usually a 90° pulse) at the Larmor frequency tips some of this magnetization into the transverse plane, creating transverse magnetization M_{xy}.\n\n(Image from radiologykey.com)\nOnce the RF pulse is turned off, the spins begin to return to equilibrium through two independent exponential processes known as relaxation:\n\nT1 relaxation (longitudinal or spin-lattice relaxation): This is the recovery of M_z back to its equilibrium value M_0 along the z-axis (parallel to B_0). Physically, the spin system releases energy to the surrounding lattice (the molecular environment) as spins realign with the field. The regrowth of M_z follows an exponential recovery:\n\nM_z(t) = M_0 \\big(1 - e^{-t/T_1}\\big),\nwhere T_1 is the time constant for 63% recovery of the longitudinal magnetization . Each tissue has a characteristic T_1 (ranging from hundreds of milliseconds to seconds in biological tissues), and T_1 is generally longer at higher field strengths. Intuitively, T_1 depends on how efficiently a nucleus can transfer energy to its surroundings; typically, protons in water have long T_1 (on the order of 2–3 s at 1.5 T), whereas protons in fat or protein have shorter T_1 (hundreds of ms or less) because of more efficient energy exchange with the lattice.\n\n\nT2 relaxation (transverse or spin-spin relaxation): This is the decay of M_{xy} in the transverse plane due to dephasing of individual spins. Immediately after an RF pulse, the spins are momentarily in phase, but various interactions cause them to fall out of phase with each other over time, leading to a loss of net transverse magnetization. The decay of M_{xy} is exponential:\n\nM_{xy}(t) = M_{xy}(0)\\, e^{-t/T_2},\nwhere T_2 is the time constant for the transverse magnetization to decay to 37% of its initial value. T_2 is typically shorter than T_1 for a given tissue (often by a factor of 5–10) . This is because many microscopic processes (like random spin-spin interactions and local field inhomogeneities) cause dephasing without energy loss, speeding up the M_{xy} decay. Pure liquids (like cerebrospinal fluid or water) have relatively long T_2 (hundreds of milliseconds), whereas solids or tightly bound protons have very short T_2 (tens of milliseconds or less) due to more rapid dephasing .\n\nImportantly, there is also T_2^* (T2-star) relaxation, which is the effective transverse relaxation that includes not only the spin-spin interactions (true T_2 decay) but also dephasing from static magnetic field inhomogeneities. In a perfectly homogeneous field, transverse decay would be governed by T_2 alone. In a real MRI scanner, however, slight field non-uniformities and susceptibility variations cause additional dephasing, making the observed decay faster (with a time constant T_2^* &lt; T_2). T_2^* is particularly important for gradient-echo MRI sequences and for BOLD fMRI, as we will see later. A spin-echo sequence can refocus static inhomogeneity effects (recovering T_2 decay), whereas gradient-echo sequences do not, thus capturing T_2^* effects.\nMRI Contrast Mechanisms (T1- vs T2-Weighted Imaging)\nMRI scanners can be “tuned” to produce images with different contrasts by exploiting the differences in T_1, T_2, and proton density of tissues. The timing parameters of a pulse sequence — primarily the repetition time (TR) and echo time (TE) — determine how the image contrast is weighted.\nRepetition time (TR) is the time interval between the start of one RF excitation pulse and the start of the next in an MRI pulse sequence. TR determines how much longitudinal magnetization (along B_0) has recovered before the next excitation.\nEcho time (TE) is the time between the application of the RF excitation pulse and the moment the MR signal (the “echo”) is measured. TE controls how much transverse magnetization decay (T2 or T2*) has occurred before signal acquisition.\n\nT1-weighted images\n\nUse a short TR and short TE. A short TR (on the order of the tissue T_1 or less, e.g. ~300–600 ms) means tissues haven’t fully recovered their M_z before the next excitation, so tissues with shorter T_1 recover more and appear brighter, while long T_1 tissues remain darker. A short TE (e.g. &lt; 30 ms) minimizes T_2 decay effects, so contrast is mainly due to T_1 differences.\nExample: Fat (short T_1 ~240 ms) appears bright on T1-weighted images, whereas fluid like cerebrospinal fluid (long T_1 ~3000 ms) appears dark. T1-weighted scans provide clear anatomical detail and are often used post-contrast, because gadolinium-based contrast agents shorten T_1 dramatically, making enhancing tissues light up on T1 images.\n\nT2-weighted images\n\nUse a long TR and long TE. A long TR (≫ T_1, e.g. &gt; 2000 ms) allows full or near-full M_z recovery in all tissues before the next excitation, so T_1 differences are minimized. A long TE (e.g. 70–100+ ms) allows substantial T_2 decay, so tissues with longer T_2 will retain more signal and appear brighter, while short T_2 tissues lose signal and appear dark.\nExample: Fluid (long T_2 ~3000 ms for CSF) remains bright on T2-weighted images (since it decays slowly), whereas tissues like white matter (shorter T_2 ~90 ms) lose most of their transverse magnetization by the echo time and appear relatively dark. T2-weighted scans are very sensitive to pathology because many lesions (edema, inflammation) have higher water content and thus prolonged T_2, making them stand out bright against darker normal tissue.\n\nProton-density (PD) weighted images:\n\nUse a long TR (to minimize T_1 differences) and a short TE (to minimize T_2 differences). This yields contrast primarily based on the number of protons (hydrogen density) in each tissue. PD-weighted images are less common in the brain but are sometimes used in orthopedic MRI.\n(Note: Many MRI sequences exist beyond simple spin-echo. For example, FLAIR is a T2-weighted sequence with fluid signal suppressed, useful in brain imaging to see periventricular lesions; gradient-echo sequences can be T2-weighted, etc. However, the basic principles of T_1 and T_2 contrast apply across these variations.)\nReferences and Further Reading\n\nBuxton RB. Introduction to Functional Magnetic Resonance Imaging: Principles and Techniques. Cambridge Univ. Press, 2009. (Comprehensive text on fMRI physics and physiology)\n[MRIQuestions.com](An excellent website with clear explanations)\n"},"fMRI-Tutorial/1.1-BOLD-Signal-and-HRF":{"slug":"fMRI-Tutorial/1.1-BOLD-Signal-and-HRF","filePath":"content/fMRI Tutorial/1.1 BOLD Signal and HRF.md","title":"1.1 BOLD Signals and HRF","links":[],"tags":["HRF","BOLD"],"content":"Hemoglobin and the Basis of the BOLD Signal\nFunctional MRI (fMRI) often relies on the Blood-Oxygen-Level-Dependent (BOLD) contrast mechanism to detect brain activity. This mechanism is based on the distinct magnetic properties of hemoglobin, which vary according to its oxygenation state. Oxyhemoglobin (oxygen-bound hemoglobin) is diamagnetic (no unpaired electrons, similar magnetic susceptibility to water and tissue), whereas deoxyhemoglobin (hemoglobin that has released its oxygen) is paramagnetic due to 4 unpaired electrons in the iron atoms. Paramagnetic substances create local distortions in the magnetic field. Thus, deoxyhemoglobin inside blood vessels behaves like a microscopic magnet, perturbing the local magnetic field in its vicinity.\nThe presence of more deoxyhemoglobin leads to faster dephasing of nearby proton spins (especially those in tissue and blood water around the vessel), effectively shortening T_2 and T_2^*.\nIn practical terms, more deoxyhemoglobin = lower MRI signal (on T2-sensitive images) because the spins lose phase coherence more quickly (a loss of signal by the echo time). Conversely, if deoxyhemoglobin is reduced (i.e. more oxyhemoglobin is present), local field homogeneity improves and T_2/T_2^* are lengthened, yielding a higher MRI signal.\nThis is the essence of BOLD contrast: it is an indirect measure of changes in blood oxygenation. Notably, the effect is strongest on T_2^*-weighted sequences (such as gradient-echo echo-planar imaging used in fMRI), with the optimal echo time tuned to the tissue’s T_2^* (typically 30–40 ms at 3 T for BOLD). Spin-echo sequences are less sensitive to BOLD because they refocus static dephasing; however, at very high fields spin-echo BOLD (sensitive to smaller vessels) can also be used, leveraging diffusion effects near capillaries.\nSo how do changes in neural activity modulate the levels of deoxyhemoglobin?\nThis occurs through a physiological process called neurovascular coupling. When neurons become more active, they consume more oxygen and energy. The local change in metabolism triggers a cascade of events (mediated by signaling molecules and vascular responses) that leads to increased cerebral blood flow (CBF) to the active region, as well as a smaller increase in cerebral blood volume (CBV) in the local microvasculature. Critically, the increase in blood flow overcompensates for the oxygen consumed by neurons. As a result, even though the neurons are using oxygen faster, the influx of fresh blood carrying oxyhemoglobin is so great that the local fraction of deoxyhemoglobin actually decreases relative to baseline. For example, at rest a venule might have ~40% deoxyhemoglobin, but during activation it might drop to 30% or less because of the excess oxygenated blood delivered. This reduction in deoxyhemoglobin causes the local magnetic field environment to become more homogeneous (less paramagnetic disturbance), thus increasing the MR signal from that region on T2*-weighted images . In short, active brain regions get brighter in BOLD fMRI because they have more oxyhemoglobin and less deoxyhemoglobin compared to when they are inactive.\nIt’s important to note that the BOLD signal is relative and measures a complex interplay of factors: changes in CBF, changes in oxygen metabolism (CMRO₂), and changes in blood volume all contribute to the signal. The stereotypical change observed (in most of cortex under normal conditions) is an increase in blood flow that dominates, yielding a positive BOLD signal change for increased neural activity. However, there can be nuances: an initial dip (transient increase in deoxyhemoglobin before blood flow kicks in) and a post-stimulus undershoot are often observed, as discussed next. Additionally, because the BOLD effect arises predominantly from changes in T_2^*, the strength of the effect depends on field strength (higher field MRI yields a larger percent signal change for BOLD) and on the vessel size and architecture (larger vessels contribute more to the signal change on gradient-echo BOLD due to bigger susceptibility gradients , whereas spin-echo BOLD preferentially weights microvascular changes ). Despite these complexities, BOLD fMRI has proven to be a robust method for mapping brain activity due to this tight coupling between neural activity and localized blood oxygenation changes.\nHemodynamic Response Function (HRF) – Shape of the BOLD Signal\nWhen the neural activity in a region changes (for instance, a brief stimulus causing a burst of neural firing), the BOLD signal in that region does not rise instantly but follows a delayed, stereotyped time course known as the hemodynamic response. The Hemodynamic Response Function (HRF) describes the BOLD signal change over time in response to an instantaneous neural event (it’s essentially the impulse-response of the vascular system in the brain ). Key features of the HRF include an initial latency, a possible small dip, a prominent peak, and a post-stimulus undershoot:\n\nCanonical BOLD hemodynamic response following a brief neural stimulus, illustrating the typical phases: a small initial dip, a larger delayed peak, and a post-stimulus undershoot_. Empirical studies (e.g., Richter &amp; Richter, 2003) have demonstrated that the HRF generally consists of a positive peak around 5-6 seconds after stimulus onset, followed by a smaller negative dip (undershoot) around 12-16 seconds. Eventually, the hemodynamics return to baseline, typically by ~20–30 s post-stimulus.*\nIn a typical event-related fMRI experiment, even a very short neural stimulus (e.g. a 1 s visual flash) will produce a BOLD signal change that unfolds over many seconds. There is an initial delay of ~1–2 s before any significant rise in signal (this reflects the hemodynamic lag — it takes a couple of seconds for blood flow to change after neural activity). Sometimes a small initial dip in the signal can be seen around 1–2 s post-stimulus, thought to correspond to the early oxygen consumption before blood flow increases. However, this dip is usually tiny (a slight decrease in signal of perhaps 0.1–0.5%) and is often masked by noise; it has been reliably observed mainly in high-field or high signal-to-noise studies. Its exact origin is still debated (it could result from a transient increase in oxygen extraction or a rapid increase in local blood volume that briefly increases deoxyhemoglobin) .\nThe main feature of the HRF is the positive peak. About 4–6 seconds after the stimulus onset, the BOLD signal reaches a maximum above baseline. This peak can be on the order of a 1–5% increase in signal in a 3 T scanner (depending on the region and amplitude of neural activity). It reflects the hyper-oxygenation of the blood: the cerebral blood flow has increased so much that it supplies more oxygen than the neurons can use, thus washing out deoxyhemoglobin and making the MR signal higher . If the stimulus is brief, the BOLD signal will peak and then start to drop back down even while the stimulus has ceased (because neural activity returned to baseline). If the stimulus is prolonged (say, a 20 s task), the BOLD signal will rise and plateau (as blood flow remains elevated), and when the stimulus stops, the signal will dip back down.\nAfter the peak and the end of the neural stimulation, the BOLD signal often overshoots below baseline, resulting in a post-stimulus undershoot that can last for several seconds (e.g., 5–15 s after the peak). In this phase, the signal is slightly lower than the original baseline. The cause of the undershoot is not fully settled, but it is commonly attributed to continued elevated cerebral blood volume even after blood flow has returned to baseline, or to lingering metabolic effects that temporarily increase deoxyhemoglobin above baseline levels. Essentially, the vessels (especially veins) might remain dilated for a while, so blood volume is high but blood flow and oxygen delivery have normalized, leading to a higher fraction of deoxyhemoglobin and hence a lower signal until the vasculature recovers. Eventually, everything returns to baseline, completing the cycle.\nFrom a signal processing perspective, the HRF acts like a smoothing filter on the neural activity: rapid bursts of neural firing will produce a sluggish, smoothed-out BOLD signal change. This is why, in fMRI data analysis, one often models the neural events convolved with a canonical HRF to predict the BOLD signal. The canonical HRF (such as the one used in SPM software) is often modeled as the sum of two gamma functions – one for the main peak and a smaller one for the undershoot – capturing the general shape (peak around 5 s, undershoot around 10–15 s) .\nFormalizing the HRF and Linear Convolution\nSuppose a participant is exposed to an event or stimulus E_i at time 0. This event elicits neural activity denoted by N_i(t) and a corresponding BOLD response denoted by B_i(t). This relationship can be formally represented as:\nN_i(t) \\rightarrow f(\\cdot) \\rightarrow B_i(t)\nThis mapping is linear if it satisfies the superposition principle:\nif f[N_1(t)] = B_1(t) and f[N_2(t)] = B_2(t), then it must be true that\nf[a_1N_1(t)+a_2N_2(t)] = a_1B_1(t)+a_2B_2(t)\nDiscrete and Continuous Representations\nConsidering a discrete neural activation function, we express it as a series of impulses:\nN(t) = \\sum_{\\tau = 0}^\\infty n_\\tau \\delta (t-\\tau)\nApplying the linear superposition principle, the corresponding BOLD response becomes:\nB(t) =f[N(t)] =f[\\sum_{\\tau = 0}^\\infty n_\\tau \\delta (t-\\tau)] = \\sum_{\\tau = 0}^t n_\\tau h(t-\\tau)\nHere, the function h(t - \\tau) is the system’s response to an impulse, known as the hemodynamic response function (HRF). Thus, the HRF characterizes how neural activation translates into BOLD signal changes.\nIn the continuous limit, the relationship becomes a convolution integral:\nB(t) = \\int_0^t N(\\tau)h(t-\\tau)d\\tau\nThis convolution is often succinctly represented as:\nB(t) = N(t) \\star h(t)\nCanonical HRF Models\nSeveral mathematical forms are commonly used to model HRFs:\n1. Single Gamma Function\nThe simplest form is a gamma function without an undershoot:\ng(t) = \\frac{t^{\\alpha - 1} e^{-\\frac{t}{\\beta}}}{\\beta^{\\alpha} \\Gamma(\\alpha)}\n2. Double Gamma (Gamma-variate) Function\nIncorporates both a main response peak and a subsequent undershoot:\ng(t) = \\frac{t^{\\alpha_1 - 1} e^{-\\frac{t}{\\beta_1}}}{\\beta_1^{\\alpha_1} \\Gamma(\\alpha_1)} - c \\frac{t^{\\alpha_2 - 1} e^{-\\frac{t}{\\beta_2}}}{\\beta_2^{\\alpha_2} \\Gamma(\\alpha_2)}\nHere, c modulates the amplitude of the undershoot.\nPractical Example Using Nilearn\nTo illustrate how an HRF might be generated and visualized using Nilearn’s built-in Glover HRF model:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nilearn.glm.first_level.hemodynamic_models import glover_hrf\n \n# Define time points for the HRF\ntime_points = np.linspace(0, 32, 1000)\n \n# Generate Glover HRF\nglover = glover_hrf(tr=1.0, oversampling=10, time_length=32, onset=0.0)\n \n# Plot HRF\nplt.plot(time_points[:len(glover)], glover)\nplt.title(&#039;Glover Hemodynamic Response Function&#039;)\nplt.xlabel(&#039;Time (seconds)&#039;)\nplt.ylabel(&#039;Amplitude&#039;)\nplt.grid(True)\nplt.show()\n\nThis code generates the canonical HRF described by Glover (1999), which is extensively used in fMRI studies for modeling neural responses. Nilearn’s functions simplify HRF computation and visualization for analysis and educational purposes.\nChoosing an HRF Model\nSelecting an appropriate HRF model depends on your experimental design, type of stimulus, and brain region under study. While canonical forms like the Glover HRF offer robust approximations, flexibility in modeling is sometimes required for capturing subtle differences in brain responses.\nReferences\n\nAshby, F. G. (2011). Statistical Analysis of fMRI Data. MIT Press.\nBandettini PA, Wong EC, et al. (1992). Time course EPI of human brain function during task activation. Magn Reson Med, 25(2): 390-7. (Early demonstration of BOLD fMRI)\nGlover, G. H. (1999). Deconvolution of impulse response in event-related BOLD fMRI. NeuroImage, 9(4), 416–429.\nOgawa S, et al. (1990). Oxygenation-sensitive contrast in magnetic resonance image of rodent brain at high magnetic fields. Magn Reson Med, 14(1): 68-78 . (Classic paper identifying deoxyhemoglobin’s magnetic properties and the basis of BOLD)\nUludağ K, et al. (2009). An integrative model for neuronal activity-induced signal changes for gradient and spin echo fMRI. NeuroImage, 48(1): 150-165 . (Advanced reading on the biophysical models of the BOLD signal)\nHuettel SA, Song AW, McCarthy G. Functional Magnetic Resonance Imaging (3rd ed.), 2014. (Good overview of BOLD mechanism and neurovascular coupling)\nRichter, W., &amp; Richter, M. (2003). The shape of the fMRI BOLD response in children and adults changes systematically with age. NeuroImage, 20(2), 1122–1131.\n"},"fMRI-Tutorial/2.1-MRI-Data-Acquisition-and-Format":{"slug":"fMRI-Tutorial/2.1-MRI-Data-Acquisition-and-Format","filePath":"content/fMRI Tutorial/2.1 MRI Data Acquisition and Format.md","title":"2.1 MRI Data Preparation","links":["rfMRI_REST_PA_6.json"],"tags":["DICOM","MRI"],"content":"2.1.1 MRI sequence\nMRI data types depend on the sequence employed during scanning. An MRI sequence consists of a series of radiofrequency pulses and gradients that produce images with distinct characteristics. For instance, consider a property from the EPI BOLD sequence commonly used in functional imaging.\nIn each experiment, multiple sequences may be applied—such as resting-state fMRI, T1-weighted structural scans, or EP2d BOLD sequences. Each sequence generates its own set of images stored in separate subfolders within the study directory; these folders are typically named to reflect their corresponding sequences. Below is an example drawn from sample data in Cemre’s study. Can you identify which sequences were used?\n\nThe folder name reflects a combination of protocol, task, and serial number, allowing you to infer the sequence it contains. For instance, folders labeled T1W_MPR_* correspond to T1-weighted scans, while those named EP_BOLD_BISEC_S* hold fMRI BOLD data from the bisection task across two sessions.\nWithin each folder, it stores individual DICOM images, like this:\n\nDICOM stands for Digital Imaging and Communications in Medicine. Each image contains independent metadata, which is redundant for our purposes. If you perform a scan 1,000 times, you will end up with 1,000 separate DICOM files in that folder. To better organize your data, it is preferable to convert the DICOM files into NIfTI format—producing a single file per scan run.\n2.1.2 DICOM to Nifti converter\nSince BIDS provides a standardized framework for managing data, using DCM2BIDS to convert DICOM files and organize your data in BIDS format is recommended (see next section).\nOrganize your raw files like this (you can rename subject/session folders to anything you like at this stage):\n/project/\n  dicom/\n    sub01/\n      ses01/   # optional if you have sessions\n        &lt;lots of DICOM files&gt;\n    sub02/\n      ses01/\n        ...\n\nYou will produce:\n/project/\n  bids/\n    sub-01/\n      ses-01/\n        anat/  func/  dwi/  fmap/   # etc.\n\n0. Installation\nconda install -c conda-forge dcm2bids\nconda install -c conda-forge dcm2niix\nThe MSense lab server has dcm2bids, so you can skip this.\n1. Scaffold an empty BIDS project\nmkdir -p /project/name\ndcm2bids_scaffold -o /project/name\n2. Inspect your DICOM series with the helper\nPick one subject/session to prototype your config.\n# Example paths — adjust to your folders\nDICOM=/project/dicom/sub01/ses01\nOUT=/project/bids\n \n# This writes a TSV + JSONs describing each series\ndcm2bids_helper -d &quot;$DICOM&quot; -o &quot;$OUT&quot;\nLook at:\n\nbids/tmp_dcm2bids/sub-01/ses-01/dcm2bids_helper.tsv\n(or similar path) — each row = a series with useful fields like SeriesNumber, SeriesDescription/ProtocolName, EchoTime, PhaseEncodingDirection, etc.\n\nYou will use these fields to write matching rules.\n3. Write your config.json\nCreate /project/bids/code/config.json (path is up to you). Start simple and expand.\nPattern: a list of “descriptions.” Each description says which DICOM series to grab and where to place the output in BIDS.\nHere is one example:\n{\n    &quot;descriptions&quot;: [\n      {\n        &quot;datatype&quot;: &quot;anat&quot;,\n        &quot;suffix&quot;: &quot;T1w&quot;,\n        &quot;criteria&quot;: {\n          &quot;SeriesDescription&quot;: &quot;T1w_MPR&quot;,\n          &quot;ImageType&quot;: [&quot;ORIGINAL&quot;, &quot;PRIMARY&quot;, &quot;M&quot;, &quot;ND&quot;, &quot;NORM&quot;]       \n        }\n      },\n      {\n        &quot;id&quot;: &quot;id_task_run1&quot;,\n        &quot;datatype&quot;: &quot;func&quot;,\n        &quot;suffix&quot;: &quot;bold&quot;,\n        &quot;custom_entities&quot;: &quot;task-tidi_run-01&quot;,\n        &quot;criteria&quot;: {\n          &quot;SeriesDescription&quot;: &quot;ep_bold_rep_s1&quot;\n        }\n      },   \n      {\n        &quot;datatype&quot;: &quot;fmap&quot;,\n        &quot;suffix&quot;: &quot;epi&quot;,\n        &quot;criteria&quot;: {\n          &quot;SeriesDescription&quot;: &quot;SpinEchoFieldMap3mm_AP_2&quot;\n        },\n        &quot;sidecar_changes&quot;: {\n            &quot;intendedFor&quot;: [ &quot;id_task_run1&quot;]\n        }\n      }\n    ]\n}\n\nTips while editing config.json:\n\nUse regular expressions in criteria (they’re case-sensitive by default; add (?i) for case-insensitive like (?i).t1.).\nMatch on fields you trust from the helper TSV: SeriesDescription, ProtocolName, EchoTime, ImageType, SeriesNumber, etc.\nUse customLabels to add BIDS entities like task-, acq-, run-, dir-.\n(e.g., “customLabels”: “task-nback_acq-MB4_run-02”)\nIf a rule accidentally captures multiple series, make your regex stricter.\n\n4. Test on one subject\ndcm2bids \\\n  -d /project/dicom/sub01/ses01 \\\n  -p 01 \\\n  -s 01 \\\n  -c /project/bids/code/config.json \\\n  -o /project/bids\nIf files aren’t where you expect, adjust the config and rerun.\n5. Convert all subjects (batch)\nfor SUB in 01 02 03; do\n  for SES in 01; do\n    dcm2bids \\\n      -d &quot;/project/dicom/sub${SUB}/ses${SES}&quot; \\\n      -p &quot;${SUB}&quot; \\\n      -s &quot;${SES}&quot; \\\n      -c /project/bids/code/config.json \\\n      -o /project/bids\n  done\ndone\n2.1.3 Other converters\nMRIcroGL\nWith MRIcroGL open, select the menu option Import followed by Convert DICOM to NIfTI. This action opens the dcm2niix converter interface. By specifying filename rules, storage location, and other settings, you can quickly convert DICOM files to NIfTI format.\nTwo key parameters for naming rules are %p and %s, which represent the protocol and series number respectively. These essentially form the folder name.\nThe DICOM files in each folder will be converted to two files:\n\nOne JSON file, which stores image information.\n\nHere is one example of json file: rfMRI_REST_PA_6.json\n\n\none NIfTI file (*.nii or *.nii.gz).\n\nConverting DICOM to NIFTI can make your raw data tidy, but not necessary save your space. When you have multiple sequences and multiple participants, it is wise to organize your data folder in a standard structure. A common standard structure that is accepted by multiple analysis software (e.g., fMRIprep) is BIDS. We will discuss this in next chapter.\nHeudiconv\nheudiconv is a flexible DICOM converter for organizing brain imaging data into structured directory layouts.\n\nIt allows flexible directory layouts and naming schemes through customizable heuristics implementations.\nIt only converts the necessary DICOMs and ignores everything else in a directory.\nYou can keep links to DICOM files in the participant layout.\nUsing dcm2niix under the hood, it’s fast.\nIt can track the provenance of the conversion from DICOM to NIfTI in W3C PROV format.\nIt provides assistance in converting to BIDS.\nIt integrates with DataLad to place converted and original data under git/git-annex version control while automatically annotating files with sensitive information (e.g., non-defaced anatomicals, etc).\n\nIf you want to install in your local computer, one way is to use Docker, see instruction here\nheudiconv comes with existing heuristics which can be used as is, or as examples. For instance, the Heuristic convertall extracts standard metadata from all matching DICOMs. heudiconv creates mapping files, &lt;something&gt;.edit.text which lets researchers simply establish their own conversion mapping.\nHaving decided on a heuristic, you could use the command line (see detailed Usage):\nheudiconv -f HEURISTIC-FILE-OR-NAME -o OUTPUT-PATH --files INPUT-PATHs\n2.1.4 Compress raw data\nGiven raw data include lots of small files, which will eventually explode our data science storage, not because of the size, but because of the limited number of files we can store (maximum: 8 million).  I suggest to put the following bash code (file) to your raw folder to compress individual subfolders.\n#!/bin/bash\n# Get a list of all subdirectories in the current directory\n \nfor dir in */ ; do\n\t# Remove the trailing slash to get the directory name\n\tdir_name=${dir%/}\n\t# Compress the directory into a tar.gz file with the same name\n\ttar -czvf &quot;${dir_name}.tar.gz&quot; &quot;${dir_name}&quot;\ndone\necho &quot;All subfolders have been compressed.&quot;\nAfter the compression, you can safely remove subfolders. You can modify the above script to do this.\n\n\n                  \n                  What not compress all subject data together \n                  \n                \n\nIn principle you can also compress all subject data to one single tar.gz file. However, I would recommend compressing individual participants, as we may want to convert or do further things with individual data. If you compress all data together, unzipping the whole dataset may take more time (prob. minutes to hours), and you have to expand all files. Even though there is a way to extract partial data, extracting folder structure from a huge zip file takes time. Trust me.\n\n"},"fMRI-Tutorial/2.2-BIDS-Structure":{"slug":"fMRI-Tutorial/2.2-BIDS-Structure","filePath":"content/fMRI Tutorial/2.2 BIDS Structure.md","title":"2.2 BIDS Structure","links":[],"tags":["BIDS","mri"],"content":"BIDS stands for the Brain Imaging Data Structure, which is a simple and intuitive way to organize and describe your data. The whole specification can be found on the official website. Here are some excerpts from the official website.\nBefore we go into details, let’s clarify several important terms.\n\nDataset - a set of neuroimaging and behavioral data acquired for the purpose of a particular study.\nSubject - a person or animal participating in the study. Used interchangeably with the term Participant.\nSession - a logical grouping of neuroimaging and behavioral data consistent across subjects. Generally, subjects remain in the scanner for one session.\nSample - a sample about a subject such as tissue, primary cell, or cell-free sample. The sample-&lt;label&gt; key/value pair is used to distinguish between different samples from the same subject.\nData type - a functional group of different types of data. BIDS defines the following data types:\n\nfunc (task based and resting state functional MRI)\ndwi (diffusion weighted imaging)\nfmap (field inhomogeneity mapping data such as field maps)\nanat (structural imaging such as T1, T2, PD, and so on)\nperf (perfusion)\nmeg (magnetoencephalography)\neeg (electroencephalography)\nieeg (intracranial electroencephalography)\nbeh (behavioral)\npet (positron emission tomography)\nmicr (microscopy)\nData files are contained in a directory named for the data type. In raw datasets, the data type directory is nested inside subject and (optionally) session directories.\n\n\nTask - a set of structured activities performed by the participant. Tasks are usually accompanied by stimuli and responses, and can greatly vary in complexity.\nEvent - something that happens or may be perceived by a test subject as happening at a particular instant during the recording. Events are most commonly associated with on- or offset of stimulus presentations, or with the distinct marker of on- or offset of a subject’s response or motor action.\nRun - an uninterrupted repetition of data acquisition that has the same acquisition parameters and task. Run is a synonym of a data acquisition.\n&lt;index&gt; - a nonnegative integer, possibly prefixed with arbitrary number of 0s for consistent indentation, for example, it is 01 in run-01 following run-&lt;index&gt; specification.\n&lt;label&gt; - an alphanumeric value, possibly prefixed with arbitrary number of 0s for consistent indentation, for example, it is rest in task-rest following task-&lt;label&gt; specification. Note that labels MUST not collide when casing is ignored (see Case collision intolerance).\nsuffix - an alphanumeric value, located after the key-value_ pairs (thus after the final _), right before the File extension, for example, it is eeg in sub-05_task-matchingpennies_eeg.vhdr.\n\nFile name structure\nA filename consists of a chain of entities, or key-value pairs, a suffix and an extension. Two prominent examples of entities are subject and session.\nFor a data file that was collected in a given session from a given subject, the filename MUST begin with the string sub-&lt;label&gt;_ses-&lt;label&gt;. If the session level is omitted in the folder structure, the filename MUST begin with the string sub-&lt;label&gt;, without ses-&lt;label&gt;.\nNote that sub-&lt;label&gt; corresponds to the subject entity because it has the sub- “key” and&lt;label&gt; “value”, where &lt;label&gt; would in a real data file correspond to a unique identifier of that subject, such as 01. The same holds for the session entity with its ses- key and its &lt;label&gt; value.\nThe extra session layer (at least one /ses-&lt;label&gt; subfolder) SHOULD be added for all subjects if at least one subject in the dataset has more than one session. If a /ses-&lt;label&gt; subfolder is included as part of the directory hierarchy, then the same ses-&lt;label&gt; key/value pair MUST also be included as part of the filenames themselves. Acquisition time of session can be defined in the sessions file.\nA chain of entities, followed by a suffix, connected by underscores (_) produces a human readable filename, such as sub-01_task-rest_eeg.edf.\nEntities within a filename MUST be unique. For example, the following filename is not valid because it uses the acq entity twice: sub-01_acq-laser_acq-uneven_electrodes.tsv\nA summary of all entities in BIDS and the order in which they MUST be specified is available in the entity table in the official website appendix.\nEntity-linked file collections\nAn entity-linked file collection is a set of files that are related to each other based on a repetitive acquisition of sequential data by changing acquisition parameters one (or multiple) at a time or by being inherent components of the same data. Entity-linked collections are identified by a common suffix, indicating the group of files that should be considered a logical unit. Within each collection, files MUST be distinguished from each other by at least one entity (for example, echo) that corresponds to an altered acquisition parameter (EchoTime) or that defines a component relationship (for example, part).\nSource vs. raw vs. derived data\nBIDS was originally designed to describe and apply consistent naming conventions to raw (unprocessed or minimally processed due to file format conversion) data. During analysis such data will be transformed and partial as well as final results will be saved. Derivatives of the raw data (other than products of DICOM to NIfTI conversion) MUST be kept separate from the raw data. This way one can protect the raw data from accidental changes by file permissions.\nSimilar rules apply to source data, which is defined as data before harmonization, reconstruction, and/or file format conversion (for example, E-Prime event logs or DICOM files). Storing actual source files with the data is preferred over links to external source repositories to maximize long term preservation, which would suffer if an external repository would not be available anymore. This specification currently does not go into the details of recommending a particular naming scheme for including different types of source data (such as the raw event logs or parameter files, before conversion to BIDS). However, in the case that these data are to be included:\n\n\nThese data MUST be kept in separate sourcedata folder with a similar folder structure as presented below for the BIDS-managed data. For example: sourcedata/sub-01/ses-pre/func/sub-01_ses-pre_task-rest_bold.dicom.tgz or sourcedata/sub-01/ses-pre/func/MyEvent.sce.\n\n\nA README file SHOULD be found at the root of the sourcedata folder or the derivatives folder, or both. This file should describe the nature of the raw data or the derived data. We RECOMMEND including the PDF print-out with the actual sequence parameters generated by the scanner in the sourcedata folder.\n\n\nAlternatively one can organize their data in the following way\n└─ my_dataset-1/\n   ├─ sourcedata/\n   │  ├─ sub-01/\n   │  ├─ sub-02/\n   │  └─ ... \n   ├─ ... \n   └─ derivatives/\n      ├─ pipeline_1/\n      ├─ pipeline_2/\n      └─ ... \n\n\nStorage of derived datasets\nDerivatives can be stored/distributed in two ways:\n\n\nUnder a derivatives/ subfolder in the root of the source BIDS dataset folder to make a clear distinction between raw data and results of data processing. A data processing pipeline will typically have a dedicated directory under which it stores all of its outputs. Different components of a pipeline can, however, also be stored under different subfolders. There are few restrictions on the directory names; it is RECOMMENDED to use the format &lt;pipeline&gt;-&lt;variant&gt; in cases where it is anticipated that the same pipeline will output more than one variant (for example, AFNI-blurring and AFNI-noblurring). For the sake of consistency, the subfolder name SHOULD be the GeneratedBy.Name field in data_description.json, optionally followed by a hyphen and a suffix (see Derived dataset and pipeline description).\nExample of derivatives with one directory per pipeline:\n&lt;dataset&gt;/derivatives/fmriprep-v1.4.1/sub-0001\n&lt;dataset&gt;/derivatives/spm/sub-0001\n&lt;dataset&gt;/derivatives/vbm/sub-0001\n\n\nExample of a pipeline with split derivative directories:\n&lt;dataset&gt;/derivatives/spm-preproc/sub-0001\n&lt;dataset&gt;/derivatives/spm-stats/sub-0001\n\n\nExample of a pipeline with nested derivative directories:\n&lt;dataset&gt;/derivatives/spm-preproc/sub-0001\n&lt;dataset&gt;/derivatives/spm-preproc/derivatives/spm-stats/sub-0001\n\n\n\n\nAs a standalone dataset independent of the source (raw or derived) BIDS dataset. This way of specifying derivatives is particularly useful when the source dataset is provided with read-only access, for publishing derivatives as independent bodies of work, or for describing derivatives that were created from more than one source dataset. The sourcedata/ subdirectory MAY be used to include the source dataset(s) that were used to generate the derivatives. Likewise, any code used to generate the derivatives from the source data MAY be included in the code/ subdirectory.\nExample of a derivative dataset including the raw dataset as source:\n\n\n└─ my_processed_data/\n   ├─ code/\n   │  ├─ processing_pipeline-1.0.0.img \n   │  ├─ hpc_submitter.sh \n   │  └─ ... \n   ├─ sourcedata/\n   │  ├─ sub-01/\n   │  ├─ sub-02/\n   │  └─ ... \n   ├─ sub-01/\n   ├─ sub-02/\n   └─ ... \n\n\nFile Formation specification\nImaging files\nAll imaging data MUST be stored using the NIfTI file format. We RECOMMEND using compressed NIfTI files (.nii.gz), either version 1.0 or 2.0. Imaging data SHOULD be converted to the NIfTI format using a tool that provides as much of the NIfTI header information (such as orientation and slice timing information) as possible. Since the NIfTI standard offers limited support for the various image acquisition parameters available in DICOM files, we RECOMMEND that users provide additional meta information extracted from DICOM files in a sidecar JSON file (with the same filename as the .nii[.gz] file, but with a .json extension). Extraction of BIDS compatible metadata can be performed using dcm2niix and dicm2nii DICOM to NIfTI converters. The BIDS-validator will check for conflicts between the JSON file and the data recorded in the NIfTI header.\nTabular files\nTabular data MUST be saved as tab delimited values (.tsv) files, that is, CSV files where commas are replaced by tabs. Tabs MUST be true tab characters and MUST NOT be a series of space characters. Each TSV file MUST start with a header line listing the names of all columns (with the exception of physiological and other continuous recordings). Names MUST be separated with tabs. It is RECOMMENDED that the column names in the header of the TSV file are written in snake_case with the first letter in lower case (for example, variable_name, not Variable_name). String values containing tabs MUST be escaped using double quotes. Missing and non-applicable values MUST be coded as n/a. Numerical values MUST employ the dot (.) as decimal separator and MAY be specified in scientific notation, using e or E to separate the significand from the exponent. TSV files MUST be in UTF-8 encoding.\nExample:\nonset   duration    response_time   correct stop_trial  go_trial\n200 200 0   n/a n/a n/a\n\n\nKey/value files (dictionaries)\nJavaScript Object Notation (JSON) files MUST be used for storing key/value pairs. JSON files MUST be in UTF-8 encoding. Extensive documentation of the format can be found at www.json.org/, and at tools.ietf.org/html/std90. Several editors have built-in support for JSON syntax highlighting that aids manual creation of such files. An online editor for JSON with built-in validation is available at jsoneditoronline.org. It is RECOMMENDED that keys in a JSON file are written in CamelCase with the first letter in upper case (for example, SamplingFrequency, not samplingFrequency). Note however, when a JSON file is used as an accompanying sidecar file for a TSV file, the keys linking a TSV column with their description in the JSON file need to follow the exact formatting as in the TSV file.\nExample of a hypothetical *_bold.json file, accompanying a *_bold.nii file:\n{\n  &quot;RepetitionTime&quot;: 3,\n  &quot;Instruction&quot;: &quot;Lie still and keep your eyes open&quot;\n}\n\n\nExample of a hypothetical *_events.json file, accompanying an *_events.tsv file. Note that the JSON file contains a key describing an arbitrary column stim_presentation_side in the TSV file it accompanies.\n{\n  &quot;stim_presentation_side&quot;: {\n    &quot;Levels&quot;: {\n      &quot;1&quot;: &quot;stimulus presented on LEFT side&quot;,\n      &quot;2&quot;: &quot;stimulus presented on RIGHT side&quot;\n    }\n  }\n}\n\n\nDirectory structure\nSingle session example\nThis is an example of the folder and file structure. Because there is only one session, the session level is not required by the format. For details on individual files see descriptions in the next section:\n├─ sub-control01/\n│  ├─ anat/\n│  │  ├─ sub-control01_T1w.nii.gz \n│  │  ├─ sub-control01_T1w.json \n│  │  ├─ sub-control01_T2w.nii.gz \n│  │  └─ sub-control01_T2w.json \n│  ├─ func/\n│  │  ├─ sub-control01_task-nback_bold.nii.gz \n│  │  ├─ sub-control01_task-nback_bold.json \n│  │  ├─ sub-control01_task-nback_events.tsv \n│  │  ├─ sub-control01_task-nback_physio.tsv.gz \n│  │  ├─ sub-control01_task-nback_physio.json \n│  │  └─ sub-control01_task-nback_sbref.nii.gz \n│  ├─ dwi/\n│  │  ├─ sub-control01_dwi.nii.gz \n│  │  ├─ sub-control01_dwi.bval \n│  │  └─ sub-control01_dwi.bvec \n│  └─ fmap/\n│     ├─ sub-control01_phasediff.nii.gz \n│     ├─ sub-control01_phasediff.json \n│     └─ sub-control01_magnitude1.nii.gz \n├─ code/\n│  └─ deface.py \n├─ derivatives/\n├─ README \n├─ participants.tsv \n├─ dataset_description.json \n└─ CHANGES \n\n\nUnspecified data\nAdditional files and folders containing raw data MAY be added as needed for special cases. All non-standard file entities SHOULD conform to BIDS-style naming conventions, including alphabetic entities and suffixes and alphanumeric labels/indices. Non-standard suffixes SHOULD reflect the nature of the data, and existing entities SHOULD be used when appropriate. For example, an ASSET calibration scan might be named sub-01_acq-ASSET_calibration.nii.gz.\nNon-standard files and directories should be named with care. Future BIDS efforts may standardize new entities and suffixes, changing the meaning of filenames and setting requirements on their contents or metadata. Validation and parsing tools MAY treat the presence of non-standard files and directories as an error, so consult the details of these tools for mechanisms to suppress warnings or provide interpretations of your filenames.\nBIDS common data types\nEach derivative data file SHOULD be described by a JSON file provided as a sidecar or higher up in the hierarchy of the derived dataset (according to the Inheritance Principle) unless a particular derivative includes REQUIRED metadata fields, in which case a JSON file is also REQUIRED. Each derivative type defines their own set of fields, but all of them share the following (non-required) ones:\nExamples\nPreprocessed bold NIfTI file in the original coordinate space of the original run. The location of the file in the original datasets is encoded in the RawSources metadata, and _desc-&lt;label&gt; is used to prevent clashing with the original filename.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_desc-preproc_bold.nii.gz \n      └─ sub-01_task-rest_desc-preproc_bold.json \n\n\n{\n    &quot;RawSources&quot;: [&quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;]\n}\n\n\nIf this file was generated with prior knowledge from additional sources, such as the same subject’s T1w, then both files MAY be included in RawSources.\n{\n    &quot;RawSources&quot;: [\n        &quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;,\n        &quot;sub-01/anat/sub-01_T1w.nii.gz&quot;\n    ]\n}\n\n\nOn the other hand, if a preprocessed version of the T1w image was used, and it also occurs in the derivatives, Sources and RawSources can both be specified.\n{\n    &quot;Sources&quot;: [\n        &quot;sub-01/anat/sub-01_desc-preproc_T1w.nii.gz&quot;\n    ],\n    &quot;RawSources&quot;: [\n        &quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;\n    ]\n}\n\n\nSpatial references\nDerivatives are often aligned to a common spatial reference to allow for the comparison of acquired data across runs, sessions, subjects or datasets. A file may indicate the spatial reference to which it has been aligned using the space entity and/or the SpatialReference metadata.\nThe space entity may take any value in Image-Based Coordinate Systems.\nIf the space entity is omitted, or the space is not in the Standard template identifiers table, then the SpatialReference metadata is REQUIRED.\nSpatialReference key allowed values\nIn the case of images with multiple references, an object must link the relevant structures to reference files. If a single volumetric reference is used for multiple structures, the VolumeReference key MAY be used to reduce duplication. For CIFTI-2 images, the relevant structures are BrainStructure values defined in the BrainModel elements found in the CIFTI-2 header.\nExamples\nPreprocessed bold NIfTI file in individual coordinate space. Please mind that in this case SpatialReference key is REQUIRED.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_space-individual_bold.nii.gz \n      └─ sub-01_task-rest_space-individual_bold.json \n\n\n{\n    &quot;SpatialReference&quot;: &quot;sub-01/anat/sub-01_desc-combined_T1w.nii.gz&quot;\n}\n\n\nPreprocessed bold CIFTI-2 files that have been sampled to the fsLR surface meshes defined in the Conte69 atlas along with the MNI152NLin6Asym template. In this example, because all volumetric structures are sampled to the same reference, the VolumeReference key is used as a default, and only the surface references need to be specified by BrainStructure names.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_space-fsLR_den-91k_bold.dtseries.nii \n      └─ sub-01_task-rest_space-fsLR_den-91k_bold.json \n\n\n{\n    &quot;SpatialReference&quot;: {\n        &quot;VolumeReference&quot;: &quot;templateflow.s3.amazonaws.com/tpl-MNI152NLin6Asym_res-02_T1w.nii.gz&quot;,\n        &quot;CIFTI_STRUCTURE_CORTEX_LEFT&quot;: &quot;github.com/mgxd/brainplot/raw/master/brainplot/Conte69_Atlas/Conte69.L.midthickness.32k_fs_LR.surf.gii&quot;,\n        &quot;CIFTI_STRUCTURE_CORTEX_RIGHT&quot;: &quot;github.com/mgxd/brainplot/raw/master/brainplot/Conte69_Atlas/Conte69.R.midthickness.32k_fs_LR.surf.gii&quot;\n    }\n}\n\n\nOther important specifications can be found in official website. For examples:\nPreprocessed or cleaned data\nBIDS derivatives"},"fMRI-Tutorial/2.3-fmriPrep-with-Docker":{"slug":"fMRI-Tutorial/2.3-fmriPrep-with-Docker","filePath":"content/fMRI Tutorial/2.3 fmriPrep with Docker.md","title":"2.3 fmriPrep with Docker","links":["[docs.docker.com](https:/docs.docker.com/desktop/setup/install/windows-install/utm_source=chatgpt.com)"],"tags":["mri","fmriPrep","fMRI"],"content":"\n\n                  \n                  Overview \n                  \n                \n\nThis is the initial step of MRI data processing after you have organized your data in the BIDS structure. This tutorial guides you through\n\nInstallation of Docker and Neurodesktop image\nQuality check with MRIQC\nPreprocessing with fMRIPrep\n\n\n\n\n\n                  \n                  Requirement\n                  \n                \n\n\nRAM : ≥ 16 GB (MRIQC/fMRIPrep are memory‑hungry)\nDISK: ≥ 20 GB free (container + derivatives)\nA FreeSurfer licence file (license.txt)—register free, save it in the project root.\n\n\n\n1 · Install Docker\nWindows / macOS (Docker Desktop)\n\nDownload the Docker Desktop installer from the official docs.\nRun the installer → accept WSL 2 backend on Windows if prompted.\nAfter reboot open Docker Desktop and ensure the whale icon is running in the system tray.\n\nVerify:\ndocker run hello-world\n2 · Pull &amp; launch Neurodesktop\nInstructions on installing and using the app: www.neurodesk.org/docs/getting-started/neurodesktop/neurodeskapp/\nLaunching the Neurodesk App\nThe Neurodesk App can be launched directly from your operating system’s application menu, or by running the neurodeskapp command in the command line.\nAdd  a custom Data Directory\nBy default, /home/jovyan/neurodesktop-storage in the app (which is bound with local directory ~/neurodesktop-storage in Unix/MacOS or C:/neurodesktop-storage in Windows)\nBy choice, in the settings window below, select Additional Directory on the left side bar, click Change button to select the local directory, then click Apply &amp; restart. The next time you start the app, the data from the local directory can be found in /home/jovyan/data. This is where you can read and analyze your MRI data.\n\n\n\n                  \n                  Info\n                  \n                \n\nIf you are using Windows it is currently not possible to mount external hard drives. We recommend copying data from the external drive to your local disk first and then processing it in Neurodesk.\n\n\nAfter you successfully added an external folder, you should see a similar interface as follows. If you are familiar with the Jupyter Notebook interface, you can directly work here. If you prefer Desktop, then you can double click on Neurodesktop, it will open a virtual linux desktop for you.\n\nWithin the Neurodesktop, you can open related Apps.\n\nUnderstand Neurodesktop Modules\n\n\n                  \n                  What are modules? \n                  \n                \n\nNeurodesktop exposes each neuroimaging tool as an Lmod “module” backed by Singularity containers served over CVMFS.\n\n\nLocation\nModules live on CVMFS under /cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/, with one folder per tool/version (e.g. fmriprep/2025-06-10)  oai_citation:0‡GitHub.\nMODULEPATH\nThe MODULEPATH environment variable points to /cvmfs/neurodesk.ardc.edu.au/containers/modules/, so that Lmod can find and load the correct wrapper scripts automatically  oai_citation:1‡Docker Hub.\nLoading modules\nIn any Neurodesktop terminal you can manually initialize and inspect available tools:\n#module use /cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/\nmodule avail                     # list all tools/versions\nmodule load fmriprep/latest     # load fmriprep into your env\n3  Code inside the container with VS Code\nThe most handy App is Visual Studio Code. If you are familiar with VS Code, this would be your choice.\n\n4. Preprocessing with fMRIPrep\nfMRIPrep is a preprocessing pipeline for functional magnetic resonance imaging (fMRI) data. It offers an accessible, state-of-the-art interface that handles variations in scan protocols with minimal user input. The pipeline provides clear and comprehensive error and output reports. It performs essential processing steps, including coregistration, normalization, unwarping, noise component extraction, segmentation, and skull stripping. The outputs are easily suitable for various group-level analyses, including task-based and resting-state fMRI, graph theory measures, and surface or volume-based approaches.\nA standard preprocessing pipeline of fMRI data looks as follows:\nflowchart TD\n\tA[Raw Data] --&gt; B[Distortion corretion]\n\tB --&gt; C[Slice-Timing correction]\n\tA --&gt; C\n\tC --&gt; D[Registration and Normalization]\n\tD --&gt; E[Alignment and Motion Correction]\n\tE --&gt; F[Spatial Smoothing]\n\tF --&gt; G[Masking and Scaling]\n\tG --&gt; I[Checking preprocessing]\n\n\nThe fMRIPrep workflow takes as principal input the path of the dataset that is to be processed. The input dataset is required to be in valid BIDS format, and it must include at least one T1w structural image and (unless disabled with a flag) a BOLD series.\nThe standard parts of the command follow the BIDS-Apps definition, such as:\nfmriprep data/bids_root/ out/ participant -w work. \n\n4.1 Using module fMRIPrep\nIn the Neurodesk container, you can either edit the bash file (preferred) or run it directly with the command line.\nmodule load fmriprep # load fmriprep module\n \n#specify input BIDS folder and output folder, as well as working folder (storing temperary files)\nBIDS_DIR=~/data/ncp_2025\nOUTPUT_DIR=$BIDS_DIR/derivatives/fmriprep\nWORK_DIR=~/data/work\n \nNext, you can run fmriprep directly. For example, the following command runs preprocessing for the given BIDS_DIR with subject ID {001, 002, 003}, without Freesurfer surface preprocessing, and output spaces are MNI152NLin2009cAsym and local T1w spaces. It will use up to 6 threads with 16 GB of memory.\nfmriprep $BIDS_DIR $OUTPUT_DIR participant \\\n  --participant-label 001 002 003 \\\n  --skip-bids-validation \\\n  --fs-license-file $BIDS_DIR/license.txt \\\n  --fs-no-reconall \\\n  --output-spaces MNI152NLin2009cAsym T1w \\\n  --nthreads 6 \\\n  --mem_mb 16000 \\\n  --work-dir $WORK_DIR \\\n  --verbose\n4.2 Use Singularity\nIf you set up a dedicated Linux server with Singularity installed, you can download the fmriPrep image and run it remotely. Here is a typical code for running fmriprep (assuming you have specified the image location with preimg).\n \nsingularity run \\\n\t--bind $bids_dir \\\n\t--bind $scratch_dir \\\n\t--bind $dss_dir \\\n\t$prepimg $bids_dir $bids_dir/derivatives/fmriprep \\\n\tparticipant \\\n\t--participant-label ${participants_to_run[*]} \\\n\t--skip-bids-validation \\\n\t--fs-license-file $proj_dir/fsl_licence.txt \\\n\t--output-spaces MNI152NLin2009cAsym T1w fsnative\\\n\t--bold2t1w-init register \\\n\t--bold2t1w-dof 12 \\\n\t--fs-no-reconall \\\n\t--force-bbr \\\n\t--skull-strip-t1w force \\\n\t--nthreads 56 \\\n\t--omp-nthreads 12 \\\n\t--mem_mb 56000 \\\n\t--stop-on-first-crash \\\n\t--notrack \\\n\t-v \\\n\t-w $scratch_dir\n4.3 Run with Linux cluster slurm\nAdditionally, it is possible to run fMRIPrep using Neurocommand on a Linux cluster.\nThe following is an example of running parallel computing via Linux cluster slurm via LRZ linux cluster.\n#!/bin/bash\n#SBATCH --job-name=fmriprep\n#SBATCH --get-user-env\n#SBATCH --clusters=cm4\n#SBATCH --partition=cm4_tiny\n#SBATCH -t 12:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem=32G\n#SBATCH --array=01-23%4\n#SBATCH --mail-user=your@email\n#SBATCH --mail-type=BEGIN,END,FAIL\n \n#SBATCH -o /dss/youraccount_folder/fmripre-%A_%a.log\n# for linux cluster interactive\n \nmodule load slurm_setup\nmodule use $HOME/spack/modules/x86_avx2/linux-sles15-haswell\nmodule load singularity squashfs\nmodule load fmriprep\n \n# Print job submission info\necho &quot;Slurm job ID: &quot; $SLURM_JOB_ID\ndate\n \n#Directory\n \nBIDS_DIR=$HOME/dss/studies/Your_Study\nOUT_DIR=$BIDS_DIR/derivatives/fmriprep\n \nWORK_DIR=$SCRATCH/fmriprep_work\n \nmkdir -p &quot;$WORK_DIR&quot; &quot;$OUT_DIR&quot;\n \nexport MPLCONFIGDIR=$SCRATCH #matplotlib temp folder\n \nSUBJECT=$(printf &quot;%03d&quot; $SLURM_ARRAY_TASK_ID)\n \nfmriprep \\\n\t&quot;$BIDS_DIR&quot; &quot;$OUT_DIR&quot; participant \\\n\t--participant-label &quot;$SUBJECT&quot; \\\n\t--skip-bids-validation \\\n\t--fs-license-file $HOME/dss/fsl_licence.txt \\\n\t--output-spaces MNI152NLin2009cAsym T1w\\\n\t--fs-no-reconall \\\n\t--nthreads 32 \\\n\t--mem 32000 \\\n\t--stop-on-first-crash \\\n\t-vv \\\n\t-w $SCRATCH\n \necho &quot;Job $SLURM_JOB_ID finished at $(date)&quot;\n\n\n                  \n                  Note\n                  \n                \n\nFor multi-line commands, you need to add \\ at the end of each line, and be aware that you should NOT add any blank spaces after \\.\n\n\n5 Check fMRIPrep outputs &amp; confounds\n\n\n                  \n                  Summary\n                  \n                \n\nGoal: decide which nuisance signals to keep (and how many) before denoising.\nWe’ll walk through the HTML report, inspect the confounds_regressors.tsv, and script our own plots using nilearn.interfaces.fmriprep.load_confounds.\n\n\nBy default, the fMRIPrep will generate output at subfolder /derivatives/fmriprep:\n└─ derivatives/fmriprep/\n   ├─ logs/\n   ├─ dataset_description.json\n   ├─ sub-001/\n   │  ├─ anat/\n   │  ├─ figures/\n   │  ├─ func/\n   │  └─ log/ \n   ├─ sub-001.html (visual report)\n   └─ ... \n\n\n5.1 Locate the QA material\n\nEach subject has a visual report at\n&lt;derivatives&gt;/fmriprep/sub-&lt;ID&gt;.html, plus figures inside sub-&lt;ID&gt;/figures/.  oai_citation:0‡fmriprep.org  It includes anatomy and functional images reports.\n\nThe first step is to scroll to the bottom and check if there are any error reports.\nNext, verify the brain masks, spatial normalization, and coregistration of functional and anatomical MRI.\nVisual Report: Functional Section\nThis section is most important for fMRI studies. It includes a summary that describes the functional data. The collected confounds are listed for each run.\n\nBrain mask and (anatomical/temporal) CompCor ROIs.\n\nHere, aCompCor (magenta contour) and tCompCor (Blue contour pick the top 2% most variable voxels within the brain masks) aim to model physiological and other voxel-wise variance using PCA over CSF/WM masks or high-variance voxels. Additionally, the brain edge (or crown) ROI is marked as a green contour. Verify these contours if any outliers occur. In addition to this, the report’s “Variance explained by a/crown/tCompCor components” panel shows cumulative variance curves for every CompCor mask, showing the number of components that must be included in a model to explain some fraction of variance (50%, 70% or 90%).\n\nThe most important subsection is BOLD Summary panel. One example as the below.\n\nFD measures the sum of absolute head-motion derivatives between frames (Power et al., 2012).  oai_citation:9‡PMC\nDVARS is the root-mean-square signal change across all voxels; spikes above ≈5 % signal are typical flags.  oai_citation:10‡PMC\nfMRIPrep marks volumes with FD &gt; 0.5 mm or stdDVARS &gt; 1.5 as motion_outlierXX by default (tune with --fd-spike-threshold, --dvars-spike-threshold).  oai_citation:11‡Neurostars\n\n\nThe bottom carpet plot visualize the entire BOLD time series across all voxels as a 2D matrix (time x voxels), enabling rapid assessment of data quality, physiological artifacts, and motion-related spikes.\nOverlaid confound traces (e.g., Framewise Displacement, DVARS, global signal, WM, CSF) help link visual anomalies to quantitative metrics. By inspecting patterns—such as vertical streaks for sudden motion, horizontal bands for scanner drifts, or crown-region fluctuations—you can decide whether to apply censoring, adjust regressors, or flag problematic runs.\nInterpreting Patterns:\n\nMotion Artifacts: Vertical stripes across all rows at a single time point indicate a sudden head-motion event; corroborate with FD spikes\nPhysiological Noise: Oscillatory horizontal bands within CSF or WM regions reflect cardiac or respiratory cycles; these may persist as rhythmic patterns.\nScanner Drifts:  Gradual shifts in baseline intensity across all tissues may indicate scanner instability; appear as sloped color transitions.\nCrown Effects: The “crown” rows (outer brain edge) often show artifactual jumps from head or neck movements; flagging crown-specific stripes can isolate edge-based noise\n\nFinally, a heat-map titled “Correlations among nuisance regressors” visualises their pair-wise r plus correlation to GS.  oai_citation:3‡fmriprep.org\n5.2 Understand what’s stored\n\nAll confounds live in\nsub-&lt;ID&gt;_task-&lt;task&gt;_run-&lt;run&gt;_desc-confounds_timeseries.tsv. Columns include std_dvars, framewise_displacement, 6 rigid-body parameters, t/a CompCor PCs, global/WM/CSF signals and any motion outliers.  oai_citation:4‡fmriprep.org\nJSON sidecars add metadata such as CumulativeVarianceExplained for every CompCor component, so you can choose k components that explain (say) 50 % variance.  oai_citation:5‡fmriprep.org\nfMRIPrep keeps only the PCs needed to reach 50 % by default (--return-all-components overrides).  oai_citation:6‡Neurostars oai_citation:7‡Neurostars\n"},"fMRI-Tutorial/2.4-Quality-Control-with-MRIQC":{"slug":"fMRI-Tutorial/2.4-Quality-Control-with-MRIQC","filePath":"content/fMRI Tutorial/2.4 Quality Control with MRIQC.md","title":"2.4 Quality Control with MRIQC","links":[],"tags":["mri","MRIQC"],"content":"MRIQC extracts no-reference IQMs (image quality metrics) from structural (T1w and T2w) and functional MRI (magnetic resonance imaging) data.\nMRIQC is an open-source project, developed under the following software engineering principles:\n\n\nModularity and integrability: MRIQC implements a nipype workflow to integrate modular sub-workflows that rely upon third party software toolboxes such as ANTs and AFNI.\n\n\nMinimal preprocessing: the MRIQC workflows should be as minimal as possible to estimate the IQMs on the original data or their minimally processed derivatives.\n\n\nInteroperability and standards: MRIQC follows the the brain imaging data structure (BIDS), and it adopts the BIDS-App standard.\n\n\nReliability and robustness: the software undergoes frequent vetting sprints by testing its robustness against data variability (acquisition parameters, physiological differences, etc.) using images from OpenfMRI. Its reliability is permanently checked and maintained with CircleCI.\n\n\nA sample script to run mriqc via singularity:\n#!/bin/bash\n \n# check if in a cluster environment\nif env | grep -i cluster &gt; /dev/null; then\n    # 2. project directory (depending on the mount folder)\n    proj_dir=/path/to/project_id/ #linux cluster\n    ncpus=8\n    mriqc_img=$HOME/mriqc23\nelse\n    # 2. project directory (at lora server)\n    proj_dir=/dss/project_folder/ #Lora virtual machine\n    ncpus=8\n    mriqc_img=$HOME/mriqc23.simg\nfi\n \n# project name\nproj_name=bisection_mri\n \n# 3. working directory \nscratch_dir=$proj_dir/tmp/$proj_name\n \nexport MPLCONFIGDIR=$scratch_dir  #matplotlib temp folder\n \n# check if the folder exists\nif [ ! -d &quot;$scratch_dir&quot; ]; then\n    mkdir &quot;$scratch_dir&quot;\nfi\n# 4. bids directory\nbids_dir=$proj_dir/$proj_name\n \n# linux cluster sandbox \nsingularity run \\\n    --bind $bids_dir \\\n    --bind $scratch_dir \\\n    $mriqc_img $bids_dir $bids_dir/derivatives/mriqc \\\n    participant \\\n    --participant-label $1 \\\n    --no-sub \\\n    --nprocs $ncpus \\\n    --omp-nthreads 8 \\\n    --verbose-reports \\\n    -w $scratch_dir\n \nAfter running subject-wise, you can run group analysis, which will generate a group summary.\nRunning MRIQC for the first time\nBecause of a weird quirk in how MRIQC uses TemplateFlow, the first time each user runs MRIQC on the PNI server, MRIQC will need access to the internet to download some TemplateFlow files. The problem is that our compute nodes (i.e. the nodes used by Slurm) do not have access to the internet. However, the head node does. What this means is the first time each individual user runs MRIQC, you should run it on the head node, not using Slurm. After that, any subsequent time you run MRIQC (even if it is for a different project), you can use Slurm.\nHow to mark outliers\nKey indicators are framewise displacement (FD), temporal SNR, the spatial root mean square after temporal differencing (DVARS). Rule of thumbs: FD &gt; 0.3, DVARS &gt; 50, temporal SNR &lt; 30."},"fMRI-Tutorial/2.5-fMRIPrep-and-FitLins":{"slug":"fMRI-Tutorial/2.5-fMRIPrep-and-FitLins","filePath":"content/fMRI Tutorial/2.5 fMRIPrep and FitLins.md","title":"2.5 fMRIPrep and FitLins","links":["fMRI-Tutorial/2.3-fmriPrep-with-Docker"],"tags":["fMRIPrep","FitLins"],"content":"1. Preprocessing with fMRIPrep\nfMRIPrep is a robust and standardized pipeline for fMRI data. It handles tasks such as skull-stripping, motion correction, distortion correction, and registration to standard space—all while creating extensive reports for quality control.\nfMRIPrep is included in NeuroDesk and can be loaded by module command.\nmodule load fmriprep\nmodule list\nIt shows:\nCurrently Loaded Modules:\n  1) fmriprep/24.1.1\n\nNow the command fmriprep is available for further analysis.\n1.1 Batch process individual participants\nHere is a a typical bash code using fmriprep:\n#!/bin/bash\n \n#1. first load the module fmriprep\nmodule load fmriprep\n \n# setup input and output folders\nproj_dir=/dss/open/ds000003 # change to your project directory\n# preprossed data directory\noutput=${proj_dir}/derivatives \n \nfmriprep $proj_dir $output \\\n\tparticipant\\\n\t--participant-label 01\\\n\t-w /dss/tmp \\\n\t--fs-license-file /dss/fsl_licence.txt \\\n\t--fs-no-reconall \\\n\t--output-spaces T1w MNI152NLin2009cAsym\n\t-v\nParameters Explained:\n\n—participant-label: Select which subjects to process.\n—fs-no-reconall: if you do not wish to run FreeSurfer’s recon-all\n—output-space: Set your target standard space\n-w working directory\n-v verbose output\n\n1.2 Inspecting fMRIPrep Outputs\n\n\nQuality Control:\nOpen the HTML reports in your web browser to check the alignment and quality of preprocessing.\n\n\nOutputs:\nYour preprocessed images and confound regressors (confounds.tsv) are ready for statistical analysis.\n\n\n2. Defining the Statistical Model with BIDS-StatsModel\nThe official website provides detailed specifications. BIDS Stats Models describe how to fit statistical models for neuroimaging data using a machine-readable JSON document.  The following instructions primarily come from the official website.\nThis model focuses on general linear mixed models (GLMs) with the data input from fMRIPrep.\n2.1 Representing multi-stage neuroimaging models\nStatistical analysis of neuroimaging data occurs in stages, with parameter estimates from lower levels propagating to higher levels. In fMRI, we typically fit a design matrix to time series data, followed by a fixed-effects model for subject-level estimates, culminating in a dataset-level random-effects one-sample t-test to assess population effects. At each analysis level, we must identify image inputs linked to the design matrix and manage outputs from previous levels. BIDS Stats Models offers a machine-readable document to describe multi-stage neuroimaging analyses accurately and flexibly. This is achieved through a graph of Nodes representing each analysis level and Edges that indicate data flow between Nodes. Each Node includes a Model for estimation and at least one Contrast for computed outputs, along with a GroupBy directive for input organization.\nAn example from the official website:\nIn a Simon task, participants were scanned across 2 runs to indicate if a diamond presented to the left or right of a fixation cross was green or red. There were two conditions: color-spatial congruent and incongruent trials. The analysis aims to determine regions with greater activity for incongruent versus congruent trials across participants.\nFirst, we model run-level timeseries for separate “Incongruent” and “Congruent” trials for each run. Then, we compute the contrast for Incongruent &gt; Congruent (IvC) trials. This contrasts then goes to a subject-level estimator, averaging the IvC effect per subject. Lastly, we send these estimates to a dataset-level estimator for a one-sample t-test across the IvC contrast subject estimates.\nThis is a visualization how the analysis goes for 3 participants:\n\nWe can formulate this analysis using BIDS Stats Model using json:\n{\n  &quot;Name&quot;: &quot;Simon IvC&quot;,\n  &quot;BIDSModelVersion&quot;: &quot;1.0.0&quot;,\n  &quot;Input&quot;: {&quot;subject&quot;: [&quot;01&quot;, &quot;02&quot;, &quot;03&quot;], &quot;task&quot;: &quot;simon&quot;},\n  &quot;Nodes&quot;: [\n    {\n      &quot;Level&quot;: &quot;Run&quot;,\n      &quot;Name&quot;: &quot;run_level&quot;,\n      &quot;GroupBy&quot;: [&quot;run&quot;, &quot;subject&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1, &quot;incongruent&quot;, &quot;congruent&quot;], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;Contrasts&quot;: [\n        {\n          &quot;Name&quot;: &quot;IvC&quot;,\n          &quot;ConditionList&quot;: [&quot;incongruent&quot;, &quot;congruent&quot;],\n          &quot;Weights&quot;: [1, -1],\n          &quot;Test&quot;: &quot;t&quot;\n        }\n      ]\n    },\n    {\n      &quot;Level&quot;: &quot;Subject&quot;,\n      &quot;Name&quot;: &quot;subject_level&quot;,\n      &quot;GroupBy&quot;: [&quot;subject&quot;, &quot;contrast&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;meta&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\n    },\n    {\n      &quot;Level&quot;: &quot;Dataset&quot;,\n      &quot;Name&quot;: &quot;one-sample_dataset&quot;,\n      &quot;GroupBy&quot;: [&quot;contrast&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\n    }\n  ]\n}\nAs you can, there are three levels of analysis: Run, Subject, Dataset. At the Run level, there are many sources of possible variables, and the most important event file _events.tsv.\n\n\n                  \n                  Tip\n                  \n                \n\nIn Visual Studio Code, Json Editor plugin can help you see a json in a simple tree view.\n\n\nThe model is defined by Model:\n&quot;Model&quot;: {&quot;X&quot;: [1, &quot;incongruent&quot;, &quot;congruent&quot;], &quot;Type&quot;: &quot;glm&quot;},\nwhere X defines variables in the design matrix, here are the constant, incongruent, and congruent conditions. The contrast analysis is sepcified by Contrast:\n&quot;Contrasts&quot;: [\n        {\n          &quot;Name&quot;: &quot;IvC&quot;,\n          &quot;ConditionList&quot;: [&quot;incongruent&quot;, &quot;congruent&quot;],\n          &quot;Weights&quot;: [1, -1],\n          &quot;Test&quot;: &quot;t&quot;\n        }\n      ]\nContrasts also define the outputs for the next level of analysis. You need to add contrast in GroupBy.\nAnother example\n{\n  &quot;Level&quot;: &quot;Run&quot;,\n  ...\n  &quot;Contrasts&quot;: [\n    {\n      &quot;Name&quot;: &quot;weighted sum&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;,\n                 &quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [1, &quot;-1/3&quot;, &quot;-1/3&quot;, &quot;-1/3&quot;]\n    },\n    {\n      &quot;Name&quot;: &quot;t-test 1&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;],\n      &quot;Weights&quot;: [1, -1],\n      &quot;Test&quot;: &quot;t&quot;\n    },\n    {\n      &quot;Name&quot;: &quot;t-test 2&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [1, -1],\n      &quot;Test&quot;: &quot;t&quot;\n    },\n    {\n      &quot;Name&quot;: &quot;My favorite F-test&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;,\n                 &quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [[1, -1, 0, 0], [0, 0, -1, 1]]\n      &quot;Test&quot;: &quot;F&quot;\n    }\n  ]\n}\nAt the subject level, we estimate the mean effect for each subject, so we only need an intercept and using DummyConstrasts.\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;meta&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\nThe final group analysis, called Dataset level, doing similar analysis:\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\nNote, the difference between subject-level run averaging and group-level is meta vs. glm.\n\n\n                  \n                  Info\n                  \n                \n\nThere are two types of first-level (i.e., Run level) glm: Nilearn’s FirstLevelModel (default) and AFNI’s 3dREMLfit. If you want to use the latter, specify --estimator afni during the command line fitlins. The difference is that Nilearn treats the residuals not autocorrelated, while AFNI uses an ARMI(1,1) model to each voxel to account for auto-correlated time series noise.\nFor the second-level analysis (e.g., Subject or Dataset), meta indicates a fixed-effects combination using Nilearn compute_fixed_effects(), while glm uses Nilearn’s SecondLevelModel estimator.\n\n\nAt present, the BIDS Stats Model spec assumes the same sequential hierarchy. Thus, the following node sequences are all valid:\n\nrun → subject → dataset\nrun → session → subject → dataset\nrun → subject\nsubject → dataset\n\nEach analysis node has fields defined in Node. The Modelspecification MUST be invoked prior to Constrasts and DummyContrasts.\nIf &quot;run&quot; is omitted, then all runs for each subject will be fit with a single model; likewise, if &quot;subject&quot; is omitted, then one model will be fit for each run, combining all subjects for higher levels than the current node.\nEnumerating Sequences of Nodes Using Edges\nEach Edge describes a single connection between two Nodes, and a combination of edges describes a sequence of Nodes.  We will see this in detailed models later.\nInputs and Outputs chaining\nImage inputs to a BIDS Model are minimally specified; However, inputs can be filtered based on keywords defined in the core BIDS spec, including task, run, session, and subject.  For example:\n  &quot;Name&quot;: &quot;my_analysis&quot;,\n  &quot;BIDSModelVersion&quot;: &quot;1.0.0&quot;,\n  &quot;Input&quot;: {\n    &quot;task&quot;: &quot;suppression&quot;,\n    &quot;run&quot;: 1\n  },\n  &quot;Nodes&quot;: [...],\n  &quot;Edges&quot;: [...],\n}\nIn this example, we want to analyze images only for the suppression task, with a run index of 1, and exclude others (see below).\n\nAll outputs from each node are automatically passed on as the inputs to the next node. Specifically, each node receives as input from the previous node any contrasts defined within the Contrasts sections of the previous node. Multiple lower-level inputs that share the same contrast name are automatically concatenated when they are read in.\n2.2 Design-level and scan-level variables\nAccording to the core BIDS spec, each analysis level (i.e., run, session, subject, or dataset) links to specific BIDS files containing optional design events or variables. These files should be automatically read and available for model specification. The core BIDS spec outlines here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevelBIDS file with variablesRun_events.tsv file corresponding to the image file  _stim.tsv.gz file corresponding to the image file  _physio.tsv.gz file corresponding to the image file  _timeseries.tsv file corresponding to the image fileSession_scans.tsv file corresponding to the sessionSubject_sessions.tsv file corresponding to the subjectDataset_participants.tsv corresponding to the entire dataset\nWithin each Node, the expectation is that the namespace should automatically include all variables found within the corresponding file(s) defined in the table. For example, if the events.tsv files for a given project contain columns named “face” and “house”, then, inside any node with Level=&quot;Run&quot;, the user can automatically use the names &quot;face&quot; and &quot;house&quot; anywhere in the Model specification.\nTransformations\nThe Transformation section of a Node allows one to specify transformations of variables that should be applied prior to constructing a design matrix. For example:\n&quot;Transformations&quot;:{\n\t&quot;Transformer&quot;:&quot;pybids-transforms-v1&quot;,\n\t&quot;Instructions&quot;:[\t\n\t\t{\n\t\t\t&quot;Name&quot;:&quot;Factor&quot;,\n\t\t\t&quot;Input&quot;:[&quot;trial_type&quot;]\n\t\t},\n\t\t{\n\t\t\t&quot;Name&quot;:&quot;Convolve&quot;,\n\t\t\t&quot;Input&quot;:[\n\t\t\t\t&quot;trial_type.word&quot;,\n\t\t\t\t&quot;trial_type.pseudoword&quot;\t],\n\t\t\t&quot;Model&quot;:&quot;spm&quot;\n\t\t}\n\t]\n}\nThe above is an example of how a transformation is used in a run-level analysis. The first Transformation Factor split the events (rows of the events.tsv file) based on the column trial_type. Then these factors are convolved with HRF, here the model is spm.\nPlease check examples from official github, you may learn more about specifications.\n3. Singularity with FitLins\nAfter defining the model json file, next you can run fitlins using singularity. In the MSense Lab server, it is located at /dss/containers/fitlins.simg\nA typical command is like this:\nfitlins data/bids_root/ out/ participant \\\n    -d data/derivatives/fmriprep/ -w work/\nHere is a concrete example:\n#!/bin/bash\nbids_dir=/dss/open/ds000003 # bids directory\nfmriprep_dir=/dss/open/ds000003/derivatives # fmriprep directory\noutput=/dss/open/ds000003/derivatives/fitlins # output director\nmodel_file=/dss/open/ds000003/code/linmodel.json # model json file\n \n# scratch folder\nscratch_dir=/dss/tmp\n#fitlins image\nfitlins=/dss/containers/fitlins.simg\n \n# run the model\nsingularity run \\\n\t$fitlins $bids_dir $output \\\n\tdataset \\\n\t--participant-label 01 02 03 05 06 07 08 09 10 11 12 13 \\\n\t--derivatives $fmriprep_dir \\\n\t--model $model_file \\\n\t--smoothing 5:run \\\n\t-w $scratch_dir \\\n\t--n-cpus 6 \\\n\t-v\n\n\n                  \n                  Tip\n                  \n                \n\nWhen you write command code in multiple lines with continuation symbol \\, you should not add spaces after that.\n\n\nVisualize the results\nThe results were organized in subfolders, such as node-FTest, node-subject, node-tTest.\nIn each sub-folder, there will be contrast-related results:\n\nYou can visualize these results using NiLearn. Here is one simple example:\nfrom nilearn import image, plotting\n!echo $PWD\n \n# point to your output directory\nresults_dir = &#039;../derivatives/fitlins/node-tTest&#039;\nnl_wp_path = results_dir + &#039;/contrast-wordGtPseudo_stat-t_statmap.nii.gz&#039;\n \nnl_wp_img = image.load_img(nl_wp_path)\n\topa = plotting.plot_glass_brain(nl_wp_img, colorbar=True, symmetric_cbar=True, cmap=&#039;bwr&#039;, threshold = 3, plot_abs=False)\nopa.title(&#039;Word &gt; pseudo t-stat&#039;)\n \n# plot smoothed or cluster results\nfrom nilearn.image import smooth_img, threshold_img\nfrom nilearn.regions import connected_regions\n \n#smooth before clustering\nwp_img_smooth = smooth_img(nl_wp_img, fwhm=6)\n# apply threshold\nwp_img_thresholded=threshold_img(wp_img_smooth, threshold=3, two_sided=True)\ncluster_threshold = 10\n \nregions, index = connected_regions(wp_img_thresholded, min_region_size=cluster_threshold)\n \n# plot the surviving clusters\n \nplotting.plot_glass_brain(wp_img_thresholded, colorbar=True, symmetric_cbar=True, cmap=&#039;bwr&#039;, plot_abs=False)\n \nplotting.plot_glass_brain(regions, colorbar = True, display_mode=&#039;lyrz&#039;)\n \nplotting.show()\n\n\n"},"fMRI-Tutorial/3.2-General-Linear-Model-(GLM)":{"slug":"fMRI-Tutorial/3.2-General-Linear-Model-(GLM)","filePath":"content/fMRI Tutorial/3.2 General Linear Model (GLM).md","title":"3.2 General Linear Model","links":["fMRI-Tutorial/1.1-BOLD-Signal-and-HRF"],"tags":["GLM","HRF"],"content":"1. General Linear Model Formulation\n\nThe general linear model (GLM) in matrix form for fMRI time-series can be written as:\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\nwhere \\mathbf{y} is an n \\times 1 vector of observed fMRI data (e.g., the BOLD signal over n time points), \\mathbf{X} is an n \\times p design matrix containing p predictors (regressors) such as task conditions convolved with the hemodynamic response function, \\boldsymbol{\\beta} is a p \\times 1 vector of model parameters (GLM beta weights for each predictor), and \\boldsymbol{\\varepsilon} is an n \\times 1 vector of residual errors. The goal of GLM analysis is to estimate \\boldsymbol{\\beta} such that \\mathbf{X}\\boldsymbol{\\beta} best fits the data \\mathbf{y} in a least-squares sense.\nOrdinary Least Squares (OLS) Solution: To find the best-fitting parameters, we minimize the sum of squared errors SSE = ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2. Setting the derivative \\partial SSE/\\partial \\boldsymbol{\\beta} to zero leads to the normal equations:\n\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0} ;\\implies; \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y}.\nIf \\mathbf{X}^T\\mathbf{X} is invertible (which requires that the p columns of \\mathbf{X} are linearly independent), the solution is:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y},\nwhich is the familiar OLS estimator. This formula gives the beta estimates that minimize the error between the predicted time series \\mathbf{X}\\hat{\\boldsymbol{\\beta}} and the observed data \\mathbf{y}.\nVariance of the \\beta Estimates\nUnder the standard GLM assumptions (errors \\boldsymbol{\\varepsilon} have zero mean, are independent, and have constant variance \\sigma^2), the uncertainty of the estimated coefficients can be quantified. The covariance of the estimator is:\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}.\nThis is because:\nVar(\\hat \\beta) = Var((X^T X)^{-1} X^T \\epsilon)= (X^T X)^{-1} X^T \\sigma^2 I X (X^T X)^{-1}= \\sigma^2 (X^T X)^{-1}\nThis equation shows two key contributors to the variance of the beta estimates:\n\nNoise variance (\\sigma^2): The inherent measurement noise in the data. Higher noise inflates the variance of \\hat{\\boldsymbol{\\beta}} linearly.\nDesign matrix structure ((\\mathbf{X}^T \\mathbf{X})^{-1}): The design variance component reflecting the quality of the design matrix \\mathbf{X}. If the predictors in \\mathbf{X} are nearly collinear or have low variance, (\\mathbf{X}^T \\mathbf{X})^{-1} will have large diagonal values, leading to larger \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}). In contrast, an orthogonal or high-variance design yields a “tighter” (\\mathbf{X}^T \\mathbf{X})^{-1} and thus lower estimator variance. In other words, design variance is the part of the standard error caused by the design matrix . High design variance corresponds to an inefficient design (and thus lower sensitivity), whereas low design variance means a more efficient design with more precise estimates .\n\nMathematically, design efficiency is defined as the inverse of design variance . A high-efficiency design (low design variance) is characterized by\n\n(1) high predictor variance – your regressors vary a lot relative to their mean, and\n(2) low predictor covariance – your regressors are not highly correlated with each other.\n\nThese principles will be revisited with examples in Part 3.\nT-Tests for \\beta Estimates\nIn the GLM framework, hypothesis testing for a single beta coefficient can be performed with a t-statistic. For a given coefficient \\hat{\\beta}_j, the t-statistic is defined as\nt = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)},\nwhere SE(\\hat{\\beta}_j) is the standard error of \\hat{\\beta}_j. Using the variance formula above, SE(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2  [(X^T X)^{-1}]_{jj}}, where \\hat{\\sigma}^2 is an estimate of the noise variance (often the residual mean square error). Intuitively, t measures how large the estimated effect is relative to its uncertainty.\nUnder the null hypothesis H_0: \\beta_j = 0 (no effect for predictor j) and assuming Gaussian errors, this statistic follows a t-distribution with (n-p) degrees of freedom. A large absolute t (relative to critical values) indicates that \\hat{\\beta}_j is significantly different from 0.\nIn summary, the GLM’s statistical inference at the first level boils down to estimating betas via OLS and assessing their significance with t-tests (or related F-tests), as we will detail next.\n2. Contrast Coding and Hypothesis Testing\nIn fMRI experiments, we often have multiple conditions or effects of interest encoded in the design matrix. Contrasts are used to test specific hypotheses about these effects. A contrast is essentially a set of weights (a vector or matrix) denoted \\mathbf{c} that defines a linear combination of the \\beta parameters. By applying a contrast to the GLM results, we can test hypotheses about one or more coefficients.\nFor a t-test, which tests a single linear combination, \\mathbf{c} is a 1×p vector. The contrast value is c^T \\hat{\\boldsymbol{\\beta}}, and the hypothesis H_0: c^T \\boldsymbol{\\beta} = 0 is assessed with a t-statistic. The variance of the contrast estimate is \\mathrm{Var}(c^T \\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\cdot c^T (\\mathbf{X}^T \\mathbf{X})^{-1} c. Therefore, the t-statistic for contrast \\mathbf{c} is: t = \\frac{c^T \\hat{\\boldsymbol{\\beta}}}{SE(c^T \\hat{\\boldsymbol{\\beta}})} = \\frac{c^T \\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\hat{\\sigma}^2 \\cdot c^T (X^T X)^{-1} c}}.\nThis general form reduces to the earlier single-beta formula when c picks out one coefficient (for example, c = [0,\\dots,1,\\dots,0] to test \\beta_j alone).\nFor an F-test, which can test multiple linear constraints simultaneously, \\mathbf{c} can be a matrix (with each row a contrast vector). For example, if we have m hypotheses to test jointly, we form a p \\times m contrast matrix C and test H_0: C^T \\boldsymbol{\\beta} = \\mathbf{0}_m. The F-statistic evaluates how much variance is explained by those m linear combinations relative to residual variance. While the full formula for an F-test is more complex (involving (C^T (X^T X)^{-1} C)^{-1} and the number of contrast degrees of freedom m), conceptually it generalizes the t-test: an F-test asks whether a set of \\beta’s (or contrasts) can collectively be zero. An F-test is often used for omnibus hypotheses, e.g. “do these three conditions differ overall” or “is there any effect of this factor with multiple levels,” where a significant F would prompt further post-hoc t-tests.\nDesign Matrix Coding and Examples of Contrasts\nHow we set up the design matrix \\mathbf{X} for categorical experimental conditions can influence the interpretation of \\boldsymbol{\\beta} and the construction of contrasts. Two common coding schemes are dummy coding (treatment coding) and effects coding (sum coding):\nDummy (Treatment) Coding\nOne level of a categorical factor is treated as the reference (baseline) with an intercept capturing its mean, and each other level gets a predictor whose \\beta represents the difference from baseline. For example, suppose an experiment recruits F and M. Using treatment coding with F as baseline, the design matrix might have an intercept (all 1’s, representing baseline F mean) and a column for M (Dummy X with 1’s for time points of M and 0 for F):\ny = \\beta_0 + \\beta_1X\nHere, \\beta_0 (intercept) is the mean BOLD response in condition F, and \\beta_1 is the difference M minus F, given X can only be 1 or 0.\nFormally, we can write \\beta_1 = \\mu_{M} - \\mu_{F} . The null hypothesis H_0: \\beta_1 = 0 thus corresponds to “no difference between conditions M and F”. In this example, the contrast vector to test the difference M vs. F is simply c = [0,1] (assuming the first coefficient is the intercept and the second is the M effect). A significant t for this contrast indicates a significant difference in activation between M and F. The intercept’s hypothesis in this coding would be H_0: \\beta_0 = 0 (is the baseline condition’s mean zero), which is usually not of interest in fMRI (since baseline is arbitrary), so we focus on contrasts among conditions.\n\n\n                  \n                  Question\n                  \n                \n\nWhat is the contrast array c for testing H_0: no significance in M.\n\n\n\nEffects (Sum) Coding\nAn alternative coding assigns weights that sum to zero for the factor levels, often coding levels as  c=[-0.5, 0.5] (for two levels) instead of c = [0,1].\nIn a two-condition example, an effects-coded design might have an intercept equal to the grand mean of F and M, and a predictor where \\beta_1 now represents half the difference between M and F. For instance, coding F as -0.5 and M as +0.5 yields an intercept \\beta_0 = (\\mu_{F}+\\mu_{M})/2 (mean of both conditions) and \\beta_1 = \\mu_{M} - \\mu_{F} (the full difference) . In general, with sum coding, the interpretation of \\beta coefficients shifts, but any hypothesis about means can still be tested with an appropriate contrast.\nNo matter the coding scheme, contrast vectors allow us to formally test hypotheses on the original condition means. The choice of coding changes how \\beta are defined, but the contrasts \\mathbf{c} can always be constructed to test the scientific question of interest .\nThe \\pm1 weights in a contrast directly express which condition means are being compared . For example, if our design matrix includes separate regressors for three conditions A, B, C (plus an intercept as the first column), we can test specific hypotheses:\n\nCondition A vs. Condition B:\n\nUse c = [0,1,-1,0]. This contrast will yield c^T \\hat{\\beta} = \\hat{\\beta}_A - \\hat{\\beta}_B, testing if A and B responses differ.\n\nCondition C vs. the average of A and B:\n\nFor example, assuming in visual search, we have conditions of target presence (A: T pointing to the left, and B: the right), and the target absence (C). If we want to test whether condition C (i.e., the target absence) elicits a different response than the average of A and B (i.e., the target presence), we can use a contrast like\nc = [0, -\\frac{1}{2}, -\\frac{1}{2}, 1]\nThis assigns weight +1 to \\beta_C and -\\frac{1}{2} to each of \\beta_A and \\beta_B, so c^T \\hat{\\beta} = \\hat{\\beta}_C - \\frac{1}{2}(\\hat{\\beta}_A + \\hat{\\beta}_B). The hypothesis H_0: c^T \\beta = 0 translates to “the activation in C is equal to the average of A and B.”\n\nOmnibus test for any differences among A, B, C:\n\nHere we would use an F-test with a contrast matrix that tests two degrees of freedom, e.g. contrasts for A vs. B and A vs. C (two independent comparisons spanning the space of differences among three conditions). For instance, one can use\nC = \\begin{bmatrix} 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\end{bmatrix}^T\nto jointly test H_0: \\beta_A=\\beta_B and H_0: \\beta_A=\\beta_C. A significant F would indicate not all condition means are equal, prompting post-hoc pairwise t-tests.\n\n\n                  \n                  Question\n                  \n                \n\nCan we add additional contrast in c for the test H_0: \\beta_B=\\beta_C?\n\n\nIn first-level fMRI GLMs, it is common to include one regressor per condition (plus perhaps confounds and a constant). This is akin to a treatment coding with an implicit baseline. In such cases, a contrast testing the difference between two conditions is often as simple as [0,\\dots, 1, \\dots, -1, \\dots, 0] placing +1 and –1 for the two conditions of interest. If the question is whether a condition activates significantly above baseline, the contrast might be [0,\\dots,1,\\dots,0] (testing \\beta_{\\text{condition}} = 0).\nFor group-level analyses or factorial designs, you may encounter different coding schemes, but the principle remains: formulate a contrast that reflects the hypothesis and then compute the t or F statistic for that contrast.\nTo recap:\n\nContrast vectors c specify the linear combination of \\beta’s we want to test.\nt-tests address single-degree-of-freedom hypotheses (c^T \\beta = 0) such as differences between two conditions or an effect vs baseline.\nF-tests handle multi-degree-of-freedom hypotheses (C^T \\beta = 0 for multiple contrasts) such as testing any effect of a multi-level factor or any effect in a group of regressors.\n\nBy appropriately coding the design matrix and constructing contrast vectors, we can flexibly test a wide array of hypotheses in the GLM framework.\n3. Design Variance and Simulations\n\nIn Part 1 GLM, we noted that the variance of estimated effects \\hat{\\boldsymbol{\\beta}} depends on both noise and the design matrix. Here, we focus on design variance, which captures how experimental design choices influence statistical power. Recall that for a given contrast c,\n\\mathrm{Var}(c^T \\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\cdot c^T (\\mathbf{X}^T \\mathbf{X})^{-1} c.\nThe term c^T (\\mathbf{X}^T \\mathbf{X})^{-1} c is often called the design variance for that contrast . It depends only on the design matrix \\mathbf{X} (and the contrast) and not on the observed data. A smaller c^T (\\mathbf{X}^T \\mathbf{X})^{-1} c means a more precise estimate of c^T \\beta (i.e. higher design efficiency), whereas a large value implies that even with low noise, the design itself makes it hard to estimate the contrast (low efficiency).\nDesign efficiency is defined as the inverse of design variance. Thus, maximizing efficiency is equivalent to minimizing design variance.\nNext, we will use simulations to illustrate how different experimental design choices affect design variance, specifically focusing on inter-stimulus interval (ISI) timing and predictor collinearity in an fMRI context.\nSimulation Setup\nWe will simulate fMRI design matrices with two conditions under different timing scenarios. To do this, we need a model for the hemodynamic response. We’ll use the Glover canonical HRF (as implemented in nilearn) to convolve discrete event timecourses into predicted BOLD responses.\nImports and HRF: First, let’s import necessary libraries and define the HRF.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nilearn.glm.first_level.hemodynamic_models import glover_hrf\n \n# Define a time axis for HRF (for a 1 s TR, oversampling if needed)\ntr = 1.0  # seconds per time bin (we&#039;ll use 1s resolution for HRF definition)\nhrf = glover_hrf(tr, oversampling=1, onset=0)               # canonical Glover HRF at 1-second resolution\nhrf = hrf / np.max(hrf)            # normalize HRF peak to 1\nt = np.arange(0, hrf.size)\nplt.plot(t, hrf)\nplt.title(&quot;Glover HRF at 1s resolution&quot;)\nplt.xlabel(&quot;Time [s]&quot;)\nplt.ylabel(&quot;HRF amplitude&quot;)\nplt.show()\n\nRunning the above would display the HRF shape (a positive peak at ~5s, followed by a smaller undershoot around 12-15s). Now we set up a function to simulate a simple design matrix given event onsets for two conditions:\ndef simulate_design_matrix(n_vols, onsets_A, onsets_B, tr=1.0):\n    &quot;&quot;&quot;\n    Simulate a design matrix with two task conditions A and B (plus intercept).\n    - n_vols: number of time points (volumes).\n    - onsets_A, onsets_B: lists of onset times (in seconds) for condition A and B events.\n    - tr: repetition time (sec) for the output time grid.\n    Returns: X (n_vols x 3 matrix) with [intercept, predictor_A, predictor_B].\n    &quot;&quot;&quot;\n    n_secs = int(n_vols * tr)            # total duration in seconds\n    dt = 1.0                             # internal resolution 1s for convolution\n    time_axis = np.arange(0, n_secs, dt)\n    # Create neural event timecourses (in 1s resolution)\n    neural_A = np.zeros_like(time_axis)\n    neural_B = np.zeros_like(time_axis)\n    for t in onsets_A:\n        if t &lt; n_secs:\n            neural_A[int(t)] = 1\n    for t in onsets_B:\n        if t &lt; n_secs:\n            neural_B[int(t)] = 1\n    # Convolve with HRF\n    conv_A = np.convolve(neural_A, hrf)[:len(time_axis)]\n    conv_B = np.convolve(neural_B, hrf)[:len(time_axis)]\n    # Downsample to TR resolution if tr &gt; 1\n    if tr != 1.0:\n        factor = int(tr / dt)  # how many 1s steps per TR\n        conv_A = conv_A[::factor]\n        conv_B = conv_B[::factor]\n    # Assemble design matrix (intercept + two conditions)\n    X = np.column_stack([np.ones(int(n_vols)), conv_A, conv_B])\n    return X\n \nX = simulate_design_matrix(n_vols=100, onsets_A=[10, 30, 50], onsets_B=[20, 40, 60], tr=1.0)\nprint(&quot;Design matrix shape:&quot;, X.shape)\nDesign matrix shape: (100:3)\n\n# plot the design matrix\nplt.figure(figsize=(10, 4))\nplt.plot(X[:, 0], label=&#039;Intercept&#039;, color=&#039;gray&#039;)\nplt.plot(X[:, 1], label=&#039;Condition A&#039;, color=&#039;blue&#039;)\nplt.plot(X[:, 2], label=&#039;Condition B&#039;, color=&#039;red&#039;)\nplt.title(&quot;Simulated Design Matrix&quot;)\nplt.xlabel(&quot;Volume (TR)&quot;)\nplt.ylabel(&quot;Amplitude&quot;)\nplt.legend()\nplt.show()\n\nEffect of ISI and Overlap on Design Variance\nNow, let’s compare two scenarios to see how overlap between events (short ISIs) increases predictor correlation and design variance:\n\nScenario 1 (Sparse, non-overlapping events): Condition A and B events are well-separated in time (long ISI), so their HRF responses do not overlap much. We expect low correlation between the regressors and high design efficiency.\nScenario 2 (Rapid, overlapping events): Condition A and B events occur in quick succession (short ISI, significant overlap of HRFs), which should produce collinear predictors and lower efficiency.\n\nLet’s simulate a simple experiment of ~6 minutes (175 time points with TR=2s, i.e., 350s total). In Scenario 1, condition B events will occur 30 seconds after condition A events (large separation). In Scenario 2, condition B follows condition A by only 4 seconds every time. We’ll use the function above to build the design matrices and then evaluate the correlation between predictors and the design efficiency for the contrast “A vs B” (i.e. c = [0, 1, -1]).\n# Simulation parameters\nTR = 2.0        # TR of 2s\nn_vols = 175    # 175 volumes =&gt; 350s experiment\n# Condition A onsets every 40s, Condition B follows either 30s after (no overlap) or 4s after (overlap)\nonsets_A = np.arange(20, 300, 60)       # e.g., 20s, 80s, 140s, 200s, 260s\nonsets_B_no_overlap = onsets_A + 30    # events for B 30s after A (non-overlapping HRFs)\nonsets_B_overlap   = onsets_A + 4      # events for B 4s after A (significant overlap)\n \n# Simulate design matrices for both scenarios\nX_no_overlap = simulate_design_matrix(n_vols*int(TR/2), onsets_A, onsets_B_no_overlap, tr=2.0)\nX_overlap   = simulate_design_matrix(n_vols*int(TR/2), onsets_A, onsets_B_overlap, tr=2.0)\n# Note: n_vols*int(TR/2) is 175*1 = 175 time points since TR=2 (simulate_design_matrix internally uses total secs)\n \n# Extract the two task regressors (columns 1 and 2 of X, intercept is col 0)\npredA_no, predB_no = X_no_overlap[:,1], X_no_overlap[:,2]\npredA_ov, predB_ov = X_overlap[:,1], X_overlap[:,2]\n\n# Compute correlation between predictors in each scenario\ncorr_no = np.corrcoef(predA_no, predB_no)[0,1]\ncorr_ov = np.corrcoef(predA_ov, predB_ov)[0,1]\n \n# Compute design efficiency for contrast c = [0, 1, -1] (A vs B)\nc = np.array([0, 1, -1])\n# Efficiency = 1 / design_variance (we ignore sigma^2 as a constant factor)\neff_no = 1.0 / (c @ np.linalg.inv(X_no_overlap.T @ X_no_overlap) @ c.T)\neff_ov = 1.0 / (c @ np.linalg.inv(X_overlap.T @ X_overlap) @ c.T)\n \nprint(f&quot;Predictor correlation (no overlap) = {corr_no:.3f}, Efficiency = {eff_no:.3f}&quot;)\nprint(f&quot;Predictor correlation (overlap)    = {corr_ov:.3f}, Efficiency = {eff_ov:.3f}&quot;)\nAfter running this code, we might get output (exact numbers may vary slightly due to discrete time resolution) similar to:\nPredictor correlation (no overlap) = -0.033, Efficiency = 4.463\nPredictor correlation (overlap)    = 0.117, Efficiency = 3.816\n\nThese results indicate that when events are well-separated (30s apart), the two condition regressors are virtually uncorrelated (corr \\approx 0) and the design efficiency for comparing A vs B is high. In contrast, when events occur only 4 seconds apart, the predictors overlap considerably, yielding a high correlation (0.117) and drastically lower efficiency. This quantitatively demonstrates that multicollinearity (predictor covariance) increases design variance and thus lowers efficiency.\nIntuitively, if Condition A and B always occur in close succession, it becomes hard for the GLM to disentangle their individual contributions – as some researchers say, the GLM “doesn’t know” whether an observed BOLD response should be attributed to A or B when their effects always overlap. This uncertainty is exactly what design variance captures.\nIn the plots above, each spike corresponds to the convolved HRF for an event. In the 30s-separated design, the blue and red spikes occur at different times, while in the 4s-separated design, the red spike rides on the tail of the blue spike, making them hard to distinguish.\nAdditional note on multicollinearity: In extreme cases, if two regressors are perfectly correlated (e.g., one is an exact copy of the other or a linear combination), the design matrix X becomes rank-deficient and (X^T X)^{-1} does not exist. This means the GLM cannot uniquely estimate the separate betas – the design is unidentifiable for those effects. In practice, one should never include perfectly collinear regressors in the model. Even near-collinearity can be problematic, inflating standard errors. Our example with heavy overlap approached this situation. A good design avoids placing events in a strictly linked fashion (e.g. avoid designs where condition B always follows condition A after a fixed short delay), as that induces multicollinearity.\nDesign Variance from Predictor Variance vs. Covariance\nOur simulations illustrate the impact of predictor covariance (overlap), but it’s worth noting that predictor variance also matters for design efficiency. If a predictor hardly varies (e.g., an event condition that occurs very few times or is almost constant), its \\beta will be estimated with high uncertainty. In fact, for a single predictor (contrast against baseline), the design variance simplifies to \\frac{1}{\\mathrm{Var}[X_j]} , meaning more variance in the regressor leads to lower design variance (higher efficiency). Thus, an optimal design should strive for:\nhigh variance in each predictor (lots of on/off variation, sufficient event occurrences) and low covariance between predictors. This is also reflected in the Max-Min-Con principle of design. That is, maximizing systematic variance, minimizing error variance, and controlling extraneous variance.\nThus, both (1) widely varying predictors and (2) low correlation among predictors contribute to lower design variance and higher statistical power. Usually using random inter-trial-interal (ITI) would result a low correlation among predictors.\nRecommendations for Robust Experimental Design\nThis webpage Design efficiency in fMRI from MRC Cognition and brain sciences unit is worthing reading. Here are some practical recommendations for designing efficient fMRI experiments, integrating the insights on design variance and contrasts:\n\n\nMaximize predictor variance: Ensure that each condition/event type occurs enough times and with sufficient signal change. Designs with richer modulation (e.g., events rather than long continuous stimulation, or including some jitter in event timing) increase \\mathrm{Var}(X_j), which lowers the variance of \\hat{\\beta}_j .\n\n\nMinimize predictor correlation: Avoid designs where regressors are highly correlated. This means do not always present conditions in a fixed order or with a fixed short interval. Randomize trial sequences and use “jittered” inter-stimulus intervals to decouple the predictors . Jittering introduces variability in onset timings, which decorrelates the regressors for different conditions and improves design efficiency for contrasts of interest.\n\n\nBlocked vs. event-related design: If your goal is purely to detect activation differences and you don’t need fine temporal resolution, blocked designs (clustering same-condition trials together) are often more efficient statistically than event-related designs . Blocked designs maximize variance between task and rest blocks and minimize within-block predictor fluctuations, yielding larger sustained signals and lower design variance for condition vs baseline contrasts. However, block designs may introduce psychological confounds (e.g., expectancy or habituation) and are less flexible. Event-related designs are more versatile and can estimate transient responses, but they tend to be less efficient; mitigate this by optimizing timing (use jitter, optimize trial order).\n\n\nUse contrasts to focus your hypothesis: Design your experiment such that the contrast corresponding to your primary hypothesis is as efficient as possible. For instance, if you care about A vs B, ensure A and B aren’t always paired with other events that cause overlap. Sometimes using short “null” periods or baseline rest between events can help decorrelate condition regressors.\n\n\nPilot simulations: It can be very useful to simulate your experimental design (as we did above) before running the study. Compute the expected design covariance, condition correlations, and efficiency for key contrasts . Tools like numpy or specialized packages (e.g., NeuroDesign or optseq) allow you to quantify design efficiency in advance. Aim for a design that maximizes 1/(c^T (X^T X)^{-1} c) for your contrasts of interest.\n\n\nBalance and counterbalancing: Ensure balanced condition frequencies and durations. Unbalanced designs (e.g., one condition is very rare) can lead to low variance for some predictors and potentially higher correlation with confounds or drift terms. Counterbalance trial orders across runs or subjects to avoid any structured order effects contributing to collinearity.\n\n\nBy following these guidelines – increase variance, reduce collinearity, choose appropriate design paradigms, and use contrasts wisely – you will optimize the GLM’s ability to detect true effects. In formula terms, you are minimizing design variance and maximizing efficiency , thereby boosting the t-values (since t \\propto 1/\\sqrt{\\text{design variance}}) . A thoughtful experimental design sets you up for successful statistical inference when you ultimately fit the GLM to your fMRI data.\nFurther Reading:\n\nLukas Snoek’s NI-edu course materials provide an accessible introduction to design variance, efficiency, and how to calculate them in Python. The examples shown above were inspired by his course material.\nAdditionally, Vasishth &amp; Nicenboim (2022) Linear Mixed Models in Cognitive Science (Chapter 6) offers insight into contrast coding in regression, illustrating how different coding schemes (treatment, sum coding, etc.) affect interpretation but not the underlying testable hypotheses .\nAmaro, E., Jr, &amp; Barker, G. J. (2006). Study design in fMRI: basic principles. Brain and Cognition, 60(3), 220–232. doi.org/10.1016/j.bandc.2005.11.009\nMRC Design efficiency in fMRI\n"},"fMRI-Tutorial/4.1-First-level-Analysis-with-FSL":{"slug":"fMRI-Tutorial/4.1-First-level-Analysis-with-FSL","filePath":"content/fMRI Tutorial/4.1 First-level Analysis with FSL.md","title":"4.1 First-level Analysis with FSL","links":[],"tags":["GLM","FSL"],"content":"Before doing first-level analysis, you must run brain extraction using BET, ensure your functional and structural data are in NIFTI format, and ensure that you have any necessary event timing files (e.g., stimulus onset, durations).\nLaunch Feat GUI:\nFeat &amp;\nThe interactive interface of Feat from FSL:\n\nSet Up the First-Level Analysis\nDefine the Data Input\n• Data Directory:\nSelect the folder containing your functional data.\n• Data Type:\nChoose whether your design is “Time-series” (single run) or “Multiple sessions” (if you have more than one run).\nSpecify Preprocessing Options (if needed)\n• Motion Correction: Enable MCFLIRT if not already done.\n• Slice Timing Correction: Check this if your sequence requires correction (and if it wasn’t already applied).\n• Spatial Smoothing: Specify the smoothing kernel size (e.g., 5 mm FWHM).\n• High-Pass Filtering: Set the cutoff (e.g., 100 s) to remove low-frequency drifts.\nSet Up the Model\n• Design Setup: • Click on “Stats” in the GUI.\n• EVs (Explanatory Variables): Define your regressors. For each experimental condition, you can either use a block or event-related model:\n• Click “Add EV” for each condition.\n• Provide the timing file or manually input the onsets/durations.\nUsually we provide a three-column text for this with the option ‘Custom (3 column format)‘. Three columns are: Onset, Duration, and amplitude (1).\n• Temporal Derivatives (optional): Include them if you want to account for slight differences in response timing.\n• Contrasts:\n• Under the “Contrasts” tab, set up contrasts to test your hypotheses (e.g., condition A &gt; baseline).\n• Add as many contrasts as needed to test different effects.\nRegistration\n• Structural Registration:\n• Register your functional data to your high-resolution structural scan.\n• Use FLIRT for linear registration (and optionally FNIRT for nonlinear registration if needed).\n• Check the transformation matrices and overlay the images to verify alignment.\n• Standard Space Registration: If you plan to compare with group-level analyses, register to a standard template (e.g., MNI152 2mm).\nThen click ‘Go’ to run the Analysis and save the template, which we need to adapt for other subjects.\nAdapt template for all participants\nThe saved Feat configuration .fsf is actually a text file, which can be edited by any text editor. We open this saved file, and change the following fields to variables, which we will replace using a bash file:\n\nOutput directory\nTotal Volumes\nFunctional image location\nT1 image location\n3-column timing files\nThe following code is an example of replaced template.\n\n# Output directory\nset fmri(outputdir) &quot;OUTPUT_DIR&quot;\n\n# Total volumes\nset fmri(npts) TOTAL_VOLUMES\n\n# 4D AVW data or FEAT directory (1)\nset feat_files(1) &quot;FUNC_IMAGE&quot;\n\n# Subject&#039;s structural image for analysis 1\nset highres_files(1) &quot;T1_IMAGE&quot;\n\n# Custom EV file (EV 1)\nset fmri(custom1) &quot;COND1_FILE&quot;\n\n# Custom EV file (EV 2)\nset fmri(custom2) &quot;COND2_FILE&quot;\n\n\nIf you have multiple RUNs, you should also make them replaceable variables.\nNext, use a bash file to replace them to run Feat for individual subjects.\n#!/bin/bash\nSCRIPT_DIR=$(cd &quot;$(dirname &quot;$0&quot;)&quot; &amp;&amp; pwd)\nBIDS_DIR=$(cd &quot;$SCRIPT_DIR/../..&quot; &amp;&amp; pwd)\nDERIVATIVES_DIR=&quot;${BIDS_DIR}/derivatives&quot; # Output directory for first-level results\nFSF_TEMPLATE=&quot;${BIDS_DIR}/code/fsl/template_level.fsf&quot;\n \n# Ensure the derivatives directory exists\n \nmkdir -p &quot;${DERIVATIVES_DIR}&quot; \n \n# Number of parallel jobs to run\n \nNJOBS=8\n \n# Track jobs to prevent overload\njob_count=0\n \n# Loop over all subject folders\nfor subj in ${BIDS_DIR}/sub-*; do\n\tsubj_id=$(basename &quot;${subj}&quot;) # e.g., sub-01\n\tfunc_dir=&quot;${subj}/func&quot;\n \n\t# Locate the T1 brain image (common to all runs)\n\tt1_image=$(ls &quot;${subj}/anat/${subj_id}_T1w_brain.nii.gz&quot; 2&gt;/dev/null | head -n 1)\n\t# Loop over runs (e.g., run-1 and run-2)\n\t# depending how many runs you have\n\tfor run in 1 2; do\n\t\t# Locate functional image for this run\n\t\tfunc_image=$(ls &quot;${func_dir}/${subj_id}_task-flanker_run-${run}_bold.nii.gz&quot; 2&gt;/dev/null | head -n 1)\n\t\techo &quot;${func_dir}/${subj_id}_task-*_run-${run}_bold.nii.gz&quot;\n\t\t# Locate the event files for congruent and incongruent conditions for this run\n\t\tcond2_file=$(ls &quot;${func_dir}/congruent_run${run}.txt&quot; 2&gt;/dev/null | head -n 1)\n\t\tcond1_file=$(ls &quot;${func_dir}/incongruent_run${run}.txt&quot; 2&gt;/dev/null | head -n 1)\n\t\t# Skip if no functional image is found for this run\n \n\t\tif [ ! -f &quot;$func_image&quot; ]; then\n\t\t\techo &quot;No functional image found for ${subj_id} run-${run}, skipping...&quot;\n\t\t\tcontinue\n\t\tfi\n \n\t\t# Get the number of volumes (4th dimension) from the functional image\n\t\ttotal_vols=$(fslval &quot;$func_image&quot; dim4)\n\t\t\n\t\t# Define run-specific FSF file and FEAT output directory\n\t\tfsf_file=&quot;${DERIVATIVES_DIR}/fsl/design_${subj_id}_run-${run}.fsf&quot;\n\t\toutput_dir=&quot;${DERIVATIVES_DIR}/fsl/${subj_id}_run-${run}.feat&quot;\n\t\techo &quot;output_dir: ${output_dir}&quot;\n\t\techo &quot;fsf_file: ${fsf_file}&quot;\n\t\t\n\t\t# Skip if FEAT analysis has already been run for this run\n\t\tif [ -d &quot;${output_dir}&quot; ]; then\n\t\t\techo &quot;Skipping ${subj_id} run-${run}, FEAT already run.&quot;\n\t\t\tcontinue\n\t\tfi\n\t\n\t\t# Create a subject- and run-specific FSF file with updated paths and parameters.\n\t\t# Note: The FSF template should include placeholders such as SUBJ-ID, RUN_NUMBER,\n\t\t# OUTPUT_DIR, FUNC_IMAGE, T1_IMAGE, COND1_FILE, COND2_FILE, and TOTAL_VOLUMES.\n\t\tsed -e &quot;s/SUBJ-ID/${subj_id}/g&quot; \\\n\t\t\t-e &quot;s/RUN_NUMBER/run-${run}/g&quot; \\\n\t\t\t-e &quot;s|OUTPUT_DIR|${output_dir}|g&quot; \\\n\t\t\t-e &quot;s|FUNC_IMAGE|${func_image}|g&quot; \\\n\t\t\t-e &quot;s|T1_IMAGE|${t1_image}|g&quot; \\\n\t\t\t-e &quot;s|COND1_FILE|${cond1_file}|g&quot; \\\n\t\t\t-e &quot;s|COND2_FILE|${cond2_file}|g&quot; \\\n\t\t\t-e &quot;s/TOTAL_VOLUMES/${total_vols}/g&quot; \\\n\t\t\t&quot;${FSF_TEMPLATE}&quot; &gt; &quot;${fsf_file}&quot;\n\t\t\n\t\t# Run FEAT for this subject and run in the background\n\t\t\n\t\techo &quot;Running FEAT for ${subj_id} run-${run} with ${total_vols} volumes...&quot;\n\t\tfeat &quot;${fsf_file}&quot; &amp;\n\t\t\n\t\t# Track the number of background jobs and wait if the limit is reached\n\t\tjob_count=$((job_count + 1))\n\t\tif [ $job_count -eq $NJOBS ]; then\n\t\t\twait\n\t\t\tjob_count=0\n\t\tfi\n\t\t\n\tdone\ndone\n \n# Wait for any remaining background jobs to complete\nwait\necho &quot;First-level FEAT analysis completed for all subjects and runs.&quot;"},"fMRI-Tutorial/4.2-High-level-Analysis-with-FSL":{"slug":"fMRI-Tutorial/4.2-High-level-Analysis-with-FSL","filePath":"content/fMRI Tutorial/4.2 High-level Analysis with FSL.md","title":"4.2 High-level Analysis with FSL","links":[],"tags":["GLM","FSL"],"content":"High-level analysis in FSL aggregates results from individual subjects to draw inferences about a population. The typical steps involve:\n\nData collection: Gather first-level FEAT outputs (cope, varcope, and sometimes t-statistic images).\nDesign specification: Define your design matrix and contrast vectors to model group-level effects.\nEstimation: Use FSL’s FEAT (or FLAME) for mixed-effects analysis.\nInference: Correct for multiple comparisons and interpret cluster or voxel-level results.\n\n1. Prepare your first-level data\nAfter the first-level analysis, you have the necessary data organized. For example, you put all subject-wise data in subfolder derivatives:\n\n2. Create Design and Contrast Matrix\nOpen Feat &amp; GUI, and add all first-level feat folders or COPE files, depending on your experimental design.\nAndy Jahn’s brain book website provides excellent instructions on this. Please check his second-level analysis and third-level analysis for details.\nEssentially, you need to create a higher-level analysis by setting up a new FEAT design file. Here is an example of such design file:\n# Higher-level FEAT design example for one-sample t-test\n \n# Set the basic parameters\nset fmri(npts) 1\nset fmri(outputdir) &quot;group_level.feat&quot;\n \n# Input subject-level cope images\nset feat_files(1) &quot;/path/to/sub01/first_level.feat/stats/cope1.nii.gz&quot;\nset feat_files(2) &quot;/path/to/sub02/first_level.feat/stats/cope1.nii.gz&quot;\nset feat_files(3) &quot;/path/to/sub03/first_level.feat/stats/cope1.nii.gz&quot;\n# ... add more subjects as needed\n \n# Define the design matrix file paths\nset fmri(groupfile) &quot;feat_dir_list.txt&quot;\nset fmri(cope1) 1\n \n# Set up the design for a one-sample t-test\nset fmri(contrasts) &quot;1&quot;\nset fmri(contrast_names1) &quot;Mean Effect&quot;\n \n# Use FLAME stage 1 (or stage 1+2 for more accurate variance estimation)\nset fmri(flame_level) 2\nThe above example uses the FLAME method (FMRIB’s Local Analysis of Mixed Effects).\n3. Inspecting and Interpreting Results\nAfter running your higher-level analysis, you should review:\n\nStatistical maps: (e.g., zstat images) to identify significant clusters.\nCluster reports: Check the clusters in the FEAT report.\nOverlay images: Use FSLview or FSLeyes to overlay significant clusters on anatomical images.\n"},"fMRI-Tutorial/5.1-NiLearn-fMRI-Basic-Analysis":{"slug":"fMRI-Tutorial/5.1-NiLearn-fMRI-Basic-Analysis","filePath":"content/fMRI Tutorial/5.1 NiLearn fMRI Basic Analysis.md","title":"5.1 NiLearn fMRI Basic Analysis","links":[],"tags":[],"content":"0  Prerequisites\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequirementWhy you need itPython ≥ 3.9 with nilearn 0.12+, numpy, pandas, scipy, nibabel, joblibCore librariesfMRIPrep-pre-processed data (BIDS derivatives)Gives you: *_desc-preproc_bold.nii.gz, *_desc-brain_mask.nii.gz, *_events.tsv, *_confounds_timeseries.tsvAn MNI brain mask (e.g. MNI152_T1_2mm_brain_mask.nii.gz)For group-level masking\nInstall everything (one-off):\npip install nilearn nibabel scipy pandas joblib\n\n\n1  Per-run first-level GLM\nimport os, glob, pandas as pd\nfrom nilearn.glm.first_level import FirstLevelModel\n \nproj_dir    = &#039;/path/to/dataset&#039;\nresults_dir = &#039;/path/to/level1_nilearn&#039;\nsubjects    = sorted([d.split(&#039;-&#039;)[1] for d in glob.glob(f&#039;{proj_dir}/sub-*&#039;)])\nruns        = [&#039;1&#039;, &#039;2&#039;]\ncontrasts   = {\n    &#039;Congruent&#039;   : &#039;congruent&#039;,\n    &#039;Incongruent&#039; : &#039;incongruent&#039;,\n    &#039;Incongr&gt;Cong&#039;: &#039;incongruent - congruent&#039;\n}\nmotion_cols = [&#039;rot_x&#039;,&#039;rot_y&#039;,&#039;rot_z&#039;,&#039;trans_x&#039;,&#039;trans_y&#039;,&#039;trans_z&#039;]\n \nfor sub in subjects:\n    for run in runs:\n        # ----- file paths ---------------------------------------------------\n        func   = f&#039;{proj_dir}/derivatives/fmriprep/sub-{sub}/func/&#039; \\\n                 f&#039;sub-{sub}_task-flanker_run-{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz&#039;\n        mask   = func.replace(&#039;preproc_bold&#039;, &#039;brain_mask&#039;)\n        events = f&#039;{proj_dir}/sub-{sub}/func/&#039; \\\n                 f&#039;sub-{sub}_task-flanker_run-{run}_events.tsv&#039;\n        confounds_tsv= f&#039;{proj_dir}/derivatives/sub-{sub}/func/&#039;\n\t\t\t\t f&#039;sub-{sub}_task-flanker_run-{run}_desc-confounds_timeseries.tsv&#039;\n \n)\n \n        events_df    = pd.read_csv(events, sep=&#039;\\t&#039;)\n        confounds_df = pd.read_csv(conf,   sep=&#039;\\t&#039;)[motion_cols]\n \n        # ----- model --------------------------------------------------------\n        model = FirstLevelModel(\n            t_r=2.0, smoothing_fwhm=6.0, mask_img=mask,\n            drift_model=&#039;cosine&#039;, high_pass=1/128, noise_model=&#039;ar1&#039;\n        ).fit(func, events=events_df, confounds=confounds_df)\n \n        out_dir = f&#039;{results_dir}/sub-{sub}/run-{run}&#039;\n        os.makedirs(out_dir, exist_ok=True)\n \n        for name, expr in contrasts.items():\n            model.compute_contrast(expr, output_type=&#039;effect_size&#039;   ).to_filename(f&#039;{out_dir}/{name}_cope.nii.gz&#039;)\n            model.compute_contrast(expr, output_type=&#039;effect_variance&#039;).to_filename(f&#039;{out_dir}/{name}_varcope.nii.gz&#039;)\n            model.compute_contrast(expr, output_type=&#039;z_score&#039;       ).to_filename(f&#039;{out_dir}/{name}_zmap.nii.gz&#039;)\nWhat happened?\n\nBuild events + motion regressors\nOne call to fit() produces design-matrix, HRF-convolution, GLS fit\nSave COPE, VARCOPE, Z-map for each contrast\n\n\n2  Within-subject fixed-effects (combine runs)\nfrom nilearn.glm import compute_fixed_effects\n \nsubject_zmaps = {}    # {sub: {contrast: fixed-z img}}\n \nfor sub in subjects:\n    subject_zmaps[sub] = {}\n    for c in contrasts:\n        copes   = [f&#039;{results_dir}/sub-{sub}/run-{r}/{c}_cope.nii.gz&#039;    for r in runs]\n        varcps  = [f&#039;{results_dir}/sub-{sub}/run-{r}/{c}_varcope.nii.gz&#039; for r in runs]\n \n        fx_cope, fx_var, _, fx_z = compute_fixed_effects(\n            copes, varcps, precision_weighted=True, return_z_score=True\n        )\n        fout = f&#039;{results_dir}/sub-{sub}/{c}_fixed_zmap.nii.gz&#039;\n        fx_z.to_filename(fout)\n        subject_zmaps[sub][c] = fout\ncompute_fixed_effects does an inverse-variance weighted average of runs.\n\n3  Group (second-level) random-effects\nfrom nilearn.glm.second_level import SecondLevelModel\nimport pandas as pd\n \ngroup_dir = f&#039;{results_dir}/group&#039;\nos.makedirs(group_dir, exist_ok=True)\n \nfor c in contrasts:\n    inputs = [subject_zmaps[sub][c] for sub in subjects]\n    design = pd.DataFrame({&#039;intercept&#039;: [1]*len(inputs)})\n \n    model  = SecondLevelModel(smoothing_fwhm=6.0, mask_img=&#039;MNI152_T1_2mm_brain_mask.nii.gz&#039;)\n    model  = model.fit(inputs, design)\n \n    zmap   = model.compute_contrast(&#039;intercept&#039;, output_type=&#039;z_score&#039;)\n    zmap_fn= f&#039;{group_dir}/{c}_group_zmap.nii.gz&#039;\n    zmap.to_filename(zmap_fn)\n\n4  Cluster-level inference + table\nfrom nilearn.glm import cluster_level_inference\nfrom nilearn.reporting import get_clusters_table\nfrom scipy.stats import norm\n \nz_thresh  = norm.isf(0.001)                        # voxel p&lt;0.001\nzmap_file = f&#039;{group_dir}/Incongr&gt;Cong_group_zmap.nii.gz&#039;\n \n# A. cluster FWE p&lt;0.05\nclusters_img = cluster_level_inference(\n    zmap_file, mask_img=&#039;MNI152_T1_2mm_brain_mask.nii.gz&#039;,\n    threshold=z_thresh, alpha=0.05\n)\nclusters_img.to_filename(f&#039;{group_dir}/Incongr&gt;Cong_clusterFWE_zmap.nii.gz&#039;)\n \n# B. cluster table (≥20 vox @ voxel p&lt;0.001)\ntable = get_clusters_table(zmap_file, stat_threshold=z_thresh, cluster_threshold=20)\nprint(table.head())\n\n5  Visualisation\nfrom nilearn import plotting\nfrom nilearn.image import index_img\nfrom nilearn.glm.thresholding import threshold_stats_img\n  \nplotting.plot_stat_map(\n\tclusters_img,\n\tthreshold=0.0, display_mode=&quot;z&quot;, vmax=1,\n\tcmap=&quot;inferno&quot;,\n\ttitle=&quot;group left-right button press, proportion true positives&quot;,\n)\n \nplotting.plot_stat_map(\n\tzmap_file,\t\n\tthreshold=z_thresh_uncorr,\n\tdisplay_mode=&quot;z&quot;,\n\ttitle=&quot;group left-right button press (uncorrected p &lt; 0.001)&quot;,\n)\n\n\n\n6  Parallelising (optional)\nfrom joblib import Parallel, delayed\nParallel(n_jobs=6)(\n    delayed(process_run)(sub, run, ...)  # wrap the first-level code in a function\n    for sub in subjects for run in runs\n)\n\n\n7  Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStepNilearn function(s)What it replaces in FSL/SPM/AFNI1FirstLevelModelFEAT (FSL), 3dDeconvolve (AFNI), SPM first-level2compute_fixed_effectsFEAT “higher-level” fixed-effects3SecondLevelModelFLAME, 3dttest++, SPM second-level4cluster_level_inference, get_clusters_tablecluster, randomise, 3dClustSim5plot_glass_brain, view_img_on_surfFSLeyes, AFNI viewer, SPM GUI\n\nFurther reading\n\n\nNilearn docs: nilearn.github.io\n\n\nGLM tutorial: nilearn.github.io/dev/auto_examples/05_glm_first_level\n\n\nCluster inference: Rosenblatt et al., 2018, NeuroImage\n\n"},"fMRI-Tutorial/5.2-Visualization-and-Statistical-Thresholding-with-Nilearn":{"slug":"fMRI-Tutorial/5.2-Visualization-and-Statistical-Thresholding-with-Nilearn","filePath":"content/fMRI Tutorial/5.2 Visualization and Statistical Thresholding with Nilearn.md","title":"5.2 Visualization and Statistical Thresholding with Nilearn","links":[],"tags":[],"content":"This tutorial assumes you have precomputed second-level fMRI statistical maps.\n1. Background: Statistical thresholding in fMRI analysis\nCommon thresholding methods used in fMRI group analysis:\n\nUncorrected voxel-wise thresholding:\n\nThis is a simple threshold based on a p-value (e.g., p &lt; 0.001) applied to each voxel independently, without any multiple-comparison correction. It controls the false positive rate per voxel but not across the whole brain. Consequently, in a typical 100,000-voxel brain map, a threshold of p&lt;0.05 could yield around 5,000 false positives just by chance. Uncorrected thresholds (such as p &lt; 0.001) are more stringent but can still result in dozens of false-positive voxels in whole-brain analyses. This method is typically used only as an initial visualization or for defining clusters to facilitate further correction.\n\nFWER correction (Family-Wise Error Rate):\n\nThis controls the probability of any false positives in the entire map (the “family” of tests). A common FWER method is Bonferroni correction, which sets the voxel-wise p-threshold to α/N (α = desired overall alpha, N = number of voxels). For example, for α=0.05 and N=100,000 voxels, the threshold becomes p &lt; 0.0000005. This guarantees a very low chance of any false positive voxel, but is extremely conservative for fMRI because voxel tests are not truly independent (neighboring voxels’ signals are correlated). Such over-correction can drastically reduce power (increase false negatives). In practice, FWER is often applied via Random Field Theory-based methods (as in SPM) or Bonferroni when valid. FWER-corrected results are voxel-wise significant at the given α (e.g., p&lt;0.05 FWE means each reported voxel is significant at that corrected level).\n\nFDR correction (False Discovery Rate):\n\nFDR controls the expected proportion of false positives among the voxels deemed significant. For instance, an FDR threshold q=0.05 means that on average 5% of the active voxels could be false positives. This is less conservative than FWER — it allows some false positives as long as the rate is low — and thus has higher sensitivity in detecting true effects. FDR adapts to the data: If there are many true activations, the threshold becomes more lenient; if there are few activations, it becomes stricter (in the limit of no signal, the FDR threshold equals Bonferroni). FDR is a voxel-wise correction in the sense that it provides a threshold for voxels, but it controls a global metric (proportion of false discoveries rather than the chance of any false voxel).\n\nCluster-wise inference:\n\nThis approach uses the concept that true activations tend to occur in contiguous clusters of voxels, rather than isolated single voxels. Instead of testing each voxel independently, we first apply a cluster-defining threshold (e.g., p&lt;0.001 uncorrected at voxel level) to get clusters, then test the clusters for significance based on their size (extent) or mass. The probability of observing a cluster of a given size under null (noise) conditions is estimated (via Random Field Theory or permutation simulations ), and clusters are deemed significant if they exceed a size threshold corresponding to (for example) p&lt;0.05 FWER at the cluster level. In practice, this means small isolated blobs are likely discarded as noise, while larger contiguous areas survive. Cluster-wise correction thus controls FWER across clusters, not individual voxels. It is more sensitive than strict voxel-wise FWER, at the cost of not pinpointing which voxels in a big cluster are truly “active” (we trust that at least some are). Voxel-wise vs. cluster-wise: Voxel-wise methods (like Bonferroni or FDR) apply criteria to individual voxels’ values. Cluster-wise methods focus on the extent of activation and require a minimum cluster size for significance. Both approaches can be used in tandem (e.g., a voxel-wise threshold to define clusters, then cluster extent threshold for correction).\nIn short, uncorrected thresholds are liberal (risking false positives), FWER (Bonferroni/RFT) is very strict (minimizes any false positive at cost of missing true effects), FDR offers a balance by controlling false discovery proportion, and cluster-wise methods improve sensitivity by leveraging spatial extent at the cost of localization specificity.\n2. Examples\nLet’s use sample data motor task activation from nilearn to illustrate these concepts.\nfrom nilearn import datasets\n \n# Load an example statistical map (motor task activation)\nstat_img = datasets.load_sample_motor_activation_image()\nprint(&quot;Sample Z map path:&quot;, stat_img)\n \n# load a standard brain atlas (MNI template mask)\nbrain_mask = datasets.load_mni152_brain_mask()\nprint(&quot;MNI152 template path:&quot;, brain_mask)\n3. Applying Thresholding Strategies with Nilearn\nNilearn offers convenient functions to threshold statistical images:\n\nnilearn.image.threshold_img: for simple thresholding by a given value or percentile.\nnilearn.glm.threshold_stats_img: for thresholding based on statistical criteria (p-values) with options for FPR, FDR, Bonferroni, and cluster size.\n\nWe will use both to illustrate uncorrected vs corrected thresholds.\n3.1 Uncorrected thresholding (voxel-wise p-value)\nLet’s apply a voxel-wise uncorrected p&lt;0.001 threshold. For Z-statistics, p&lt;0.001 (one-tailed) corresponds to Z &gt; ~3.09. We’ll use a slightly stricter Z &gt; 3.29 (≈ p&lt;0.0005 two-tailed) as in many studies, and also remove tiny clusters (cluster size &lt; 10 voxels) to focus on substantive blobs:\nfrom nilearn.image import threshold_img\n \n# Threshold the Z map at |Z|&gt;3.29 (p&lt;0.001 approx), remove clusters &lt; 10 voxels\nthreshold_value = 3.29\nuncorr_map = threshold_img(stat_img, threshold=threshold_value, \n                           two_sided=True, cluster_threshold=10, \n                           mask_img=brain_mask)\nThe result uncorr_map is a new image where all voxels with |Z| ≤ 3.29 are zeroed out (i.e., not significant at p&lt;0.001), and only clusters with &gt;10 voxels are retained. This procedure is essentially what one would get by an uncorrected p&lt;0.001 threshold in a Z-stat image with a 10-voxel extent cutoff.\n3.2 Using threshold_stats_img for advanced corrections\nNow we’ll use threshold_stats_img to apply FDR and FWER corrections. This function computes the appropriate Z threshold for a given α, using the image data:\nfrom nilearn.glm import threshold_stats_img\n \n# 1) Uncorrected (FPR) at p&lt;0.001, with cluster &gt;10 (should match the manual threshold above)\nthresholded_map_fpr, thresh_fpr = threshold_stats_img(stat_img, \n\tmask_img=brain_mask, alpha=0.001, height_control=&#039;fpr&#039;,\n\ttwo_sided=True, cluster_threshold=10)\nprint(&quot;Uncorrected p&lt;0.001 Z threshold =&quot;, round(thresh_fpr, 2))\n \n# 2) FDR correction at q=0.05\nthresholded_map_fdr, thresh_fdr = threshold_stats_img(stat_img, \n\tmask_img=brain_mask,alpha=0.05, height_control=&#039;fdr&#039;)\nprint(&quot;FDR=0.05 Z threshold =&quot;, round(thresh_fdr, 2))\n \n# 3) Bonferroni correction at p&lt;0.05\nthresholded_map_fwer, thresh_fwer = threshold_stats_img(stat_img,\n\tmask_img=brain_mask,alpha=0.05, height_control=&#039;bonferroni&#039;)\nprint(&quot;Bonferroni p&lt;0.05 Z threshold =&quot;, round(thresh_fwer, 2))\nOutput:\nUncorrected p&lt;0.001 Z threshold = 3.29  \nFDR=0.05 Z threshold = 3.0\nBonferroni p&lt;0.05 Z threshold = 5.56\nThe above output values mean:\n\np&lt;0.001 (uncorrected) corresponds to Z &gt; ~3.29 (as expected) ￼.\nFDR 0.05 yielded a threshold 3, which is much lower (more inclusive) – many voxels can be included while keeping false discovery rate at 5% ￼.\nBonferroni 0.05 gave Z &gt; 5.56, a very high bar (more exclusive) – only the strongest peaks survive when controlling any false positive at 5% ￼.\n\n4. Visualizing Statistical Maps: Plotting in Nilearn\nNilearn’s plotting module provides several ways to visualize 3D brain maps:\n\nplot_stat_map: Display the map as colored blobs overlaid on an anatomical brain (default background is an MNI template). This shows orthogonal slices by default.\nplot_glass_brain: Display a transparent “glass brain” maximal-intensity projection – useful for an overview of activation spread without anatomical detail.\nview_img: Interactive viewer for Jupyter notebooks, allowing you to scroll through slices.\n\nWe will use each of these. First, let’s plot the unthresholded Z map to see all activations (positive and negative):\nfrom nilearn import plotting\n \n# Plot the raw (unthresholded) Z map\ndisplay = plotting.plot_stat_map(stat_img, title=&quot;Raw z map (unthresholded)&quot;,\n                                 threshold=None, colorbar=True)\n\nBy default, it will choose cut coordinates that show some of the highest activations. We can extract those coordinates to reuse for other plots (so that all plots show the same slices for easier comparison):\ncut_coords = display.cut_coords  # save the chosen cut coordinates\nprint(&quot;Cut coordinates chosen:&quot;, cut_coords)\nNow, plot the thresholded results side by side. We’ll use the same cut coordinates for consistency:\n# 1. Uncorrected p&lt;.001 (with clusters &gt; 10)\nplotting.plot_stat_map(thresholded_map_fpr, cut_coords=cut_coords, \n                       threshold=thresh_fpr, \n                       title=&quot;Thresholded z map, p&lt;.001 unc, clusters &gt; 10 voxels&quot;)\n \n# 2. FDR-corrected (q=0.05)\nplotting.plot_stat_map(thresholded_map_fdr, cut_coords=cut_coords, \n                       threshold=thresh_fdr, \n                       title=&quot;Thresholded z map, FDR q=0.05&quot;)\n \n# 3. FWER-corrected (Bonferroni p&lt;.05)\nplotting.plot_stat_map(thresholded_map_fwer, cut_coords=cut_coords, \n                       threshold=thresh_fwer, \n                       title=&quot;Thresholded z map, FWER p&lt;0.05 (Bonferroni)&quot;)\nplotting.show()\n\n\n\nVoxel-wise uncorrected threshold p&lt;0.001 (Z&gt;3.29), with clusters smaller than 10 voxels removed. Many activation clusters are visible, as no multiple-comparison correction was applied beyond the initial threshold — we expect some false positives among these. False Discovery Rate (FDR) corrected at q=0.05. The threshold is lower (Z≈3), which shows similar output as the first option, but somehow more voxels are colored, including some weaker activations that were cut off in the p&lt;0.001 map. FDR allows a controlled proportion (5%) of these voxels to be false positives, thus it detects more widespread activity than FWER. Family-Wise Error Rate (FWER) correction at p&lt;0.05 (Bonferroni). Only the most significant clusters remain. The required Z≈4.9 is very high, so all but the strongest signals (e.g., primary motor cortex) are gone. This map has the fewest activations, reflecting a very strict control of false positives (no more than 5% chance of any false voxel)\n4.1 Glass Brain Views\nSometimes we want a quick look at the overall pattern of activations without anatomical details. Glass brain plots show a brain outline with projections of the active regions. This is useful for checking where clusters are generally located (especially subcortical vs cortical, bilateral vs unilateral).\n# Glass brain plot of the unthresholded map (thresholding lightly for visibility)\nplotting.plot_glass_brain(stat_img, threshold=3.0, colorbar=True, \n                          title=&quot;Glass brain (Z&gt;3 projection)&quot;)\n\nGlass brain view of the activation map. This projects the supra-threshold voxels in each view (frontal, lateral, axial) as if we’re looking through a transparent brain. It provides an overview of cluster locations without structural context.\nWe can also adjust the glass brain view:\n\ndisplay_mode can be set to l, r, lzry, etc., to show specific hemispheric projections (e.g., only left, only right, or multiple panels) ￼.\nblack_bg=True can be used for a dark background (useful if you plan to overlay on slides with dark background, for example).\n\n4.2 Interactive Viewing\nFor an interactive exploration, Nilearn offers view_img, which can be very handy in Jupyter notebooks. It lets you scroll through slices and adjust threshold dynamically in a widget or as an HTML file:\nview = plotting.view_img(stat_img, threshold=3.0, cmap=&#039;cold_hot&#039;)\nview  # This will display an interactive viewer in Jupyter\n\nYou can also specify cut_coords=[x, y, z] to center the view, or use view_img(stat_img, threshold=3.0).open_in_browser() to pop it out in a web browser. The interactive view supports features like dragging slice sliders and toggling the colorbar.\n(If working in a static environment, you can instead save the widget to an HTML file: view.save_as_html(&#039;interactive_map.html&#039;).)\nCustomizing Plots (Cuts, Colormaps, Backgrounds, Overlays)\nNilearn plotting functions have many options to customize the figure for publication-quality output. Here are some common customizations:\n\nCut coordinates and display modes: By default plot_stat_map shows 3 orthogonal cuts through a point of high activation. You can control this:\ncut_coords=(x, y, z) will center the cuts at specific coordinates (in MNI space, usually).\ndisplay_mode can be ‘ortho’ (default), ‘x’, ‘y’, ‘z’ (single-axis views), or combinations like ‘xz’, ‘yz’, ‘xy’, or ‘mosaic’ for many slices. For example, to show multiple axial slices, you could do display_mode=‘z’, cut_coords=7 (seven evenly spaced axial slices).\nFor a montage of slices in a grid, use display_mode=&#039;mosaic&#039; and specify how many slices in each direction via a tuple. E.g., display_mode=&#039;mosaic&#039;, cut_coords=(7, 5, 5) would create a mosaic with 7 slices in z, 5 in x, 5 in y directions.\nColormaps and colorbars: The default colormap for plot_stat_map is a diverging red-blue (‘cold_hot’) good for showing positive vs negative deviations. You can set the cmap parameter to any matplotlib colormap (e.g., ‘viridis’, ‘coolwarm’, ‘RdBu_r’, etc.) to suit your data. If your map has only positive values (e.g., all Z scores positive for an activation contrast), you might use a sequential colormap and set vmin=0 or symmetric_cbar=False so that the color range isn’t forced to be symmetrical around zero. The colorbar argument (True/False) toggles the side color scale bar. You can also format the colorbar numbers via cbar_tick_format (for example, “%.2g” or “%i”).\nBackground image: By default, plot_stat_map uses a standard anatomical (MNI152) background image behind the activations. You can supply your own anatomical image via the bg_img parameter (for example, an average T1 image of the group or an anatomical scan of a representative subject). If you set bg_img=None, no underlay is used – the activation blobs will be plotted on a blank background (which can be black or white). The parameter black_bg=True will set the background to black (and also automatically switch the colormap text to white for contrast) – this is often useful for glass brains or if you prefer a dark theme. Conversely, for a white background (which is default when black_bg=False or ‘auto’), ensure your colormap shows up well on light gray/white.\nAdding overlays/contours: You might want to overlay additional information, such as ROI boundaries or a second map’s contours, on your plot. Nilearn’s plotting returns a display object that has methods like add_overlay and add_contours.\nFigure size and resolution: For publication, you’ll often need high-resolution figures. You can control the figure size by creating a matplotlib figure first and passing it to the plotting function (via the figure or axes parameter)\n\nTo save a plot, you can either use the output_file argument in plotting functions (which accepts a filename and will save at default resolution) ￼, or save the display object. For instance:\n \ndisplay.savefig(&#039;activation_map_highres.png&#039;, dpi=300)\n \nHigh-resolution tip: If your activation map is low-resolution (blurry or blocky), you can resample it to a higher resolution template before plotting. For example, load a 1mm MNI template and resample the stat map to it.\nfrom nilearn.image import resample_to_img\n \ntemplate = datasets.load_mni152_template(resolution=1) # 1mm MNI brain\n \nresampled_stat = resample_to_img(thresholded_map_fdr, template) # resample stat map to 1mm\n \nplotting.plot_stat_map(resampled_stat, bg_img=template, threshold=thresh_fdr,\n \ncut_coords=cut_coords, title=&quot;High-res background (1mm)&quot;)\n\nFinally, remember that the interactive viewer (from view_img) can be saved to HTML as mentioned, which is useful for sharing results with colleagues who can’t run the code. Static images can be saved using the methods above. By organizing the code and explanations as shown, you can easily adapt this notebook to visualize your own fMRI results with publication-quality figures.\n# plot image using view_img and open in a browser\n \nview = plotting.view_img(resampled_stat, bg_img=template, threshold=thresh_fdr,\n \ncut_coords=cut_coords, cmap=&#039;cold_hot&#039;).open_in_browser()\nWith the opened image in browser, you can save it as an HTML file for sharing or further exploration. This interactive viewer allows you to scroll through slices, adjust thresholds, and visualize the activation patterns dynamically.\nFurther resources:\n\n\nNilearn documentation on plotting: see the Visualization user guide and the plot_stat_map and plot_glass_brain function references for more details.\n\n\nNilearn example “Statistical testing of a second-level analysis” for a complete pipeline of group analysis and thresholding .\n\n\nAndy’s Brain Book appendix on multiple comparisons for an accessible explanation of these statistical concepts.\n\n"},"index":{"slug":"index","filePath":"content/index.md","title":"MSense Lab fMRI Tutorial","links":[],"tags":[],"content":"This tutorial draws from a series of sessions held during our MSense lab seminars. The content is regularly updated."}}
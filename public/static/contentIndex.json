{"fMRI-Tutorial/1.1-Basic-Linux-Commands":{"slug":"fMRI-Tutorial/1.1-Basic-Linux-Commands","filePath":"content/fMRI Tutorial/1.1 Basic Linux Commands.md","title":"1.1 Basic Linux Commands","links":["tmux"],"tags":["Command"],"content":"1 Basic commands\nShells such as Bash, Zsh, and Tcsh provide powerful command-line environments for Unix-like systems, each with its own syntax and features. While they all allow users to execute multiple commands sequentially or in parallel, their scripting syntax and built-in functionalities differ. For example, Bash offers rich scripting capabilities and is widely adopted for its compatibility and flexibility, while Zsh includes advanced features like improved tab completion and a robust plugin system. Tcsh, known for its C-like syntax, is often preferred for interactive use due to its user-friendly command-line editing features. For example, AFNI uses the tsch shell.\nPrint Working Directory (pwd) will show you the current directory.\npwd\n\nList the files in the directory (ls), with some parameters, such as long format with human readable.\nls -lh  directory\n\nIf you want to list files Recursively, you can use ls -R.\nChange the Current Directory (cd)\ncd to/sub/folder\n\nCoPy files (cp)\ncp original duplicate\n\nrsync — a fast, versatile, remote (and local) file-copying tool. You need to have an SSH connection setup.\n#copy from remote files to local\nrsync -rv username@remote:/path/to/directory local/directory\n \n#copy from local to remote\nrsync -rv local/directory username@remote:/path/to/destination\nYou can remove (rm) files, rename (rm) files, or move (mv) files to elsewhere.\nFor directories, you can make a new directory (mkdir) or remove directory (rmdir).\nTo quickly show a text file in the console, you can cat the file.\ncat readme.txt\n\nIf you only want to show the head (head) or the tail (tail), you can do\n# show the first 5 rows \nhead -n 5 readme.txt\n\nSimilarly, you can selective show columns from a table text file using cut, but you need to specify the columns (fields, -f) and tell the delimiter (-d).\ncut -f 2-5,8 -d , exp1.csv\n\nThe above command will show the columns 2 to 5 and column 8 with the delimiter of , from the csv file exp1.csv.\nTo store a command’s output in a file &gt;, which serves as a pipe line.\nhead readme.txt &gt; head.csv\n\nIf you want to append the output to the existing file, you replace &gt; with &gt;&gt;.\nA general pipe symbol is |. For example, list the last 5 trials of the first block (48 trials) from data.csv file,\nhead -n 48 data.csv | tail -n 5\n\n2 Regular expression\ngrep takes a piece of text followed by one or more filenames and prints all of the lines in those files that contain specified text or patterns.\nFor example, grep memory readme.txt prints lines from readme.txt that contain ‘memory’.\ngrep common parameters:\n\n-c: a count of matching lines rather than the lines themselves\n-i: ignore case (e.g., treat “Memory” and ‘memory’ as the same)\n-l: print the names of files that contain matches, not the matches\n-n: print line numbers for matching lines\n-v: invert selection, list not matched lines.\n\nThe command wc (short for “word count”) prints the number of characters, words, and lines in a file. You can make it print only one of these using -c, -w, or -l respectively.\nFor example, let’s suppose we have Exp1.csv file with the column 8 name curDur. You want to show how many trials with curDur = 0.5:\ncut -f 8 -d , Exp1.csv | grep -v curDur | grep 0.5 | wc -l \n\nwildcards\nThe most common wildcard is *, which mean ‘match everything’. There are others:\n\n? matches a single character, so sub-0?.txt will match sub-01.txt or sub-02.txt, but not sub-11.txt.\n[...] matches any one of the characters inside the square brackets, so 201[78].txt matches 2017.txt or 2018.txt, but not 2016.txt.\n{...} matches any of the comma-separated patterns inside the curly brackets, so {*.txt, *.csv} matches any file whose name ends with .txt or .csv.\n\n\narithmatic expansion. $[] or $(()) will calculate the value inside. For example, echo $[3 + 2*4] will output 11.\n\nVariable replacements\nYou can construct a text string using variables. For example,\na=sub\necho ${a}-01.txt\n\nit will output sub-01.txt.  Note, if $a is separable from the rest, such as the above example, {} can be omitted. Otherwise, it may caused a confusion. For example,\necho $ab.txt  #error\necho ${a}b.txt #output: subb.txt\n\nsorting\nsort puts data in order. By default, it in ascending order, but the flags -r can reverse the order (i.e., descending), -n to sort numerically, -b to ignore leading blanks, and -f tells it to fold case (i.e., be case-insensitive).\nunique\nuniq is to remove duplicated lines that are adjacent. So to remove all duplicate lines, you need to sort the data first. With the flag -c, it also output the number of duplicates.\nFor example, count the number of trials for each sample durations (column 8 curDur) from Exp1.csv file:\ncut -d , -f 8 Exp1.csv | grep -v curDur | sort | uniq -c\n\n3 String manipulations\nManipulating file names is a common task using command lines. For example, we want to substitute a file name with a specified pattern. Stream editor sed command can perform many functions on file like searching, finding and replacing, insertion, or deletion.\nReplacing or substituting string\nsed &#039;s/unix/linux&#039; aText.txt\n\nIt will replace the word unix with linux in the file. Here, s specifies the substitution operation, and the ”/” delimiters are used.\nReplacing the nth occurrence of a pattern in a line by adding a number at the end of the expression. For example,\nsed &#039;s/unix/linux/2&#039; aText.txt\n\nReplacing all the occurrence of the pattern in a line with the flag /g (global replacement)\nsed &#039;s/unix/linux/g&#039; aText.txt\n\nReplacing from nth occurrence to all occurrence in a line. Use the combination of number and g flag together.\nsed &#039;s/unix/linux/3g&#039; aText.txt\n\nThe delimiter / could be |.\n\n\n                  \n                  Task \n                  \n                \n\nSuppose you want to replace all filenames with ‘_run’ to ‘_task-bisection_run’. Think about the pipe and commands.\n\n\n\n\n                  \n                  Solution\n                  \n                \n\nls -R | grep _run | sed &#039;s/_run/_task-bisection_run/&#039;\n\n\n\n4 tmux\ntmux is a command-line terminal multiplexer for Unix-like systems. You can control tmux using key combinations; you first type a prefix key combination (by default ctrl + b) followed by additional command keys.\n# list existing tmux sessions\n$ tmux ls\n# create a new session\n$ tmux new -s fmriAna\n\nTo make the connection and remember what you are doing; I recommend using tmux.\nBasic usage as follows:\n# list existing tmux sessions\ntmux ls\n \n# create a new session\ntmux new -s fmriAna\n \n# attach to an existing tmux session\ntmux a -t mysession\n \n# terminate an existing tmux session\ntmux kill-session -t mysession\n \n# creat a new window (ctrl + b, c)\n# navigate windows (ctrl + b, [0-9])\n# flip to next window (ctrl + b, n)\n# split horizontal (ctrl+b, &quot;)\n# split vertical (ctrl+b, %)"},"fMRI-Tutorial/1.2-GNode-and-Datalad":{"slug":"fMRI-Tutorial/1.2-GNode-and-Datalad","filePath":"content/fMRI Tutorial/1.2 GNode and Datalad.md","title":"1.2 GNode and Datalad","links":["knowledge/git-annex"],"tags":["GNode","Datalad"],"content":"1. G-Node\nFocusing on the development and free distribution of tools for handling and analyzing neurophysiological data, G-Node aims to address these aspects as part of the International Neuroinformatics Coordinating Facility (INCF) and the German Bernstein Network for Computational Neuroscience (NNCN). G-Node also serves as an international forum for Computational Neuroscientists interested in sharing experimental data and tools for data analysis and modeling. G-Node is funded through the German Federal Ministry of Education and Research and hosted by Ludwig-Maximilians-Universität München.\nBenefits of G-Node over Github:\n\nHosting large binary files\nAssigning unique DOI. It is good to open data and get citations.\nUsing git-annex as infrastructure helps manage versions.\n\ngin\nManagement of scientific data, including consistent organization, annotation, and storage of data, is a challenging task. Accessing and managing data from multiple workplaces while keeping it in sync, backed up, and easily accessible from within or outside the lab is even more demanding. The GIN (G-Node Infrastructure) service is a free and open data management system designed for comprehensive and reproducible management of neuroscientific data.\ngin client installation\nPlease see the official website for installations. For Mac OS users, the easiest way to install the client on macOS is via homebrew. G-Node homebrew formulae are maintained in the G-Node tap. Install the client, including any dependencies, with:\nbrew tap g-node/pkg\nbrew install g-node/pkg/gin-cli\n\nAlternatively, if you already have git and git-annex installed on your system, or you want to install them manually or via homebrew, the recommended and simplest way to install git-annex is via Homebrew using brew install git-annex. Alternatively, download git-annex from the git-annex website.\nOnce you’ve installed git-annex, simply download the gin client for macOS, extract the archive, and put the file named gin in a location that’s included in your $PATH.\nBasic usage of gin\n\nregister g-node.org website and sign into the GIN Server.\nCreate a new repository using the ”+” on the top right. Alterantively, you can create locally:\n\ngin create &lt;repository name&gt;\n\n\nCopy new files into the newly created directory via Drag &amp; Drop, Copy &amp; Paste etc.\nIn the GIN client (terminal) window, navigate into the newly created local workspace by typing cd &lt;repository name&gt;.\nUpload the new files using\n\n gin upload .\n\nNote the period at the end of the command. This command will commit your changes. In other words, it will detect the new files in the directory, add them to the repository, and start uploading to the GIN server. Every time you perform a gin upload . the changes are saved and uploaded and a checkpoint is made of your data.\nYou can instead upload individual files or directories by listing them on the command line. For example:\n    gin upload file1.data recordings/recording1.h5\n\nThis will upload changes made to two files: file1.data and recording1.h5, where the latter is in the recordings directory.\nNote that upload here doesn’t only mean sending new files and changes to the server. This command sends all changes made in the directory to the server, including deletions, renames, etc. Therefore, if you delete files from the directory on your computer and perform a gin upload, the deletion will also be sent and the file will be removed from the server as well. Such changes can be synchronized without uploading any new files by not specifying any files or directories.\n    gin upload\n\nFetch any repository updates from the server\nIf changes are made to your data elsewhere, for example on another computer (assuming they were uploaded to the server), or from another user that you share your data with, you can download these changes by typing the download command from within the repository.\ngin download\n\nThis command will only download changes made to the repository (file deletions, renames, etc.) but any new files are downloaded as placeholders. Placeholder files are empty files that represent files uploaded to the repository but do not hold any of the data. This is useful for downloading the contents of larger files on demand without downloading the entire repository.\nIf you would like to download all the data contained in a repository, you can do so using the --content flag.\ngin download --content\n\nThis will synchronize the local directory with all changes made on the server and download the content of all files.\nSelective download\nWhen new data has become available or existing files have been changed on the GIN server, a selected subset of the changes can be downloaded to the local workspace.\n\nDownload a summary of the changes on the GIN server using gin download. IMPORTANT: This does not download any data. New files and files changed on the GIN server are considered to be “unsynced”.\nUse gin ls to check the sync status of the files in the repository.\nUse gin get-content &lt;file name&gt; to download the data of a specific file.\n\nSelective upload\nWhen new data has been added to or existing files changed in the local workspace, a selected subset of the changes can be uploaded to the GIN server.\n\nUse gin ls to check the sync status of the files in the repository.\nUse gin upload &lt;file name&gt; to only upload the specified new file or changes to the specified existing file.\n\n2 DataLad\nA good start of Datalad Handbook.\nIt assists with the combination of all things necessary in the digital workflow of data and science.\nDataLad only cares (knows) about two things: Datasets and files. A dataset is a Git repository. A DataLad dataset can take care of managing and version controlling arbitrarily large data.\nDataLad can manage Gnode repository, see Gin and DataLad.\nBasic usage:\ndatalad save -m &#039;save something&#039;\ndatalad update \ndatalad push\n\nThis is similar to the follow traditional git commands\ngit add\ngit commit\ngit pull\ngit push\n"},"fMRI-Tutorial/1.3-Cloud-Computing-and-Access":{"slug":"fMRI-Tutorial/1.3-Cloud-Computing-and-Access","filePath":"content/fMRI Tutorial/1.3 Cloud Computing and Access.md","title":"1.3 Cloud Computing","links":[],"tags":["Cloud","HPC"],"content":"aThe following information is useful for those who can access Linux cluster at LRZ.de\nAccess Cloud Computing\nMsense Lab also has two cloud computing systems, Lora and Emma. Emma is accessible from the external internet, while Lora is walled by the intranet.\n\n\n                  \n                  Tips\n                  \n                \n\nFor convenience, it’s better that you create aliases for those servers. For example, in your .zshrc or .bashrc, put similar lines:\nalias lora=&quot;10.195.2.##&quot; (please change the ip accordingly)\n\n\n1. Via SSH\n# access to Lora\nssh -Y your_account@lora\n\n\n2. Via VNC\n\nVia VNC\nFirst, you need to tunnel to the server and initiate the VNC server (this only needs to be done once). The following commands should be run on the server side.\n\n2.1 Setup VNC password\nvncpasswd\nYou’ll be prompted to enter and verify your password. This password is required when connecting with your VNC client.\n2.2 Start VNC server\nvncserver\nIt will show you “New ‘X’ desktop is lora:#“. Remember this, we will first kill it and edit the file -  xstartup and restart it again.\n2.3 Edit VNC xstartup file\nFirst, kill the VNC service\nvncserver -kill :# (replace the number)\nNow edit the xstartup\nnano ~/.vnc/xstartup\nEdit/Put the following text in the file:\n#!/bin/sh\nxrdb $HOME/.Xresources\n\n# Start Xfce\nstartxfce4 &amp;\n\n2.4 Start VNC server localhost\nvncserver -localhost -geometry 1024x768\n\nDepending on your preference, you can specify the dimension of the virtual desktop. If you want to change it, you have to kill the service first, and restart it again (see above instructions). And remember which port number it started. You can check it in the ~/.vnc/ folder, by finding the lora:#.pd file.\n2.5 Local connection\nNow you can exit the server, and start a local command window, type:\n# Access to Lora and replace the # with your VNC port number\nssh -L 590#:localhost:590# -C -N -l your_account 10.195.?.?\n\nReplace the correct server IP, depending on your server.\nNext, you can start your local VNC (under Mac) using “Finder ⇒ Go ⇒ Connect to Server”, type in:\nvnc://localhost:590#\n\nThe connection password is the password you set earlier on.\n3. Via Code Server\nThe code server is configured individually. If your account is set up, you can first establish the tunnel:\nssh -Y -N -L 808@:localhost:808@ your_account@lora_ip\nPlease replace the port number, account name, and IP above. And leave this command window open.\nNext, open your browser and type localhost:808@. A code server interface will appear. Code Server is convenient when you don’t have a local installation of Visual Studio Code. However, if you accidentally close the brower tab, you may lose some working that is running.\n4. Via Visual Studio Code\nYou need to install ‘remote-SSH’ and ‘Remote Explorer’ plugins. After you install these plugins, by click the left icon of ‘Remote Explorer’, add remote SSH, using the option 1 ‘Via SSH’ to connect to the server.\nWhen the VS Code connects to the server, it will show the connection in the lower-left corner. The first time it opens nothing. You need to select ‘Open Folder’ to open a desired destination.\n5. Mapping the remote server locally\nThis is desirable when checking remote MRI results like local files. It won’t do any remote computation. And it requires changing the security level in your local Mac computer.\nFirst, install MacFUSE and SSHFS\nbrew install macfuse\nbrew tap gromgit/fuse\nbrew install gromgit/fuse/sshfs\nSecond, create a local mount point\nmkdir ~/dss\nThird, mount the remote linux server folder:\nsshfs username@server:/path/remote ~/dss\nAccess Linux Cluster\nIt requires a Linux cluster LRZ account and 2FA. Details can be found in the LRZ website.\n# connect to linux cluster at LRZ\n \nssh -Y your_account@cool.hpc.lrz.de\n \n1. Resource\nOverview of cluster specifications and limits\ndoku.lrz.de/job-processing-on-the-linux-cluster-10745970.html\nStatus here:\ndoku.lrz.de/high-performance-computing-10613431.html\n2. Install singularity\nWhen logged in to the Linux cluster (on the login nodes), you can follow this recipe (this has been done):\nmodule rm intel-mpi intel-mkl intel  \nmodule load user_spack  \nspack install squashfs squashfuse  \nspack load squashfs squashfuse  \nspack install singularity ~suid  \nspack -c&quot;modules:tcl:blacklist_implicits:False&quot; module tcl refresh --upstream-modules squashfs singularity\n\nUsage later is then simple (new terminal/or in a Slurm Job):\n\nmodule use spack/modules/x86_avx2/linux-sles15-haswell   \nmodule load singularity squashfs  \n\n\n3. Slurm partition and job settings\nThe following information needs to be updated; please check lrz.\njob settings\n**cm2_std:** \n--clusters=cm2  \n--partition=cm2_std  \n--qos=cm2_std\n\n**cm2_inter**:\n--clusters=inter  \n--partition=cm2_inter\n\n**cm2_inter_large_mem**\n56 cpu, memory 120 G\n--clusters=inter  \n--partition=cm2_inter_large_mem  \n--mem=&lt;memory_per_node_MB&gt;M\n\nCommon Slurm commands for job management:\n56 cpu 56 G\nsqueue -M cm2_tiny -u $USER  \nscancel -M cm2_tiny &lt;JOB-ID&gt;  \nsacct -M cm2_tiny -X -u $USER --starttime=2021-01-01T00:00:01\n\nsqueue -M inter -u $USER  \nscancel -M inter &lt;JOB-ID&gt;  \nsacct -M inter -X -u $USER --starttime=2021-01-01T00:00:01\n\n4. Interactive\nsalloc -N 1 -M inter -p cm4_inter -t 8:00:00\n\n\nshow CPUs and Memory\nlscpu | grep &#039;^CPU(s):&#039;\nfree -h\n\nsubmit to slurm\nsbatch sing_cluster.sh\n"},"fMRI-Tutorial/1.4-Neurocommand":{"slug":"fMRI-Tutorial/1.4-Neurocommand","filePath":"content/fMRI Tutorial/1.4 Neurocommand.md","title":"1.4 Neurocommand","links":[],"tags":["Neurocommand"],"content":"Neurocommand is a command-line interface to Neurodesk, which is designed to run on virtual machine or HPC.\nTo get NeuroCommand to work, you need to tell the bash shell system where to find the singularity folder and modules. For those set up users, the following codes are already in your .bashrc.\nexport PATH=~/bin:/usr/local/go/bin:${PATH}\nsource /etc/profile.d/lmod.sh\n \nexport SINGULARITY_BINDPATH=&#039;/cvmfs,/mnt,/home,/dss&#039;\nexport SINGULARITYENV_TEMPLATEFLOW_HOME=/dss/.templateflow\nmodule use /cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/*\n1 Command line and bash file usage\nmodule load module_name\nmodule list\nFor example, if you want to load module, you can use\nmodule load fsl\nmodule list\nit shows\nCurrently Loaded Modules:\n  1) fsl/6.0.7.16\n\nmodule avail can show all modules available from the Neurocommand.\nScenario 1: Interactive Analysis\nIf you tunnel to the server with option -Y (i.e., ssh -Y account@lora), you should be able to send back fsl interface back to your local computer.\nmodule load fsl\nFeat &amp;\nIt will show the ‘Feat’ interface locally.\n\n\n                  \n                  Note\n                  \n                \n\nTunnelling via SSH for the interface may be slow, particularly when you want to open the design matrix with multiple inputs. A better way to do this is via VNC interface, remotely running Feat or related software.\n\n\nScenario 2: Batch analysis using bash files\nThe following is an example of using the bet function from the fsl package in a bash file.\n#!/bin/bash\nmodule load fsl\n# Set the main BIDS directory (modify if needed)\nBIDS_DIR=&quot;/dss/open/ds000102&quot;\n# Loop through subject folders\nfor subj in ${BIDS_DIR}/sub-*; do\n\tsubj_id=$(basename ${subj}) # Extract subject ID (e.g., sub-01)\n\t# Define input and output paths\n\tanat_dir=&quot;${subj}/anat&quot;\n\tinput_image=&quot;${anat_dir}/${subj_id}_T1w.nii.gz&quot;\n\toutput_image=&quot;${anat_dir}/${subj_id}_T1w_brain.nii.gz&quot;\n\t# Check if input file exists\n\tif [ -f &quot;${input_image}&quot; ]; then\n\t\techo &quot;Running BET on ${input_image}...&quot;\n\t\tbet &quot;${input_image}&quot; &quot;${output_image}&quot; -R -f 0.25 -g 0 # Adjust parameters as needed\n\telse\n\t\techo &quot;T1w image not found for ${subj_id}, skipping...&quot;\n\tfi\ndone\n \necho &quot;Brain extraction completed for all subjects.&quot;\n2 Use in Python\nUnfortunately, the Python environment cannot directly read the bash environment. So you need to set up the environment first in your Python. If you are using Jupyter Notebook, copy the following code to the beginning of your notebook:\nimport os\nos.environ[&quot;SINGULARITY_BINDPATH&quot;] = &quot;/cvmfs,/mnt,/home,/home/lu32pog/dss&quot;\nos.environ[&quot;LMOD_CMD&quot;] = &quot;/usr/share/lmod/lmod/libexec/lmod&quot;\nos.environ[&quot;SINGULARITYENV_TEMPLATEFLOW_HOME&quot;] = &quot;/home/lu32pog/dss/.templateflow&quot;\nos.environ[&quot;MODULEPATH&quot;] = &quot;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/*&quot;\nThen you can load modules via lmod\n# need to load base (Python) from miniconda (all packages installed)\nimport lmod\n#await lmod.purge(force=True)\nawait lmod.load(&#039;fsl&#039;)\nawait lmod.list()\n3. Use with Google Colab\nThe official guidance is here\nAt the beginning of your notebook, you need to add:\nimport os\nos.environ[&quot;LD_PRELOAD&quot;] = &quot;&quot;;\nos.environ[&quot;APPTAINER_BINDPATH&quot;] = &quot;/content&quot;\nos.environ[&quot;MPLCONFIGDIR&quot;] = &quot;/content/matplotlib-mpldir&quot;\nos.environ[&quot;LMOD_CMD&quot;] = &quot;/usr/share/lmod/lmod/libexec/lmod&quot;\n \n!curl -J -O raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n!chmod +x googlecolab_setup.sh\n!./googlecolab_setup.sh\n \nos.environ[&quot;MODULEPATH&quot;] = &#039;:&#039;.join(map(str, list(map(lambda x: os.path.join(os.path.abspath(&#039;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/&#039;), x),os.listdir(&#039;/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/&#039;)))))\nThen, you can use it:\nimport lmod\nawait lmod.avail()\n \nawait lmod.load(&#039;fsl/6.0.4&#039;)\n!bet\n "},"fMRI-Tutorial/2.1-MRI-Data-Preparation":{"slug":"fMRI-Tutorial/2.1-MRI-Data-Preparation","filePath":"content/fMRI Tutorial/2.1 MRI Data Preparation.md","title":"2.1 MRI Data Preparation","links":["rfMRI_REST_PA_6.json"],"tags":["mri","DICOM","MRI"],"content":"2.1.1 MRI sequence\nTypes of MRI data depend on what MRI sequence you use. An MRI sequence is a number of radiofrequency pulses and gradients that result in a set of images with a particular appearance. For example, here is a property from the EPI bold sequence that we use.\n\nFor each experiment, we may use multiple sequences, such as resting state, T1 structure, EP2d Bold, etc. Running each sequence will generate images in a separate subfolder in your study folder, with the sub-folder name indicating the sequence. Below is one example from one sample data from Cemre’s study. Can you guess what sequences have been used?\n\nFrom the protocol, you will guess the sequence of that folder. For example, T1W_MPR_* are T1 scanning. EP_BOLD_BISEC_S* are fMRI BOLD signals for the task bisection (two sessions). The foldder name is a combination of protocol _ task _ serialNo.\nWithin each folder, it stores individual DICOM images, like this:\n\nDICOM stands for Digital Imaging and Communications in Medicine. Each image contain independent meta information, which is actually redundant for our purpose. If you run scan 1000 times, you will have 1000 DICOM files in that folder. To better organize your data, it is better convert DICOM data to Nifti file - a single file for each run.\n2.1.2 DICOM to Nifti converter\nWhen you copy data from an MRI computer, all files are stored in DICOM format, with each image (single scan) as a single file. After you run one session of the experiment, you probably get about two to three thousand files. It would be wise to first convert your files to nifti format (*.nii.gz).\nOne handy commandline tool for this job is dcm2nii DICOM to NIfTI converter. If you want to use the GUI version, another useful tool is MRIcroGL. MRIcroGL  is a medical image viewer that allows you to load overlays (e.g., statistical maps), draw regions of interest.\nWith MRIcroGL open, click menu Import - Convert DICOM to NIfTI, it will pop up dcm2niix converter page. Specify filename rules, store location, and other settings, you can quickly convert DICOM to NIfTI.\n\nTwo important parameters for naming rules is %p and %s, represent the protocol and series number respectively. This essentially is the folder name.\nThe DICOM files in each folder will be converted to two files:\n\none json file, which stores  image information.\n\nHere is one example of json file: rfMRI_REST_PA_6.json\n\n\none NIfTI file (*.nii or *.nii.gz).\n\nConverting DICOM to NIFTI can make your raw data tidy, but not necessary save your space. When you have multiple sequences and multiple participants, it is wise to organize your data folder in a standard structure. A common standard structure that is accepted by multiple analysis software (e.g., fMRIprep) is BIDS. We will discuss this in next chapter.\nHeudiconv\nheudiconv is a flexible DICOM converter for organizing brain imaging data into structured directory layouts.\n\nIt allows flexible directory layouts and naming schemes through customizable heuristics implementations.\nIt only converts the necessary DICOMs and ignores everything else in a directory.\nYou can keep links to DICOM files in the participant layout.\nUsing dcm2niix under the hood, it’s fast.\nIt can track the provenance of the conversion from DICOM to NIfTI in W3C PROV format.\nIt provides assistance in converting to BIDS.\nIt integrates with DataLad to place converted and original data under git/git-annex version control while automatically annotating files with sensitive information (e.g., non-defaced anatomicals, etc).\n\n\n\nIf you want to install in your local computer, one way is to use Docker, see instruction here\nheudiconv comes with existing heuristics which can be used as is, or as examples. For instance, the Heuristic convertall extracts standard metadata from all matching DICOMs. heudiconv creates mapping files, &lt;something&gt;.edit.text which lets researchers simply establish their own conversion mapping.\nHaving decided on a heuristic, you could use the command line (see detailed Usage):\nheudiconv -f HEURISTIC-FILE-OR-NAME -o OUTPUT-PATH --files INPUT-PATHs\nTutorials\nStep 0. Check the raw folders\nCheck each participant, each folder, remove those unfinished sessions (or those repeated sessions).\nStep 1. Create a heuristic file\nheudiconv --files raw/sub01/*/*/*.IMA -o Nifti/ -f convertall -s 01 -c none\nThis will create a skeleton in the Nifiti/.heudiconv hidden directory. In that directory, you will find a skeleton heuristic.py and dicominfo.tsv.\nCopy out the heuristic.py and dicominfo.tsv. These are two files we need to use to modify conversion rules.\nStep 2. Modify Heuristic\nSession 1: Create keys in infotodict, such as\nt1w = create_key(&#039;sub-{subject}/anat/sub-{subject}_T1w&#039;)\ntask1 = create_key(&#039;sub-{subject}/func/sub-{subject}_task-learning_run-1_bold&#039;)\ntask2 = create_key(&#039;sub-{subject}/func/sub-{subject}_task-learning_run-2_bold&#039;)\n \ninfo: dict[tuple[str, tuple[str, ...], None], list[str]] = {t1w: [], task1: [],\n\t\ttask2: []}\nSession 2: check dicominfo.tsv find the pattern and add inside the for loop:\nfor s in seqinfo:\n\tif (s.protocol_name == &#039;T1w_MPR&#039;) and (&#039;NORM&#039; in s.image_type):\n\t\tinfo[t1w] = [s.series_id] # folder name is the series_id\n\tif (s.protocol_name == &#039;ep_bold_mb4_task1&#039;):\n\t\t\tinfo[task1]=[s.series_id]\n\tif (s.protocol_name == &#039;ep_bold_mb4_task2&#039;):\t\n\t\tinfo[task2]=[s.series_id]\n \nreturn info\n \nStep 3. Conversion\nheudiconv --files dicom/{subject}/*/*.ima -o Nifti/ -f Nifti/code/heuristic.py -s 219 -ss itbs -c dcm2niix -b --minmeta --overwrite\nI would suggest to write a bash script to convert the file. Here is one example:\n#!/bin/bash\n# subjects array\nsubs=(&quot;SS14_014&quot; &quot;SS20_034&quot; &quot;DS11_032&quot; &quot;DS18_034&quot; &quot;SS05_017&quot; &quot;SS15_007&quot;)\n# set project folder\nproj_dir=/dss/lu32pog/zhangbei_2020/study2\n# loop through each subject\nfor subj in ${subs[@]}; do\n\techo $subj\n\t# use the first two letters and last three letters from subj to create subid\n\tsubid=${subj:0:2}${subj: -3}\n\t# using heduiconv\t\n\theudiconv --files ${proj_dir}/raw_data/${subj}/*/*/*.IMA -o ${proj_dir}/Nifti \\\n\t\t-f ${proj_dir}/Nifti/code/heuristic.py -s $subid -c dcm2niix -b --minmeta --overwrite\n \ndone\nOfficial tutorials can be found here.\nStep 4. Compress raw data\nGiven raw data include lots of small files, which will eventually explode our data science storage, not because of the size, but because of the limited number of files we can store (maximum: 8 million).  I suggest to put the following bash code (file) to your raw folder to compress individual subfolders.\n#!/bin/bash\n# Get a list of all subdirectories in the current directory\n \nfor dir in */ ; do\n\t# Remove the trailing slash to get the directory name\n\tdir_name=${dir%/}\n\t# Compress the directory into a tar.gz file with the same name\n\ttar -czvf &quot;${dir_name}.tar.gz&quot; &quot;${dir_name}&quot;\ndone\necho &quot;All subfolders have been compressed.&quot;\nAfter the compression, you can safely remove subfolders. You can modify the above script to do this.\n\n\n                  \n                  What not compress all subject data together \n                  \n                \n\nIn principle you can also compress all subject data to one single tar.gz file. However, I would recommend compressing individual participants, as we may want to convert or do further things with individual data. If you compress all data together, unzipping the whole dataset may take more time (prob. minutes to hours), and you have to expand all files. Even though there is a way to extract partial data, extracting folder structure from a huge zip file takes time. Trust me.\n\n"},"fMRI-Tutorial/2.2-BIDS-Structure":{"slug":"fMRI-Tutorial/2.2-BIDS-Structure","filePath":"content/fMRI Tutorial/2.2 BIDS Structure.md","title":"2.2 BIDS Structure","links":[],"tags":["BIDS","mri"],"content":"BIDS stands for the Brain Imaging Data Structure, which is a simple and intuitive way to organize and describe your data. The whole specification can be found on the official website. Here are some excerpts from the official website.\nBefore we go into details, let’s clarify several important terms.\n\nDataset - a set of neuroimaging and behavioral data acquired for a purpose of a particular study.\nSubject - a person or animal participating in the study. Used interchangeably with term Participant.\nSession - a logical grouping of neuroimaging and behavioral data consistent across subjects. In general, subjects will stay in the scanner during one session.\nSample - a sample pertaining to a subject such as tissue, primary cell or cell-free sample. The sample-&lt;label&gt; key/value pair is used to distinguish between different samples from the same subject.\nData type - a functional group of different types of data. BIDS defines the following data types:\n\nfunc (task based and resting state functional MRI)\ndwi (diffusion weighted imaging)\nfmap (field inhomogeneity mapping data such as field maps)\nanat (structural imaging such as T1, T2, PD, and so on)\nperf (perfusion)\nmeg (magnetoencephalography)\neeg (electroencephalography)\nieeg (intracranial electroencephalography)\nbeh (behavioral)\npet (positron emission tomography)\nmicr (microscopy)\nData files are contained in a directory named for the data type. In raw datasets, the data type directory is nested inside subject and (optionally) session directories.\n\n\nTask - a set of structured activities performed by the participant. Tasks are usually accompanied by stimuli and responses, and can greatly vary in complexity.\nEvent - something that happens or may be perceived by a test subject as happening at a particular instant during the recording. Events are most commonly associated with on- or offset of stimulus presentations, or with the distinct marker of on- or offset of a subject’s response or motor action.\nRun - an uninterrupted repetition of data acquisition that has the same acquisition parameters and task. Run is a synonym of a data acquisition.\n&lt;index&gt; - a nonnegative integer, possibly prefixed with arbitrary number of 0s for consistent indentation, for example, it is 01 in run-01 following run-&lt;index&gt; specification.\n&lt;label&gt; - an alphanumeric value, possibly prefixed with arbitrary number of 0s for consistent indentation, for example, it is rest in task-rest following task-&lt;label&gt; specification. Note that labels MUST not collide when casing is ignored (see Case collision intolerance).\nsuffix - an alphanumeric value, located after the key-value_ pairs (thus after the final _), right before the File extension, for example, it is eeg in sub-05_task-matchingpennies_eeg.vhdr.\n\nFile name structure\nA filename consists of a chain of entities, or key-value pairs, a suffix and an extension. Two prominent examples of entities are subject and session.\nFor a data file that was collected in a given session from a given subject, the filename MUST begin with the string sub-&lt;label&gt;_ses-&lt;label&gt;. If the session level is omitted in the folder structure, the filename MUST begin with the string sub-&lt;label&gt;, without ses-&lt;label&gt;.\nNote that sub-&lt;label&gt; corresponds to the subject entity because it has the sub- “key” and&lt;label&gt; “value”, where &lt;label&gt; would in a real data file correspond to a unique identifier of that subject, such as 01. The same holds for the session entity with its ses- key and its &lt;label&gt; value.\nThe extra session layer (at least one /ses-&lt;label&gt; subfolder) SHOULD be added for all subjects if at least one subject in the dataset has more than one session. If a /ses-&lt;label&gt; subfolder is included as part of the directory hierarchy, then the same ses-&lt;label&gt; key/value pair MUST also be included as part of the filenames themselves. Acquisition time of session can be defined in the sessions file.\nA chain of entities, followed by a suffix, connected by underscores (_) produces a human readable filename, such as sub-01_task-rest_eeg.edf.\nEntities within a filename MUST be unique. For example, the following filename is not valid because it uses the acq entity twice: sub-01_acq-laser_acq-uneven_electrodes.tsv\nA summary of all entities in BIDS and the order in which they MUST be specified is available in the entity table in the official website appendix.\nEntity-linked file collections\nAn entity-linked file collection is a set of files that are related to each other based on a repetitive acquisition of sequential data by changing acquisition parameters one (or multiple) at a time or by being inherent components of the same data. Entity-linked collections are identified by a common suffix, indicating the group of files that should be considered a logical unit. Within each collection, files MUST be distinguished from each other by at least one entity (for example, echo) that corresponds to an altered acquisition parameter (EchoTime) or that defines a component relationship (for example, part).\nSource vs. raw vs. derived data\nBIDS was originally designed to describe and apply consistent naming conventions to raw (unprocessed or minimally processed due to file format conversion) data. During analysis such data will be transformed and partial as well as final results will be saved. Derivatives of the raw data (other than products of DICOM to NIfTI conversion) MUST be kept separate from the raw data. This way one can protect the raw data from accidental changes by file permissions.\nSimilar rules apply to source data, which is defined as data before harmonization, reconstruction, and/or file format conversion (for example, E-Prime event logs or DICOM files). Storing actual source files with the data is preferred over links to external source repositories to maximize long term preservation, which would suffer if an external repository would not be available anymore. This specification currently does not go into the details of recommending a particular naming scheme for including different types of source data (such as the raw event logs or parameter files, before conversion to BIDS). However, in the case that these data are to be included:\n\n\nThese data MUST be kept in separate sourcedata folder with a similar folder structure as presented below for the BIDS-managed data. For example: sourcedata/sub-01/ses-pre/func/sub-01_ses-pre_task-rest_bold.dicom.tgz or sourcedata/sub-01/ses-pre/func/MyEvent.sce.\n\n\nA README file SHOULD be found at the root of the sourcedata folder or the derivatives folder, or both. This file should describe the nature of the raw data or the derived data. We RECOMMEND including the PDF print-out with the actual sequence parameters generated by the scanner in the sourcedata folder.\n\n\nAlternatively one can organize their data in the following way\n└─ my_dataset-1/\n   ├─ sourcedata/\n   │  ├─ sub-01/\n   │  ├─ sub-02/\n   │  └─ ... \n   ├─ ... \n   └─ derivatives/\n      ├─ pipeline_1/\n      ├─ pipeline_2/\n      └─ ... \n\n\nStorage of derived datasets\nDerivatives can be stored/distributed in two ways:\n\n\nUnder a derivatives/ subfolder in the root of the source BIDS dataset folder to make a clear distinction between raw data and results of data processing. A data processing pipeline will typically have a dedicated directory under which it stores all of its outputs. Different components of a pipeline can, however, also be stored under different subfolders. There are few restrictions on the directory names; it is RECOMMENDED to use the format &lt;pipeline&gt;-&lt;variant&gt; in cases where it is anticipated that the same pipeline will output more than one variant (for example, AFNI-blurring and AFNI-noblurring). For the sake of consistency, the subfolder name SHOULD be the GeneratedBy.Name field in data_description.json, optionally followed by a hyphen and a suffix (see Derived dataset and pipeline description).\nExample of derivatives with one directory per pipeline:\n&lt;dataset&gt;/derivatives/fmriprep-v1.4.1/sub-0001\n&lt;dataset&gt;/derivatives/spm/sub-0001\n&lt;dataset&gt;/derivatives/vbm/sub-0001\n\n\nExample of a pipeline with split derivative directories:\n&lt;dataset&gt;/derivatives/spm-preproc/sub-0001\n&lt;dataset&gt;/derivatives/spm-stats/sub-0001\n\n\nExample of a pipeline with nested derivative directories:\n&lt;dataset&gt;/derivatives/spm-preproc/sub-0001\n&lt;dataset&gt;/derivatives/spm-preproc/derivatives/spm-stats/sub-0001\n\n\n\n\nAs a standalone dataset independent of the source (raw or derived) BIDS dataset. This way of specifying derivatives is particularly useful when the source dataset is provided with read-only access, for publishing derivatives as independent bodies of work, or for describing derivatives that were created from more than one source dataset. The sourcedata/ subdirectory MAY be used to include the source dataset(s) that were used to generate the derivatives. Likewise, any code used to generate the derivatives from the source data MAY be included in the code/ subdirectory.\nExample of a derivative dataset including the raw dataset as source:\n\n\n└─ my_processed_data/\n   ├─ code/\n   │  ├─ processing_pipeline-1.0.0.img \n   │  ├─ hpc_submitter.sh \n   │  └─ ... \n   ├─ sourcedata/\n   │  ├─ sub-01/\n   │  ├─ sub-02/\n   │  └─ ... \n   ├─ sub-01/\n   ├─ sub-02/\n   └─ ... \n\n\nFile Formation specification\nImaging files\nAll imaging data MUST be stored using the NIfTI file format. We RECOMMEND using compressed NIfTI files (.nii.gz), either version 1.0 or 2.0. Imaging data SHOULD be converted to the NIfTI format using a tool that provides as much of the NIfTI header information (such as orientation and slice timing information) as possible. Since the NIfTI standard offers limited support for the various image acquisition parameters available in DICOM files, we RECOMMEND that users provide additional meta information extracted from DICOM files in a sidecar JSON file (with the same filename as the .nii[.gz] file, but with a .json extension). Extraction of BIDS compatible metadata can be performed using dcm2niix and dicm2nii DICOM to NIfTI converters. The BIDS-validator will check for conflicts between the JSON file and the data recorded in the NIfTI header.\nTabular files\nTabular data MUST be saved as tab delimited values (.tsv) files, that is, CSV files where commas are replaced by tabs. Tabs MUST be true tab characters and MUST NOT be a series of space characters. Each TSV file MUST start with a header line listing the names of all columns (with the exception of physiological and other continuous recordings). Names MUST be separated with tabs. It is RECOMMENDED that the column names in the header of the TSV file are written in snake_case with the first letter in lower case (for example, variable_name, not Variable_name). String values containing tabs MUST be escaped using double quotes. Missing and non-applicable values MUST be coded as n/a. Numerical values MUST employ the dot (.) as decimal separator and MAY be specified in scientific notation, using e or E to separate the significand from the exponent. TSV files MUST be in UTF-8 encoding.\nExample:\nonset   duration    response_time   correct stop_trial  go_trial\n200 200 0   n/a n/a n/a\n\n\nKey/value files (dictionaries)\nJavaScript Object Notation (JSON) files MUST be used for storing key/value pairs. JSON files MUST be in UTF-8 encoding. Extensive documentation of the format can be found at www.json.org/, and at tools.ietf.org/html/std90. Several editors have built-in support for JSON syntax highlighting that aids manual creation of such files. An online editor for JSON with built-in validation is available at jsoneditoronline.org. It is RECOMMENDED that keys in a JSON file are written in CamelCase with the first letter in upper case (for example, SamplingFrequency, not samplingFrequency). Note however, when a JSON file is used as an accompanying sidecar file for a TSV file, the keys linking a TSV column with their description in the JSON file need to follow the exact formatting as in the TSV file.\nExample of a hypothetical *_bold.json file, accompanying a *_bold.nii file:\n{\n  &quot;RepetitionTime&quot;: 3,\n  &quot;Instruction&quot;: &quot;Lie still and keep your eyes open&quot;\n}\n\n\nExample of a hypothetical *_events.json file, accompanying an *_events.tsv file. Note that the JSON file contains a key describing an arbitrary column stim_presentation_side in the TSV file it accompanies.\n{\n  &quot;stim_presentation_side&quot;: {\n    &quot;Levels&quot;: {\n      &quot;1&quot;: &quot;stimulus presented on LEFT side&quot;,\n      &quot;2&quot;: &quot;stimulus presented on RIGHT side&quot;\n    }\n  }\n}\n\n\nDirectory structure\nSingle session example\nThis is an example of the folder and file structure. Because there is only one session, the session level is not required by the format. For details on individual files see descriptions in the next section:\n├─ sub-control01/\n│  ├─ anat/\n│  │  ├─ sub-control01_T1w.nii.gz \n│  │  ├─ sub-control01_T1w.json \n│  │  ├─ sub-control01_T2w.nii.gz \n│  │  └─ sub-control01_T2w.json \n│  ├─ func/\n│  │  ├─ sub-control01_task-nback_bold.nii.gz \n│  │  ├─ sub-control01_task-nback_bold.json \n│  │  ├─ sub-control01_task-nback_events.tsv \n│  │  ├─ sub-control01_task-nback_physio.tsv.gz \n│  │  ├─ sub-control01_task-nback_physio.json \n│  │  └─ sub-control01_task-nback_sbref.nii.gz \n│  ├─ dwi/\n│  │  ├─ sub-control01_dwi.nii.gz \n│  │  ├─ sub-control01_dwi.bval \n│  │  └─ sub-control01_dwi.bvec \n│  └─ fmap/\n│     ├─ sub-control01_phasediff.nii.gz \n│     ├─ sub-control01_phasediff.json \n│     └─ sub-control01_magnitude1.nii.gz \n├─ code/\n│  └─ deface.py \n├─ derivatives/\n├─ README \n├─ participants.tsv \n├─ dataset_description.json \n└─ CHANGES \n\n\nUnspecified data\nAdditional files and folders containing raw data MAY be added as needed for special cases. All non-standard file entities SHOULD conform to BIDS-style naming conventions, including alphabetic entities and suffixes and alphanumeric labels/indices. Non-standard suffixes SHOULD reflect the nature of the data, and existing entities SHOULD be used when appropriate. For example, an ASSET calibration scan might be named sub-01_acq-ASSET_calibration.nii.gz.\nNon-standard files and directories should be named with care. Future BIDS efforts may standardize new entities and suffixes, changing the meaning of filenames and setting requirements on their contents or metadata. Validation and parsing tools MAY treat the presence of non-standard files and directories as an error, so consult the details of these tools for mechanisms to suppress warnings or provide interpretations of your filenames.\nBIDS common data types\nEach derivative data file SHOULD be described by a JSON file provided as a sidecar or higher up in the hierarchy of the derived dataset (according to the Inheritance Principle) unless a particular derivative includes REQUIRED metadata fields, in which case a JSON file is also REQUIRED. Each derivative type defines their own set of fields, but all of them share the following (non-required) ones:\nExamples\nPreprocessed bold NIfTI file in the original coordinate space of the original run. The location of the file in the original datasets is encoded in the RawSources metadata, and _desc-&lt;label&gt; is used to prevent clashing with the original filename.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_desc-preproc_bold.nii.gz \n      └─ sub-01_task-rest_desc-preproc_bold.json \n\n\n{\n    &quot;RawSources&quot;: [&quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;]\n}\n\n\nIf this file was generated with prior knowledge from additional sources, such as the same subject’s T1w, then both files MAY be included in RawSources.\n{\n    &quot;RawSources&quot;: [\n        &quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;,\n        &quot;sub-01/anat/sub-01_T1w.nii.gz&quot;\n    ]\n}\n\n\nOn the other hand, if a preprocessed version of the T1w image was used, and it also occurs in the derivatives, Sources and RawSources can both be specified.\n{\n    &quot;Sources&quot;: [\n        &quot;sub-01/anat/sub-01_desc-preproc_T1w.nii.gz&quot;\n    ],\n    &quot;RawSources&quot;: [\n        &quot;sub-01/func/sub-01_task-rest_bold.nii.gz&quot;\n    ]\n}\n\n\nSpatial references\nDerivatives are often aligned to a common spatial reference to allow for the comparison of acquired data across runs, sessions, subjects or datasets. A file may indicate the spatial reference to which it has been aligned using the space entity and/or the SpatialReference metadata.\nThe space entity may take any value in Image-Based Coordinate Systems.\nIf the space entity is omitted, or the space is not in the Standard template identifiers table, then the SpatialReference metadata is REQUIRED.\nSpatialReference key allowed values\nIn the case of images with multiple references, an object must link the relevant structures to reference files. If a single volumetric reference is used for multiple structures, the VolumeReference key MAY be used to reduce duplication. For CIFTI-2 images, the relevant structures are BrainStructure values defined in the BrainModel elements found in the CIFTI-2 header.\nExamples\nPreprocessed bold NIfTI file in individual coordinate space. Please mind that in this case SpatialReference key is REQUIRED.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_space-individual_bold.nii.gz \n      └─ sub-01_task-rest_space-individual_bold.json \n\n\n{\n    &quot;SpatialReference&quot;: &quot;sub-01/anat/sub-01_desc-combined_T1w.nii.gz&quot;\n}\n\n\nPreprocessed bold CIFTI-2 files that have been sampled to the fsLR surface meshes defined in the Conte69 atlas along with the MNI152NLin6Asym template. In this example, because all volumetric structures are sampled to the same reference, the VolumeReference key is used as a default, and only the surface references need to be specified by BrainStructure names.\n└─ sub-01/\n   └─ func/\n      ├─ sub-01_task-rest_space-fsLR_den-91k_bold.dtseries.nii \n      └─ sub-01_task-rest_space-fsLR_den-91k_bold.json \n\n\n{\n    &quot;SpatialReference&quot;: {\n        &quot;VolumeReference&quot;: &quot;templateflow.s3.amazonaws.com/tpl-MNI152NLin6Asym_res-02_T1w.nii.gz&quot;,\n        &quot;CIFTI_STRUCTURE_CORTEX_LEFT&quot;: &quot;github.com/mgxd/brainplot/raw/master/brainplot/Conte69_Atlas/Conte69.L.midthickness.32k_fs_LR.surf.gii&quot;,\n        &quot;CIFTI_STRUCTURE_CORTEX_RIGHT&quot;: &quot;github.com/mgxd/brainplot/raw/master/brainplot/Conte69_Atlas/Conte69.R.midthickness.32k_fs_LR.surf.gii&quot;\n    }\n}\n\n\nOther important specifications can be found in official website. For examples:\nPreprocessed or cleaned data\nBIDS derivatives"},"fMRI-Tutorial/2.3-Preprocessing-with-fmriPrep":{"slug":"fMRI-Tutorial/2.3-Preprocessing-with-fmriPrep","filePath":"content/fMRI Tutorial/2.3 Preprocessing with fmriPrep.md","title":"2.3 Preprocessing with fmriPrep","links":[],"tags":["mri","fmriPrep","fMRI"],"content":"A standard preprocessing pipeline of fMRI data looks as the follows:\nflowchart TD\n\tA[Raw Data] --&gt; B[Distortion corretion]\n\tB --&gt; C[Slice-Timing correction]\n\tA --&gt; C\n\tC --&gt; D[Registration and Normalization]\n\tD --&gt; E[Alignment and Motion Correction]\n\tE --&gt; F[Spatial Smoothing]\n\tF --&gt; G[Masking and Scaling]\n\tG --&gt; I[Checking preprocessing]\n\n\nA typical code for running fmriPrep with singularity is as follows:\n \nsingularity run \\\n\t--bind $bids_dir \\\n\t--bind $scratch_dir \\\n\t--bind $dss_dir \\\n\t$prepimg $bids_dir $bids_dir/derivatives/fmriprep \\\n\tparticipant \\\n\t--participant-label ${participants_to_run[*]} \\\n\t--skip-bids-validation \\\n\t--fs-license-file $proj_dir/fsl_licence.txt \\\n\t--output-spaces MNI152NLin2009cAsym T1w fsnative\\\n\t--bold2t1w-init register \\\n\t--bold2t1w-dof 12 \\\n\t--fs-no-reconall \\\n\t--force-bbr \\\n\t--skull-strip-t1w force \\\n\t--nthreads 56 \\\n\t--omp-nthreads 12 \\\n\t--mem_mb 56000 \\\n\t--stop-on-first-crash \\\n\t--notrack \\\n\t-v \\\n\t-w $scratch_dir\nOther bash scripts just set variables and environments to run the above singularity docker fmriprep.  Please refer to the official website for meanings of those parameters.\nThe following is an example of running parallel computing via Linux cluster slurm:\n#!/bin/bash\n#\n#SBATCH --job-name=ds_fmriprep\n#SBATCH --get-user-env\n#SBATCH --clusters=cm2_tiny\n#SBATCH --partition=cm2_tiny\n#SBATCH -t 24:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=56\n#SBATCH --mail-user=user@email_address\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --array=1-6%6\n#SBATCH -o /dss/dssfs03/ub111/ub111-dss-0001/tmp/suppression_fmripre-%A_%a.log\n \n# for linux cluster interactive\nmodule load slurm_setup\nmodule use $HOME/spack/modules/x86_avx2/linux-sles15-haswell\nmodule load singularity squashfs\n \n# Print job submission info\necho &quot;Slurm job ID: &quot; $SLURM_JOB_ID\ndate\n \n# setup folders\ndss_dir=/dss/dssfs03/ub111/ub111-dss-0001 #DSS storage\nproj_dir=${dss_dir} #project paraent folder\nproj_name=distractor_suppression # project name\nbids_dir=$proj_dir/$proj_name # bids directory\necho $bids_dir\n \nprepimg=${dss_dir}/fmriprep-23.1.4 # fmriprep image sandbox\n \n#. working directory \n# if exist $SCRATCH, use it, otherwise use dss_dir/tmp\nif [ -z &quot;$SCRATCH&quot; ]; then\n    scratch_dir=$dss_dir/tmp/${proj_name}\nelse\n    scratch_dir=$SCRATCH/${proj_name}\nfi\necho $scratch_dir\n# check if the folder exists\nif [ ! -d &quot;$scratch_dir&quot; ]; then\n    mkdir &quot;$scratch_dir&quot;\nfi\n \nexport MPLCONFIGDIR=$scratch_dir  #matplotlib temp folder\n \n# manually set participant groups\n# finished: &quot;DS001&quot; &quot;DS004&quot; &quot;SS022&quot; &quot;SS034&quot; &quot;DS016&quot; &quot;DS002&quot; &quot;DS005&quot; &quot;DS018&quot; &quot;DS032&quot; &quot;SS012&quot; &quot;SS008&quot; &quot;SS024&quot; &quot;DS033&quot; &quot;SS007&quot; &quot;SS020&quot; &quot;DS034&quot; &quot;SS017&quot; \n# \n \ngroup1=(&quot;SS015&quot; &quot;SS028&quot; &quot;DS021&quot;)\ngroup2=(&quot;SS030&quot; &quot;DS020&quot; &quot;DS023&quot;) \ngroup3=(&quot;DS029&quot; &quot;DS010&quot; &quot;DS027&quot;)  \ngroup4=(&quot;SS014&quot; &quot;SS026&quot; &quot;SS033&quot;)\ngroup5=(&quot;SS003&quot; &quot;SS019&quot; &quot;SS031&quot;)\ngroup6=(&quot;DS011&quot; &quot;DS024&quot;)\n \n# Select participant group based on SLURM_ARRAY_TASK_ID\ncase $SLURM_ARRAY_TASK_ID in\n    1)\n        participants_to_run=&quot;${group1[@]}&quot;\n        ;;\n    2)\n        participants_to_run=&quot;${group2[@]}&quot;\n        ;;\n    3)\n        participants_to_run=&quot;${group3[@]}&quot;\n        ;;\n    4)\n        participants_to_run=&quot;${group4[@]}&quot;\n        ;;\n    5)\n        participants_to_run=&quot;${group5[@]}&quot;\n        ;;\n    6)\n        participants_to_run=&quot;${group6[@]}&quot;\n        ;;\nesac\n \n \nsingularity run \\\n    --bind $bids_dir \\\n    --bind $scratch_dir \\\n    --bind $dss_dir \\\n    $prepimg $bids_dir $bids_dir/derivatives/fmriprep \\\n    participant \\\n    --participant-label ${participants_to_run[*]} \\\n    --skip-bids-validation \\\n    --fs-license-file $proj_dir/fsl_licence.txt \\\n    --output-spaces MNI152NLin2009cAsym T1w fsnative\\\n    --bold2t1w-init register \\\n    --bold2t1w-dof 12 \\\n    --fs-no-reconall \\\n    --force-bbr \\\n    --skull-strip-t1w force \\\n    --nthreads 56 \\\n    --omp-nthreads 12 \\\n    --mem_mb 56000 \\\n    --stop-on-first-crash \\\n    --notrack \\\n    -v \\\n    -w $scratch_dir\n \n#    --cifti-output 91k \\\n#    --medial-surface-nan \\\n \n# remove scratch (remember to remove it when all finished)\n# if it has multiple jobs, remove it after all jobs are finished, ... not here. \n# rm -rf $scratch_dir"},"fMRI-Tutorial/2.4-Quality-Control-with-MRIQC":{"slug":"fMRI-Tutorial/2.4-Quality-Control-with-MRIQC","filePath":"content/fMRI Tutorial/2.4 Quality Control with MRIQC.md","title":"2.4 Quality Control with MRIQC","links":[],"tags":["mri","MRIQC"],"content":"MRIQC extracts no-reference IQMs (image quality metrics) from structural (T1w and T2w) and functional MRI (magnetic resonance imaging) data.\nMRIQC is an open-source project, developed under the following software engineering principles:\n\n\nModularity and integrability: MRIQC implements a nipype workflow to integrate modular sub-workflows that rely upon third party software toolboxes such as ANTs and AFNI.\n\n\nMinimal preprocessing: the MRIQC workflows should be as minimal as possible to estimate the IQMs on the original data or their minimally processed derivatives.\n\n\nInteroperability and standards: MRIQC follows the the brain imaging data structure (BIDS), and it adopts the BIDS-App standard.\n\n\nReliability and robustness: the software undergoes frequent vetting sprints by testing its robustness against data variability (acquisition parameters, physiological differences, etc.) using images from OpenfMRI. Its reliability is permanently checked and maintained with CircleCI.\n\n\nA sample script to run mriqc via singularity:\n#!/bin/bash\n \n# check if in a cluster environment\nif env | grep -i cluster &gt; /dev/null; then\n    # 2. project directory (depending on the mount folder)\n    proj_dir=/path/to/project_id/ #linux cluster\n    ncpus=8\n    mriqc_img=$HOME/mriqc23\nelse\n    # 2. project directory (at lora server)\n    proj_dir=/dss/project_folder/ #Lora virtual machine\n    ncpus=8\n    mriqc_img=$HOME/mriqc23.simg\nfi\n \n# project name\nproj_name=bisection_mri\n \n# 3. working directory \nscratch_dir=$proj_dir/tmp/$proj_name\n \nexport MPLCONFIGDIR=$scratch_dir  #matplotlib temp folder\n \n# check if the folder exists\nif [ ! -d &quot;$scratch_dir&quot; ]; then\n    mkdir &quot;$scratch_dir&quot;\nfi\n# 4. bids directory\nbids_dir=$proj_dir/$proj_name\n \n# linux cluster sandbox \nsingularity run \\\n    --bind $bids_dir \\\n    --bind $scratch_dir \\\n    $mriqc_img $bids_dir $bids_dir/derivatives/mriqc \\\n    participant \\\n    --participant-label $1 \\\n    --no-sub \\\n    --nprocs $ncpus \\\n    --omp-nthreads 8 \\\n    --verbose-reports \\\n    -w $scratch_dir\n \nAfter running subject-wise, you can run group analysis, which will generate a group summary.\nRunning MRIQC for the first time\nBecause of a weird quirk in how MRIQC uses TemplateFlow, the first time each user runs MRIQC on the PNI server, MRIQC will need access to the internet to download some TemplateFlow files. The problem is that our compute nodes (i.e. the nodes used by Slurm) do not have access to the internet. However, the head node does. What this means is the first time each individual user runs MRIQC, you should run it on the head node, not using Slurm. After that, any subsequent time you run MRIQC (even if it is for a different project), you can use Slurm.\nHow to mark outliers\nKey indicators are framewise displacement (FD), temporal SNR, the spatial root mean square after temporal differencing (DVARS). Rule of thumbs: FD &gt; 0.3, DVARS &gt; 50, temporal SNR &lt; 30."},"fMRI-Tutorial/3.1-Hemodynamic-Response-Function-and-GLM":{"slug":"fMRI-Tutorial/3.1-Hemodynamic-Response-Function-and-GLM","filePath":"content/fMRI Tutorial/3.1 Hemodynamic Response Function and GLM.md","title":"3.1 Hemodynamic Response Function and GLM","links":[],"tags":["GLM","HRF"],"content":"Hemodynamic response function 1\nThe most widely used fMRI analysis is the general linear model (GLM) capitalizes on the changes in blood flow and oxygenation associated with neural activity (the hemodynamic response), and on the differing magnetic properties of oxygenated and deoxygenated blood.\nWhen a person performs a specific task or experiences a particular sensation, the brain’s blood flow increases in the regions that are associated with that activity. This increase in blood flow is known as the hemodynamic response.\nSuppose we present a stimulus event E_i to a participant at time 0. Let N_i(t) denote the neural activation induced by this event at time t and let B_i(t) denote the correspond BOLD response. So we have the following mapping:\nN_i(t) \\rightarrow f(\\cdot) \\rightarrow B_i(t)\nA system of this type mapping is called linear if and only if it satisfies the superposition principle:\nif f[N_1(t)] = B_1(t) and f[N_2(t)] = B_2(t), then it must be true that\nf[a_1N_1(t)+a_2N_2(t)] = a_1B_1(t)+a_2B_2(t)\nIf the superposition principle holds, then there is a straightforward way to determine the BOLD response to any neural activation from teh results of one simple experiment. Let’s consider a delta function \\delta(t - \\tau) = 1   \\text{ if } t= \\tau; \\text{ otherwise }=0,\nSuppose we have the following neural activity function after discretize. We can rewrite the function as\nN(t) = n_1\\delta(t-1) + n_2\\delta(t-2) + ... + n_9\\delta(t-9)\nMore general form will be:\nN(t) = \\sum_{\\tau = 0}^\\infty n_\\tau \\delta (t-\\tau)\nThe above is a convolution operation.\nWith the superposition principle, we can decompose the BOLD response:\nB(t) =f[N(t)] =f[\\sum_{\\tau = 0}^\\infty n_\\tau \\delta (t-\\tau)] = \\sum_{\\tau = 0}^t n_\\tau h(t-\\tau)\nHere h(t-\\tau) is the response of the system to the delta function, traditionally called the impulse response function. In the fMRI literature, however, h(t) is known as the hemodynamic response function (HRF).\nThe hemodynamic response function is not a synonym for the BOLD response. Rather, it is the hypothetical BOLD response to an idealized impulse of neural activation.\nIf we let successive values \\tau  small enough, we get an integral from the above equation:\nB(t) = \\int_0^t N(\\tau)h(t-\\tau)d\\tau\nThis is known as convolution integral, and often written as B(t) = N(t) \\star h(t).\nWhat does a hemodynamic response function look like?\nRichter &amp; Richter (2003) used a flashing checker board image for 500 ms and recorded the BOLD response in area V1 for three different groups. The HRFs look like:\n\nCandidates for HRF\n1. Gamma function\nThe Gmma function makes a good proximation, but no negative dip.\n2. A difference of two gamma functions\nOne for early peak, and one for the late dip.\n3. a weighted linear combination of basis function\nh(t) = \\theta_1 b_1(t) + \\theta_2 b_2(t) + \\theta_3 b_3(t) \nwhere b_i(t) = a\\cdot t^b \\cdot e^{-t}.\nCanonical HRF functions\nThree canonical hemodynamic response functions (HRFs) are commonly used: Gamma, Gamma-variate, and Double-Gamma functions.\nThe Gamma functions are the most commonly used HRFs in AFNI. They consist of a single gamma function, which describes the shape of the hemodynamic response. The Gamma functions are parameterized by the peak response time, the undershoot time, and the ratio of the peak response to the undershoot.\nGamma functions:\ng(t) = \\frac{t^{\\alpha - 1} e^{-\\frac{t}{\\beta}}}{\\beta^{\\alpha} \\Gamma(\\alpha)}\nHere, t represents time, \\alpha and \\beta are the shape and scale parameters of the gamma function, \\Gamma is the gamma function\nThe Gamma-variate functions are similar to the Gamma functions, but they consist of a combination of two gamma functions. The first gamma function describes the shape of the initial peak response, and the second gamma function describes the shape of the undershoot. The Gamma-variate functions are parameterized by the peak response time, the undershoot time, the peak-undershoot ratio, and the relative amplitude of the second gamma function.\nGamma-variate functions:\ng(t) = \\frac{t^{\\alpha_1 - 1} e^{-\\frac{t}{\\beta_1}}}{\\beta_1^{\\alpha_1} \\Gamma(\\alpha_1)} + \\frac{a_2 \\cdot t^{\\alpha_2 - 1} e^{-\\frac{t}{\\beta_2}}}{\\beta_2^{\\alpha_2} \\Gamma(\\alpha_2)}\nwhere a_2 is the relative amplitude of the second gamma function.\nThe Double-Gamma functions are also similar to the Gamma functions, but they consist of two gamma functions that are convolved together. The Double-Gamma functions are parameterized by the peak response time, the undershoot time, the peak-undershoot ratio, and the relative amplitude and time delay of the second gamma function.\nDouble-Gamma functions:\ng(t) = \\frac{t^{\\alpha_1 - 1} e^{-\\frac{t}{\\beta_1}}}{\\beta_1^{\\alpha_1} \\Gamma(\\alpha_1)} \\otimes \\frac{a_2 \\cdot t^{\\alpha_2 - 1} e^{-\\frac{t}{\\beta_2}}}{\\beta_2^{\\alpha_2} \\Gamma(\\alpha_2)}\n\\otimes denotes convolution. In the Gamma-variate and Double-Gamma functions, the parameters \\alpha_1, \\beta_1, \\alpha_2, and \\beta_2 are the shape and scale parameters of the two gamma functions, respectively.\nIn general, the choice of which HRF to use in AFNI depends on the specific research question and the data being analyzed. The Gamma functions are a good starting point for most applications, but the other HRFs may be more appropriate for certain types of data or research questions.\nGeneral linear model\nY = \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X3 + \\epsilon \nwhere beta is berta weights, X regressor, \\epsilon residual.\nIn fMRI analysis, we estimate the average amplitude of the BOLD signal in response to each condition in our model.\nSince every voxel has its own time-series, we do the procedure above for every voxel in the brain. This is known as a mass univariate analysis, since we estimate beta weights for each voxel’s time-series.\nTodo…\nFootnotes\n\n\nAshby, F. G. (2011). Statistical Analysis of fMRI data ↩\n\n\n"},"fMRI-Tutorial/3.2-First-level-Analysis-with-FSL":{"slug":"fMRI-Tutorial/3.2-First-level-Analysis-with-FSL","filePath":"content/fMRI Tutorial/3.2 First-level Analysis with FSL.md","title":"3.2 First-level Analysis with FSL","links":[],"tags":["GLM","FSL"],"content":"Before doing first-level analysis, you must run brain extraction using BET, ensure your functional and structural data are in NIFTI format, and ensure that you have any necessary event timing files (e.g., stimulus onset, durations).\nLaunch Feat GUI:\nFeat &amp;\nThe interactive interface of Feat from FSL:\n\nSet Up the First-Level Analysis\nDefine the Data Input\n• Data Directory:\nSelect the folder containing your functional data.\n• Data Type:\nChoose whether your design is “Time-series” (single run) or “Multiple sessions” (if you have more than one run).\nSpecify Preprocessing Options (if needed)\n• Motion Correction: Enable MCFLIRT if not already done.\n• Slice Timing Correction: Check this if your sequence requires correction (and if it wasn’t already applied).\n• Spatial Smoothing: Specify the smoothing kernel size (e.g., 5 mm FWHM).\n• High-Pass Filtering: Set the cutoff (e.g., 100 s) to remove low-frequency drifts.\nSet Up the Model\n• Design Setup: • Click on “Stats” in the GUI.\n• EVs (Explanatory Variables): Define your regressors. For each experimental condition, you can either use a block or event-related model:\n• Click “Add EV” for each condition.\n• Provide the timing file or manually input the onsets/durations.\nUsually we provide a three-column text for this with the option ‘Custom (3 column format)‘. Three columns are: Onset, Duration, and amplitude (1).\n• Temporal Derivatives (optional): Include them if you want to account for slight differences in response timing.\n• Contrasts:\n• Under the “Contrasts” tab, set up contrasts to test your hypotheses (e.g., condition A &gt; baseline).\n• Add as many contrasts as needed to test different effects.\nRegistration\n• Structural Registration:\n• Register your functional data to your high-resolution structural scan.\n• Use FLIRT for linear registration (and optionally FNIRT for nonlinear registration if needed).\n• Check the transformation matrices and overlay the images to verify alignment.\n• Standard Space Registration: If you plan to compare with group-level analyses, register to a standard template (e.g., MNI152 2mm).\nThen click ‘Go’ to run the Analysis and save the template, which we need to adapt for other subjects.\nAdapt template for all participants\nThe saved Feat configuration .fsf is actually a text file, which can be edited by any text editor. We open this saved file, and change the following fields to variables, which we will replace using a bash file:\n\nOutput directory\nTotal Volumes\nFunctional image location\nT1 image location\n3-column timing files\nThe following code is an example of replaced template.\n\n# Output directory\nset fmri(outputdir) &quot;OUTPUT_DIR&quot;\n\n# Total volumes\nset fmri(npts) TOTAL_VOLUMES\n\n# 4D AVW data or FEAT directory (1)\nset feat_files(1) &quot;FUNC_IMAGE&quot;\n\n# Subject&#039;s structural image for analysis 1\nset highres_files(1) &quot;T1_IMAGE&quot;\n\n# Custom EV file (EV 1)\nset fmri(custom1) &quot;COND1_FILE&quot;\n\n# Custom EV file (EV 2)\nset fmri(custom2) &quot;COND2_FILE&quot;\n\n\nIf you have multiple RUNs, you should also make them replaceable variables.\nNext, use a bash file to replace them to run Feat for individual subjects.\n#!/bin/bash\nSCRIPT_DIR=$(cd &quot;$(dirname &quot;$0&quot;)&quot; &amp;&amp; pwd)\nBIDS_DIR=$(cd &quot;$SCRIPT_DIR/../..&quot; &amp;&amp; pwd)\nDERIVATIVES_DIR=&quot;${BIDS_DIR}/derivatives&quot; # Output directory for first-level results\nFSF_TEMPLATE=&quot;${BIDS_DIR}/code/fsl/template_level.fsf&quot;\n \n# Ensure the derivatives directory exists\n \nmkdir -p &quot;${DERIVATIVES_DIR}&quot; \n \n# Number of parallel jobs to run\n \nNJOBS=8\n \n# Track jobs to prevent overload\njob_count=0\n \n# Loop over all subject folders\nfor subj in ${BIDS_DIR}/sub-*; do\n\tsubj_id=$(basename &quot;${subj}&quot;) # e.g., sub-01\n\tfunc_dir=&quot;${subj}/func&quot;\n \n\t# Locate the T1 brain image (common to all runs)\n\tt1_image=$(ls &quot;${subj}/anat/${subj_id}_T1w_brain.nii.gz&quot; 2&gt;/dev/null | head -n 1)\n\t# Loop over runs (e.g., run-1 and run-2)\n\t# depending how many runs you have\n\tfor run in 1 2; do\n\t\t# Locate functional image for this run\n\t\tfunc_image=$(ls &quot;${func_dir}/${subj_id}_task-flanker_run-${run}_bold.nii.gz&quot; 2&gt;/dev/null | head -n 1)\n\t\techo &quot;${func_dir}/${subj_id}_task-*_run-${run}_bold.nii.gz&quot;\n\t\t# Locate the event files for congruent and incongruent conditions for this run\n\t\tcond2_file=$(ls &quot;${func_dir}/congruent_run${run}.txt&quot; 2&gt;/dev/null | head -n 1)\n\t\tcond1_file=$(ls &quot;${func_dir}/incongruent_run${run}.txt&quot; 2&gt;/dev/null | head -n 1)\n\t\t# Skip if no functional image is found for this run\n \n\t\tif [ ! -f &quot;$func_image&quot; ]; then\n\t\t\techo &quot;No functional image found for ${subj_id} run-${run}, skipping...&quot;\n\t\t\tcontinue\n\t\tfi\n \n\t\t# Get the number of volumes (4th dimension) from the functional image\n\t\ttotal_vols=$(fslval &quot;$func_image&quot; dim4)\n\t\t\n\t\t# Define run-specific FSF file and FEAT output directory\n\t\tfsf_file=&quot;${DERIVATIVES_DIR}/fsl/design_${subj_id}_run-${run}.fsf&quot;\n\t\toutput_dir=&quot;${DERIVATIVES_DIR}/fsl/${subj_id}_run-${run}.feat&quot;\n\t\techo &quot;output_dir: ${output_dir}&quot;\n\t\techo &quot;fsf_file: ${fsf_file}&quot;\n\t\t\n\t\t# Skip if FEAT analysis has already been run for this run\n\t\tif [ -d &quot;${output_dir}&quot; ]; then\n\t\t\techo &quot;Skipping ${subj_id} run-${run}, FEAT already run.&quot;\n\t\t\tcontinue\n\t\tfi\n\t\n\t\t# Create a subject- and run-specific FSF file with updated paths and parameters.\n\t\t# Note: The FSF template should include placeholders such as SUBJ-ID, RUN_NUMBER,\n\t\t# OUTPUT_DIR, FUNC_IMAGE, T1_IMAGE, COND1_FILE, COND2_FILE, and TOTAL_VOLUMES.\n\t\tsed -e &quot;s/SUBJ-ID/${subj_id}/g&quot; \\\n\t\t\t-e &quot;s/RUN_NUMBER/run-${run}/g&quot; \\\n\t\t\t-e &quot;s|OUTPUT_DIR|${output_dir}|g&quot; \\\n\t\t\t-e &quot;s|FUNC_IMAGE|${func_image}|g&quot; \\\n\t\t\t-e &quot;s|T1_IMAGE|${t1_image}|g&quot; \\\n\t\t\t-e &quot;s|COND1_FILE|${cond1_file}|g&quot; \\\n\t\t\t-e &quot;s|COND2_FILE|${cond2_file}|g&quot; \\\n\t\t\t-e &quot;s/TOTAL_VOLUMES/${total_vols}/g&quot; \\\n\t\t\t&quot;${FSF_TEMPLATE}&quot; &gt; &quot;${fsf_file}&quot;\n\t\t\n\t\t# Run FEAT for this subject and run in the background\n\t\t\n\t\techo &quot;Running FEAT for ${subj_id} run-${run} with ${total_vols} volumes...&quot;\n\t\tfeat &quot;${fsf_file}&quot; &amp;\n\t\t\n\t\t# Track the number of background jobs and wait if the limit is reached\n\t\tjob_count=$((job_count + 1))\n\t\tif [ $job_count -eq $NJOBS ]; then\n\t\t\twait\n\t\t\tjob_count=0\n\t\tfi\n\t\t\n\tdone\ndone\n \n# Wait for any remaining background jobs to complete\nwait\necho &quot;First-level FEAT analysis completed for all subjects and runs.&quot;"},"fMRI-Tutorial/3.3-High-level-Analysis-with-FSL":{"slug":"fMRI-Tutorial/3.3-High-level-Analysis-with-FSL","filePath":"content/fMRI Tutorial/3.3 High-level Analysis with FSL.md","title":"3.3 High-level Analysis with FSL","links":[],"tags":["GLM","FSL"],"content":"High-level analysis in FSL aggregates results from individual subjects to draw inferences about a population. The typical steps involve:\n\nData collection: Gather first-level FEAT outputs (cope, varcope, and sometimes t-statistic images).\nDesign specification: Define your design matrix and contrast vectors to model group-level effects.\nEstimation: Use FSL’s FEAT (or FLAME) for mixed-effects analysis.\nInference: Correct for multiple comparisons and interpret cluster or voxel-level results.\n\n1. Prepare your first-level data\nAfter the first-level analysis, you have the necessary data organized. For example, you put all subject-wise data in subfolder derivatives:\n\n2. Create Design and Contrast Matrix\nOpen Feat &amp; GUI, and add all first-level feat folders or COPE files, depending on your experimental design.\nAndy Jahn’s brain book website provides excellent instructions on this. Please check his second-level analysis and third-level analysis for details.\nEssentially, you need to create a higher-level analysis by setting up a new FEAT design file. Here is an example of such design file:\n# Higher-level FEAT design example for one-sample t-test\n \n# Set the basic parameters\nset fmri(npts) 1\nset fmri(outputdir) &quot;group_level.feat&quot;\n \n# Input subject-level cope images\nset feat_files(1) &quot;/path/to/sub01/first_level.feat/stats/cope1.nii.gz&quot;\nset feat_files(2) &quot;/path/to/sub02/first_level.feat/stats/cope1.nii.gz&quot;\nset feat_files(3) &quot;/path/to/sub03/first_level.feat/stats/cope1.nii.gz&quot;\n# ... add more subjects as needed\n \n# Define the design matrix file paths\nset fmri(groupfile) &quot;feat_dir_list.txt&quot;\nset fmri(cope1) 1\n \n# Set up the design for a one-sample t-test\nset fmri(contrasts) &quot;1&quot;\nset fmri(contrast_names1) &quot;Mean Effect&quot;\n \n# Use FLAME stage 1 (or stage 1+2 for more accurate variance estimation)\nset fmri(flame_level) 2\nThe above example uses the FLAME method (FMRIB’s Local Analysis of Mixed Effects).\n3. Inspecting and Interpreting Results\nAfter running your higher-level analysis, you should review:\n\nStatistical maps: (e.g., zstat images) to identify significant clusters.\nCluster reports: Check the clusters in the FEAT report.\nOverlay images: Use FSLview or FSLeyes to overlay significant clusters on anatomical images.\n"},"fMRI-Tutorial/4.-fMRIPrep-and-FitLins":{"slug":"fMRI-Tutorial/4.-fMRIPrep-and-FitLins","filePath":"content/fMRI Tutorial/4. fMRIPrep and FitLins.md","title":"4. fMRIPrep and FitLins","links":["fMRI-Tutorial/2.3-Preprocessing-with-fmriPrep"],"tags":["fMRIPrep","FitLins"],"content":"1. Preprocessing with fMRIPrep\nfMRIPrep is a robust and standardized pipeline for fMRI data. It handles tasks such as skull-stripping, motion correction, distortion correction, and registration to standard space—all while creating extensive reports for quality control.\nfMRIPrep is included in NeuroDesk and can be loaded by module command.\nmodule load fmriprep\nmodule list\nIt shows:\nCurrently Loaded Modules:\n  1) fmriprep/24.1.1\n\nNow the command fmriprep is available for further analysis.\n1.1 Batch process individual participants\nHere is a a typical bash code using fmriprep:\n#!/bin/bash\n \n#1. first load the module fmriprep\nmodule load fmriprep\n \n# setup input and output folders\nproj_dir=/dss/open/ds000003 # change to your project directory\n# preprossed data directory\noutput=${proj_dir}/derivatives \n \nfmriprep $proj_dir $output \\\n\tparticipant\\\n\t--participant-label 01\\\n\t-w /dss/tmp \\\n\t--fs-license-file /dss/fsl_licence.txt \\\n\t--fs-no-reconall \\\n\t--output-spaces T1w MNI152NLin2009cAsym\n\t-v\nParameters Explained:\n\n—participant-label: Select which subjects to process.\n—fs-no-reconall: if you do not wish to run FreeSurfer’s recon-all\n—output-space: Set your target standard space\n-w working directory\n-v verbose output\n\n1.2 Inspecting fMRIPrep Outputs\n\n\nQuality Control:\nOpen the HTML reports in your web browser to check the alignment and quality of preprocessing.\n\n\nOutputs:\nYour preprocessed images and confound regressors (confounds.tsv) are ready for statistical analysis.\n\n\n2. Defining the Statistical Model with BIDS-StatsModel\nThe official website provides detailed specifications. BIDS Stats Models describe how to fit statistical models for neuroimaging data using a machine-readable JSON document.  The following instructions primarily come from the official website.\nThis model focuses on general linear mixed models (GLMs) with the data input from fMRIPrep.\n2.1 Representing multi-stage neuroimaging models\nStatistical analysis of neuroimaging data occurs in stages, with parameter estimates from lower levels propagating to higher levels. In fMRI, we typically fit a design matrix to time series data, followed by a fixed-effects model for subject-level estimates, culminating in a dataset-level random-effects one-sample t-test to assess population effects. At each analysis level, we must identify image inputs linked to the design matrix and manage outputs from previous levels. BIDS Stats Models offers a machine-readable document to describe multi-stage neuroimaging analyses accurately and flexibly. This is achieved through a graph of Nodes representing each analysis level and Edges that indicate data flow between Nodes. Each Node includes a Model for estimation and at least one Contrast for computed outputs, along with a GroupBy directive for input organization.\nAn example from the official website:\nIn a Simon task, participants were scanned across 2 runs to indicate if a diamond presented to the left or right of a fixation cross was green or red. There were two conditions: color-spatial congruent and incongruent trials. The analysis aims to determine regions with greater activity for incongruent versus congruent trials across participants.\nFirst, we model run-level timeseries for separate “Incongruent” and “Congruent” trials for each run. Then, we compute the contrast for Incongruent &gt; Congruent (IvC) trials. This contrasts then goes to a subject-level estimator, averaging the IvC effect per subject. Lastly, we send these estimates to a dataset-level estimator for a one-sample t-test across the IvC contrast subject estimates.\nThis is a visualization how the analysis goes for 3 participants:\n\nWe can formulate this analysis using BIDS Stats Model using json:\n{\n  &quot;Name&quot;: &quot;Simon IvC&quot;,\n  &quot;BIDSModelVersion&quot;: &quot;1.0.0&quot;,\n  &quot;Input&quot;: {&quot;subject&quot;: [&quot;01&quot;, &quot;02&quot;, &quot;03&quot;], &quot;task&quot;: &quot;simon&quot;},\n  &quot;Nodes&quot;: [\n    {\n      &quot;Level&quot;: &quot;Run&quot;,\n      &quot;Name&quot;: &quot;run_level&quot;,\n      &quot;GroupBy&quot;: [&quot;run&quot;, &quot;subject&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1, &quot;incongruent&quot;, &quot;congruent&quot;], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;Contrasts&quot;: [\n        {\n          &quot;Name&quot;: &quot;IvC&quot;,\n          &quot;ConditionList&quot;: [&quot;incongruent&quot;, &quot;congruent&quot;],\n          &quot;Weights&quot;: [1, -1],\n          &quot;Test&quot;: &quot;t&quot;\n        }\n      ]\n    },\n    {\n      &quot;Level&quot;: &quot;Subject&quot;,\n      &quot;Name&quot;: &quot;subject_level&quot;,\n      &quot;GroupBy&quot;: [&quot;subject&quot;, &quot;contrast&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;meta&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\n    },\n    {\n      &quot;Level&quot;: &quot;Dataset&quot;,\n      &quot;Name&quot;: &quot;one-sample_dataset&quot;,\n      &quot;GroupBy&quot;: [&quot;contrast&quot;],\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\n    }\n  ]\n}\nAs you can, there are three levels of analysis: Run, Subject, Dataset. At the Run level, there are many sources of possible variables, and the most important event file _events.tsv.\n\n\n                  \n                  Tip\n                  \n                \n\nIn Visual Studio Code, Json Editor plugin can help you see a json in a simple tree view.\n\n\nThe model is defined by Model:\n&quot;Model&quot;: {&quot;X&quot;: [1, &quot;incongruent&quot;, &quot;congruent&quot;], &quot;Type&quot;: &quot;glm&quot;},\nwhere X defines variables in the design matrix, here are the constant, incongruent, and congruent conditions. The contrast analysis is sepcified by Contrast:\n&quot;Contrasts&quot;: [\n        {\n          &quot;Name&quot;: &quot;IvC&quot;,\n          &quot;ConditionList&quot;: [&quot;incongruent&quot;, &quot;congruent&quot;],\n          &quot;Weights&quot;: [1, -1],\n          &quot;Test&quot;: &quot;t&quot;\n        }\n      ]\nContrasts also define the outputs for the next level of analysis. You need to add contrast in GroupBy.\nAnother example\n{\n  &quot;Level&quot;: &quot;Run&quot;,\n  ...\n  &quot;Contrasts&quot;: [\n    {\n      &quot;Name&quot;: &quot;weighted sum&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;,\n                 &quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [1, &quot;-1/3&quot;, &quot;-1/3&quot;, &quot;-1/3&quot;]\n    },\n    {\n      &quot;Name&quot;: &quot;t-test 1&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;],\n      &quot;Weights&quot;: [1, -1],\n      &quot;Test&quot;: &quot;t&quot;\n    },\n    {\n      &quot;Name&quot;: &quot;t-test 2&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [1, -1],\n      &quot;Test&quot;: &quot;t&quot;\n    },\n    {\n      &quot;Name&quot;: &quot;My favorite F-test&quot;,\n      &quot;ConditionList&quot;: [&quot;cond_001&quot;, &quot;cond_002&quot;,\n                 &quot;cond_003&quot;, &quot;cond_004&quot;],\n      &quot;Weights&quot;: [[1, -1, 0, 0], [0, 0, -1, 1]]\n      &quot;Test&quot;: &quot;F&quot;\n    }\n  ]\n}\nAt the subject level, we estimate the mean effect for each subject, so we only need an intercept and using DummyConstrasts.\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;meta&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\nThe final group analysis, called Dataset level, doing similar analysis:\n      &quot;Model&quot;: {&quot;X&quot;: [1], &quot;Type&quot;: &quot;glm&quot;},\n      &quot;DummyContrasts&quot;: {&quot;Test&quot;: &quot;t&quot;}\nNote, the difference between subject-level run averaging and group-level is meta vs. glm.\n\n\n                  \n                  Info\n                  \n                \n\nThere are two types of first-level (i.e., Run level) glm: Nilearn’s FirstLevelModel (default) and AFNI’s 3dREMLfit. If you want to use the latter, specify --estimator afni during the command line fitlins. The difference is that Nilearn treats the residuals not autocorrelated, while AFNI uses an ARMI(1,1) model to each voxel to account for auto-correlated time series noise.\nFor the second-level analysis (e.g., Subject or Dataset), meta indicates a fixed-effects combination using Nilearn compute_fixed_effects(), while glm uses Nilearn’s SecondLevelModel estimator.\n\n\nAt present, the BIDS Stats Model spec assumes the same sequential hierarchy. Thus, the following node sequences are all valid:\n\nrun → subject → dataset\nrun → session → subject → dataset\nrun → subject\nsubject → dataset\n\nEach analysis node has fields defined in Node. The Modelspecification MUST be invoked prior to Constrasts and DummyContrasts.\nIf &quot;run&quot; is omitted, then all runs for each subject will be fit with a single model; likewise, if &quot;subject&quot; is omitted, then one model will be fit for each run, combining all subjects for higher levels than the current node.\nEnumerating Sequences of Nodes Using Edges\nEach Edge describes a single connection between two Nodes, and a combination of edges describes a sequence of Nodes.  We will see this in detailed models later.\nInputs and Outputs chaining\nImage inputs to a BIDS Model are minimally specified; However, inputs can be filtered based on keywords defined in the core BIDS spec, including task, run, session, and subject.  For example:\n  &quot;Name&quot;: &quot;my_analysis&quot;,\n  &quot;BIDSModelVersion&quot;: &quot;1.0.0&quot;,\n  &quot;Input&quot;: {\n    &quot;task&quot;: &quot;suppression&quot;,\n    &quot;run&quot;: 1\n  },\n  &quot;Nodes&quot;: [...],\n  &quot;Edges&quot;: [...],\n}\nIn this example, we want to analyze images only for the suppression task, with a run index of 1, and exclude others (see below).\n\nAll outputs from each node are automatically passed on as the inputs to the next node. Specifically, each node receives as input from the previous node any contrasts defined within the Contrasts sections of the previous node. Multiple lower-level inputs that share the same contrast name are automatically concatenated when they are read in.\n2.2 Design-level and scan-level variables\nAccording to the core BIDS spec, each analysis level (i.e., run, session, subject, or dataset) links to specific BIDS files containing optional design events or variables. These files should be automatically read and available for model specification. The core BIDS spec outlines here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevelBIDS file with variablesRun_events.tsv file corresponding to the image file  _stim.tsv.gz file corresponding to the image file  _physio.tsv.gz file corresponding to the image file  _timeseries.tsv file corresponding to the image fileSession_scans.tsv file corresponding to the sessionSubject_sessions.tsv file corresponding to the subjectDataset_participants.tsv corresponding to the entire dataset\nWithin each Node, the expectation is that the namespace should automatically include all variables found within the corresponding file(s) defined in the table. For example, if the events.tsv files for a given project contain columns named “face” and “house”, then, inside any node with Level=&quot;Run&quot;, the user can automatically use the names &quot;face&quot; and &quot;house&quot; anywhere in the Model specification.\nTransformations\nThe Transformation section of a Node allows one to specify transformations of variables that should be applied prior to constructing a design matrix. For example:\n&quot;Transformations&quot;:{\n\t&quot;Transformer&quot;:&quot;pybids-transforms-v1&quot;,\n\t&quot;Instructions&quot;:[\t\n\t\t{\n\t\t\t&quot;Name&quot;:&quot;Factor&quot;,\n\t\t\t&quot;Input&quot;:[&quot;trial_type&quot;]\n\t\t},\n\t\t{\n\t\t\t&quot;Name&quot;:&quot;Convolve&quot;,\n\t\t\t&quot;Input&quot;:[\n\t\t\t\t&quot;trial_type.word&quot;,\n\t\t\t\t&quot;trial_type.pseudoword&quot;\t],\n\t\t\t&quot;Model&quot;:&quot;spm&quot;\n\t\t}\n\t]\n}\nThe above is an example of how a transformation is used in a run-level analysis. The first Transformation Factor split the events (rows of the events.tsv file) based on the column trial_type. Then these factors are convolved with HRF, here the model is spm.\nPlease check examples from official github, you may learn more about specifications.\n3. Singularity with FitLins\nAfter defining the model json file, next you can run fitlins using singularity. In the MSense Lab server, it is located at /dss/containers/fitlins.simg\nA typical command is like this:\nfitlins data/bids_root/ out/ participant \\\n    -d data/derivatives/fmriprep/ -w work/\nHere is a concrete example:\n#!/bin/bash\nbids_dir=/dss/open/ds000003 # bids directory\nfmriprep_dir=/dss/open/ds000003/derivatives # fmriprep directory\noutput=/dss/open/ds000003/derivatives/fitlins # output director\nmodel_file=/dss/open/ds000003/code/linmodel.json # model json file\n \n# scratch folder\nscratch_dir=/dss/tmp\n#fitlins image\nfitlins=/dss/containers/fitlins.simg\n \n# rune the model\nsingularity run \\\n\t$fitlins $bids_dir $output \\\n\tdataset \\\n\t--participant-label 01 02 03 05 06 07 08 09 10 11 12 13 \\\n\t--derivatives $fmriprep_dir \\\n\t--model $model_file \\\n\t--smoothing 5:run \\\n\t-w $scratch_dir \\\n\t--n-cpus 6 \\\n\t-v\n\n\n                  \n                  Tip\n                  \n                \n\nWhen you write command code in multiple lines with continuation symbol \\, you should not add spaces after that.\n\n\nVisualize the results\nThe results were organized in subfolders, such as node-FTest, node-subject, node-tTest.\nIn each sub-folder, there will be contrast-related results:\n\nYou can visualize these results using NiLearn. Here is one simple example:\nfrom nilearn import image, plotting\n!echo $PWD\n \n# point to your output directory\nresults_dir = &#039;../derivatives/fitlins/node-tTest&#039;\nnl_wp_path = results_dir + &#039;/contrast-wordGtPseudo_stat-t_statmap.nii.gz&#039;\n \nnl_wp_img = image.load_img(nl_wp_path)\n\topa = plotting.plot_glass_brain(nl_wp_img, colorbar=True, symmetric_cbar=True, cmap=&#039;bwr&#039;, threshold = 3, plot_abs=False)\nopa.title(&#039;Word &gt; pseudo t-stat&#039;)\n \n# plot smoothed or cluster results\nfrom nilearn.image import smooth_img, threshold_img\nfrom nilearn.regions import connected_regions\n \n#smooth before clustering\nwp_img_smooth = smooth_img(nl_wp_img, fwhm=6)\n# apply threshold\nwp_img_thresholded=threshold_img(wp_img_smooth, threshold=3, two_sided=True)\ncluster_threshold = 10\n \nregions, index = connected_regions(wp_img_thresholded, min_region_size=cluster_threshold)\n \n# plot the surviving clusters\n \nplotting.plot_glass_brain(wp_img_thresholded, colorbar=True, symmetric_cbar=True, cmap=&#039;bwr&#039;, plot_abs=False)\n \nplotting.plot_glass_brain(regions, colorbar = True, display_mode=&#039;lyrz&#039;)\n \nplotting.show()\n\n\n"},"index":{"slug":"index","filePath":"content/index.md","title":"MSense Lab fMRI Tutorial","links":["fMRI-Tutorial/1.3-Cloud-Computing-and-Access","fMRI-Tutorial/2.1-MRI-Data-Preparation","fMRI-Tutorial/2.2-BIDS-Structure","fMRI-Tutorial/2.3-Preprocessing-with-fmriPrep"],"tags":[],"content":"This tutorial draws from a series of sessions held during our lab seminar. The content is regularly updated. Below is quick access to essential information for an immediate start:\n\nHow to access the cloud server: 1.3 Cloud Computing and Access\nHow to convert raw MRI data to BIDS format: 2.1 MRI Data Preparation and 2.2 BIDS Structure\n2.3 Preprocessing with fmriPrep\n"},"knowledge/Tips-and-Tricks":{"slug":"knowledge/Tips-and-Tricks","filePath":"content/knowledge/Tips and Tricks.md","title":"Tips and Tricks","links":[],"tags":[],"content":"Neurocommand in Linux Cluster\nInstall Container\n\nSearch and install Specific Containers\n\nbash containers.sh fmriprep\nThe above will give you options of which specific containers you want to install. For example:\n| fmriprep | 24.1.0 | 20241003 | functional imaging,workflows | Run:\n\n--------------------------------------------------------------------\n\n./local/fetch_containers.sh fmriprep 24.1.0 20241003\n\nInstall Singularity containers\nFind docker images, and then build it with singularity. For example, install FitLins:\nsingularity build /path/to/fitlins-&lt;VERSION&gt;.simg \\\n  docker://poldracklab/fitlins:&lt;VERSION&gt;"},"knowledge/git-annex":{"slug":"knowledge/git-annex","filePath":"content/knowledge/git-annex.md","title":"git-annex","links":[],"tags":[],"content":"git annex tips\nTo ensure that your CSV files are managed by Git and not by git-annex, you can use the git annex unannex command to convert files that are currently managed by git-annex back to being managed by Git. After using unannex, you will commit the change to record the fact that these files are no longer managed by git-annex.\nHere’s how you can do it:\n\n\nNavigate to your repository’s directory in the terminal.\n\n\nUnannex the CSV files you want to be managed by Git. You can do this for individual files or for all CSV files in a directory. Here’s how you do it for all CSV files in the current directory:\ngit annex unannex *.csv\nIf your CSV files are spread out in different directories or you want to be more selective, you’ll need to adjust the command accordingly.\n\n\nCommit the change to record that these files are no longer managed by git-annex:\ngit commit -am &quot;Move CSV files from git-annex to Git management&quot;\n\n\nPush the changes to your remote repository:\ngit push\n\n"}}